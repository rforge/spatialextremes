\documentclass[a4paper,11pt]{book}
\usepackage{layout}

\begin{document}
% \VignetteIndexEntry{A R Package for Modelling Spatial Extremes} 
% \VignetteDepends{SpatialExtremes,RandomFields}
% \VignetteKeyword{Extreme Value Theory, Spatial Extremes, Max-stable processes} 
% \VignettePackage{SpatialExtremes}

\pagestyle{empty}
\frontmatter
\begin{titlepage}
  \vspace*{2cm}
  \begin{center}
    \LARGE A User's Guide to the SpatialExtremes Package\\
    \vspace{1em}
    \Large Mathieu Ribatet\\
    \vspace{1em}
    Copyright \copyright{2009}\\
    \vspace{2em}
    \large
    Chair of Statistics\\
    École Polytechnique Fédérale de Lausanne\\
    Switzerland\\
  \end{center}
  \vfill
  \begin{center}
<<echo=FALSE,fig=TRUE>>=
library(SpatialExtremes)
##image(volcano, col = terrain.colors(100), xaxt = "n", yaxt="n")
##contour(volcano, add = TRUE)
set.seed(12)
n.site <- 50
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

##Simulate a max-stable process - with unit Frechet margins
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="wh",
                   param=c(0,1,0,1, 2), maxstable="extr",
                   n = 40)
ms0 <- t(ms0)

##Compute the lambda-madogram
lmadogram(ms0, locations, n.bins = 25, border = "grey", box = FALSE)
@ 
  \end{center}
\end{titlepage}

\mainmatter
\pagenumbering{roman}
\normalsize

\tableofcontents
%%\listoftables
\listoffigures

<<echo=false>>=
options(SweaveHooks=list(fig=function()
par(mar=c(5.1, 4.1, 1.1, 2.1))))
@

\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter\ $\cdot$\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ $\cdot$\ #1}}

\chapter*{Introduction}
\label{cha:introduction}
\addcontentsline{toc}{chapter}{Introduction}
\pagenumbering{arabic}


\section*{What is the SpatialExtremes package?}
\label{sec:what-spat-pack}

The \textbf{SpatialExtremes} package is an add-on package for the R
\citep{Rsoft} statistical computing system. It provides functions for
the analysis of spatial extremes using (currently) max-stable
processes. 

All comments, criticisms and queries on the package or associated
documentation are gratefully received.

\section*{Obtaining the package/guide}
\label{sec:obta-pack}

The package can be downloaded from CRAN (The Comprehensive R Archive
Network) at \url{http://cran.r-project.org/}.  This guide (in pdf)
will be in the directory \verb+SpatialExtremes/doc/+ underneath
wherever the package is installed. You can get it by invoking
<<eval=FALSE>>=
vignette("SpatialExtremesGuide")
@ 

To have a quick overview of what the package does, you might want to
have a look at its own web page
\url{http://spatialextremes.r-forge.r-project.org/}.

\section*{Contents}

To help users to use properly the \emph{SpatialExtremes} packages,
this report introduces all the theory and references needed. Some
practical examples are inserted directly within the text to show how
it works in practice. Chapter~\ref{cha:an-introduction-max} is an
introduction to max-stable processes and introduces several models
that might be useful in concrete applications. Statistics and tools to
analyze the spatial dependence of extremes are presented in
Chapter~\ref{cha:spatial-depend-max}. Chapter~\ref{chat:fit-maxstab-frech}
tackles the problem of fitting max-stable process to data that are
assumed to be unit Fréchet
distributed. Chapter~\ref{cha:model-selection} is devoted to model
selection. Chapter~\ref{cha:fit-maxstab-gev} presents models and
procedures on how to fit max-stable processes to data that do not have
unit Fréchet margins. Chapter~\ref{cha:model-checking} is devoted to
model checking while Chapter~\ref{cha:infer-proc-pred} is devoted to
inferential procedures and predictions. Lastly,
Chapter~\ref{cha:conclusion} draws conclusions on spatial
extremes. Note that several computations are reported in the Annex
part.

\section*{Caveat}

I have checked these functions as best I can but they may contain
bugs.  If you find a bug or suspected bug in the code or the
documentation please report it to me at
\href{mailto:mathieu.ribatet@epfl.ch}{mathieu.ribatet@epfl.ch}.
Please include an appropriate subject line.

\section*{Legalese}

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 3
of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but without any warranty; without even the implied warranty of
merchantability or fitness for a particular purpose.  
See the GNU General Public License for more details.


A copy of the GNU General Public License can be obtained from 
\url{http://www.gnu.org/copyleft/gpl.html}.

\section*{Acknowledgements}

This work has been supported by the Competence Center Environment and
Sustainability \url{http://www.cces.ethz.ch/index} within the EXTREMES
project \url{http://www.cces.ethz.ch/projects/hazri/EXTREMES}. I would
like to thank S. A. Padoan for his support when we were both
calculating the tedious partial derivatives required for the
estimation of the asymptotic sandwich covariance matrix.

\chapter{An Introduction to Max-Stable Processes}
\label{cha:an-introduction-max}

A max-stable process $Z(\cdot)$ is the limit process of maxima of
independent identically distributed random fields $Y_i(x)$, $x \in
\mathbb{R}^d$. Namely, for suitable $a_n(x) > 0$ and $b_n(x) \in
\mathbb{R}$,
\begin{equation}
  \label{eq:maxstab-def}
  Z(x) = \lim_{n \rightarrow +\infty} \frac{\max_{i=1}^n Y_i(x) -
    b_n(x)}{a_n(x)}, \qquad x \in \mathbb{R}^d
\end{equation}
Note that \eqref{eq:maxstab-def} does not ensure that the limit
exists. However, provided it does and from \eqref{eq:maxstab-def}, we
can see that max-stable processes might be appropriate models for
modelling annual maxima of spatial data, for example.

Theoretically speaking, there is no loss of generality in transforming
the margins to have a unit Fréchet scale i.e.
\begin{equation}
  \label{eq:CDFFrechet}
  \Pr\left[Z(x) \leq z\right] = \exp\left(-\frac{1}{z} \right), \qquad
    \forall x \in \mathbb{R}^d, \qquad z > 0
\end{equation}
and we will first assume that the unit Fréchet assumption holds for
which we have $a_n(x) = n$ and $b_n(x) = 0$.

Currently, there are two different characterisations of a max-stable
process. The first one, often referred to as the \emph{rainfall-storm}
model, was first introduced by \citet{Smith1991}. More recently,
\citet{Schlather2002} introduced a new characterisation of a
max-stable process allowing for a random shape. In this Chapter, we
will present several max-stable models that might be relevant in
studying spatial extremes.

\section{The Smith Model}
\label{sec:smiths-char}

\citet{Haan1984} was the first to proposed a characterisation of
max-stable processes. Later, \citet{Smith1991} use this
characterisation to provide a parametric model for spatial
extremes. The construction was the following. Let $\{(\xi_i, y_i), i
\geq 1\}$ denote the points of a Poisson process on $(0,+\infty)
\times \mathbb{R}^d$ with intensity measure $\xi^{-2} d\xi \nu(dy)$,
where $\nu(dy)$ is a positive measure on $\mathbb{R}^d$. Then one
characterisation of a max-stable process with unit Fréchet margins is
\begin{equation}
  \label{eq:smithChar}
  Z(x) = \max_i \left\{ \xi_i f(y_i,x) \right\}, \qquad x \in
  \mathbb{R}^d
\end{equation}
where $\{f(y,x), x,y \in \mathbb{R}^d\}$ is a non-negative function
such that
\begin{equation*}
  \int_{\mathbb{R}^d} f(x,y) \nu(dy) = 1, \qquad \forall x \in
  \mathbb{R}^d
\end{equation*}

To see that equation~\eqref{eq:smithChar} defines a stationary
max-stable process with unit Fréchet margins, we have to check that
the margins are indeed unit Fréchet and $Z(x)$ has the max-stable
property. Following Smith, consider the set defined by:
\begin{equation*}
  E = \left\{(\xi, y) \in \mathbb{R}^+_* \times \mathbb{R}^d: \xi
    f(y,x) > z \right\}
\end{equation*}
for a fixed location $x \in \mathbb{R}^d$ and $z>0$. Then
\begin{eqnarray*}
  \Pr\left[Z(x) \leq z \right] &=& \Pr \left[\text{no points in } E
  \right] = \exp \left[ -\int_{\mathbb{R}^d} \int_{z/f(y,x)}^{+\infty}
    \xi^{-2} d\xi \nu(\mbox{dy}) \right]\\ 
  &=& \exp \left[ - \int_{\mathbb{R}^d} z^{-1} f(x,y) \nu(\mbox{dy})
  \right] = \exp \left(- \frac{1}{z} \right)
\end{eqnarray*}
and the margins are unit Fréchet.

The max-stable property of $Z(\cdot)$ follows because the
superposition of $n$ independent, identical Poisson processes is a
Poisson process with its intensity multiplied by $n$. More precisely,
we have:
\begin{equation*}
  \left\{\max_{i=1}^n Z_i(x_1), \ldots, \max_{i=1}^n Z_i(x_k) \right\}
  \stackrel{\cdot}{\sim} n \left\{Z(x_1), \ldots, Z(x_k)\right\}, \qquad k
    \in \mathbb{N}.
\end{equation*}

The process defined by \eqref{eq:smithChar} is often referred to as
the rainfall-storm process, as one can have a more physical
interpretation of the above construction. Think of $y_i$ as
realisations of rainfall storm centres in $\mathbb{R}^d$ and $\nu(dy)$
as the spatial distribution of these storm centres over $\mathbb{R}^d$
- usually $d = 2$. Each $\xi_i$ represents the intensity of the $i$-th
storm and therefore $\xi_i f(y_i, x)$ represents the amount of
rainfall for this specific event at location $x$. In other words,
$f(y_i, \cdot)$ drives how the $i$-th storm centred at $y_i$ diffuses
in space.

Definition \eqref{eq:smithChar} is rather general and Smith considered
a particular setting where $\nu(dy)$ is the Lebesgue measure and
$f(y,x) = f_0(y-x)$, where $f_0(y-x)$ is a multivariate normal density
with mean $y$ and covariance matrix $\Sigma$\footnote{Another form of
  Smith's model that uses a Student distribution instead of the normal
  one. However, it is not currently implemented.}. With these
additional assumptions, it can be shown that the bivariate CDF is
\begin{equation}
  \label{eq:smith}
  \Pr[Z(x_1) \leq z_1, Z(x_2) \leq z_2] = \exp\left[-\frac{1}{z_1} \Phi
    \left(\frac{a}{2} + \frac{1}{a} \log \frac{z_2}{z_1} \right) -
    \frac{1}{z_2} \Phi \left(\frac{a}{2} + \frac{1}{a}
      \log\frac{z_1}{z_2} \right) \right]
\end{equation}
where $\Phi$ is the standard normal cumulative distribution function
and, for two given locations 1 and 2  
\begin{equation*}
  a^2 = \Delta x^T \Sigma^{-1} \Delta x, \qquad
  \Sigma = 
  \begin{bmatrix}
    \sigma_{11} & \sigma_{12}\\
    \sigma_{12} & \sigma_{22}
  \end{bmatrix}
  \quad \text{or} \quad \Sigma = 
  \begin{bmatrix}
    \sigma_{11} & \sigma_{12} & \sigma_{13}\\
    \sigma_{12} & \sigma_{22} & \sigma_{23}\\
    \sigma_{13} & \sigma_{23} & \sigma_{33}
  \end{bmatrix}
  \quad \text{and so forth}
\end{equation*}
where $\Delta x$ is the distance vector between location 1 and
location 2. Figure~\ref{fig:Smith2sim} plots two simulations of
Smith's model with different variance-covariance matrices.
\begin{proof}
  \begin{eqnarray*}
    -\log \Pr \left[Z(x_1) \leq z_1, Z(x_2) \leq z_2 \right] &=&
    \iint_{\min\{z_1/f_0(s - x_1), z_2 / f_0(s- x_2) \}} \xi^{-2} d\xi
    \mbox{ds}\\
    &=& \int \max\left\{\frac{f_0(s - x_1)}{z_1}, \frac{f_0(s-
        x_2)}{z_2}\right\} \mbox{ds}\\
    &=& \int \frac{f_0(s - x_1)}{z_1} \mathbb{I}\left(\frac{f_0(s -
        x_1)}{z_1} > \frac{f_0(s - x_2)}{z_2} \right) \mbox{ds}\\
    &+& \int \frac{f_0(s - x_2)}{z_2} \mathbb{I}\left(\frac{f_0(s -
        x_1)}{z_1} \leq \frac{f_0(s - x_2)}{z_2} \right) \mbox{ds}\\
    &=& \int \frac{f_0(s)}{z_1} \mathbb{I}\left(\frac{f_0(s)}{z_1} >
      \frac{f_0(s - x_2 + x_1)}{z_2} \right) \mbox{ds}\\
    &+& \int \frac{f_0(s)}{z_2} \mathbb{I}\left(\frac{f_0(s - x_1 +
        x_2)}{z_1} \leq \frac{f_0(s)}{z_2} \right) \mbox{ds}\\
    &=& \frac{1}{z_1} \mathbb{E}\left[\mathbb{I}\left(\frac{f_0(X)}{z_1}
        > \frac{f_0(X - x_2 + x_1)}{z_2} \right) \right]\\
    &+& \frac{1}{z_2} \mathbb{E}\left[ \mathbb{I}\left(\frac{f_0(X - x_1
          + x_2)}{z_1} \leq \frac{f_0(X)}{z_2} \right)\right]
  \end{eqnarray*}
  where $X$ is a r.v.\ having $f_0$ as density. To get the closed form of
  the bivariate distribution, it remains to compute the probabilities of
  the event $\{f_0(X)/z_1 > f_0(X - x_2 + x_1)/z_2\}$.
  {\small
    \begin{eqnarray*}
      \frac{f_0(X)}{z_1} > \frac{f_0(X - x_2 + x_1)}{z_2}
      &\Longleftrightarrow& 2 \log z_1 + X^T \Sigma^{-1} X < 2 \log z_2
      + \left(X - x_2 + x_1 \right)^T \Sigma^{-1} \left(X - x_2 + x_1
      \right)\\
      &\Longleftrightarrow& X^T \Sigma^{-1} (x_1 - x_2) > \log
      \frac{z_1}{z_2} - \frac{1}{2} (x_1 - x_2)^T \Sigma^{-1} (x_1 - x_2)
    \end{eqnarray*}
  }
  As $X$ has density $f_0$, $X^T \Sigma^{-1} (x_1 - x_2)$ is normal with
  mean $0$ and variance $a^2=(x_1 - x_2)^T \Sigma^{-1} (x_1 - x_2)$. And
  finally, we get
  \begin{eqnarray*}
    \frac{1}{z_1} \mathbb{E}\left[ \mathbb{I} \left(
        \frac{f_0(X)}{z_1} > \frac{f_0(X - x_2 + x_1)}{z_2} \right)
    \right] &=& \frac{1}{z_1} \left\{1 - \Phi\left(\frac{\log
          z_1/z_2}{a} - \frac{a}{2} \right) \right\}\\
    &=& \frac{1}{z_1} \Phi\left(\frac{a}{2} + \frac{\log z_2/z_1}{a}
    \right)\\
  \end{eqnarray*}
  and
  \begin{eqnarray*}
    \frac{1}{z_2} \mathbb{E}\left[ \mathbb{I} \left( \frac{ f_0(X -
          x_1 + x_2)}{z_1} \leq \frac{f_0(X)}{z_2} \right)\right] &=& 
    \frac{1}{z_2} \Phi\left(\frac{a}{2} + \frac{\log z_1/z_2}{a} \right)
  \end{eqnarray*}
  This proves equation~\eqref{eq:smith}.
\end{proof}

<<label=Smith2Sim,echo=FALSE>>=
x <- y <- seq(0, 10, length = 100)
sigma <- matrix(c(9/8, 0, 0, 9/8),ncol = 2)
sigma2 <- matrix(c(9/8, 1, 1, 9/8),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
sigma.inv2 <- solve(sigma2)
sqrtCinv2 <- t(chol(sigma.inv2))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
model2 <- list(list(model = "gauss", var = 1, aniso = sqrtCinv2 / 2))
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=TRUE, model = model, maxstable = "Bool")
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=TRUE, model = model2, maxstable = "Bool")
ms1 <- t(ms1)

par(mfrow=c(1,2))
image(x, y, ms0, col = terrain.colors(30))
image(x, y, ms1, col = terrain.colors(30))
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<Smith2Sim>>
@ 
  \caption{Two simulations of the Smith model with different $\Sigma$
    matrices. Left panel: $\sigma_{11} = \sigma_{22} = 9/8$ and
    $\sigma_{12} = 0$. Right panel: $\sigma_{11} = \sigma_{22} = 9/8$
    and $\sigma_{12} = 1$.}
  \label{fig:Smith2sim}
\end{figure}

In equation~\eqref{eq:smith}, $a$ is the Mahanalobis distance and is
similar to the Euclidean distance except that it gives different
weights to each component of $\Delta x$. It is positive and the
limiting cases $a \rightarrow 0^+$ and $a \rightarrow +\infty$
correspond respectively to perfect dependence and
independence. Therefore, for $\Sigma$ fixed, the dependence decreases
monotically and continuously as $||\Delta x ||$ increases. On the
contrary, if $\Delta x$ is fixed, the dependence decreases monotically
as $a$ increases.

The covariance matrix $\Sigma$ plays a major role in
equation~\eqref{eq:smith} as it determines the shape of the storm
events. Indeed, due the use of a multivariate normal distribution, the
storm events have an elliptical shape. Considering the
eigen-decomposition of $\Sigma$, we can write
\begin{equation}
  \label{eq:svdSigma}
  \Sigma = U \Lambda U^T,
\end{equation}
where $U$ is a rotation matrix and $\Lambda$ is a diagonal matrix of
the eigenvalues. Thus, $U$ controls the direction of the principal
axes and $\Lambda$ controls their lengths. 

If $\Sigma$ is diagonal and all the diagonal terms are identical, then
$\Sigma = \Lambda$, so that the ellipsoids change to circles and
model~\eqref{eq:smith} becomes isotropic. Figure~\ref{fig:Smith2sim}
is a nice illustration of this. The left panel corresponds to an
isotropic random field while the right one depicts a clear anisotropy
for which we have
\begin{equation*}
  \Sigma =
  \begin{bmatrix}
    9/8 & 1\\
    1 & 9/8
  \end{bmatrix}
  =
  \begin{bmatrix}
    \phantom{-}\cos (-3 \pi / 4) & \sin (-3 \pi / 4)\\
    -\sin (-3 \pi / 4) & \cos (-3 \pi / 4)
  \end{bmatrix}
  \begin{bmatrix}
    1/8 & 0\\
    0 & 17/8
  \end{bmatrix}
  \begin{bmatrix}
    \cos (-3 \pi / 4) & -\sin (-3 \pi / 4)\\
    \sin (-3 \pi / 4) & \phantom{-}\cos (-3 \pi / 4)
  \end{bmatrix},
\end{equation*}
so that the main direction of the major principal axis is $\pi/4$ and
a one unit move along the direction $-\pi / 4$ yields the same
decrease in dependence as $17$ unit moves along the direction $\pi /
4$.

\section{The Schlather Model}
\label{sec:schl-char}

More recently, \citet{Schlather2002} introduced a second
characterisation of max-stable processes. Let $Y(\cdot)$ be a
stationary process on $\mathbb{R}^d$ such that $\mathbb{E}[\max\{0,
Y(x)\}] = 1$ and $\{\xi_i, i \geq 1\}$ be the points of a Poisson
process on $\mathbb{R}^+_*$ with intensity measure $\xi^{-2}
d\xi$. Then Schlather shows that a stationary max-stable process with
unit Fréchet margins can be defined by:
\begin{equation}
  \label{eq:SchlatherChar}
  Z(x) = \max_i \xi_i \max \left\{0, Y_i(x) \right\}
\end{equation}
where the $Y_i(\cdot)$ are i.i.d copies of $Y(\cdot)$.

As before, the max-stable property of $Z(\cdot)$ stems from the
superposition of $n$ independent, identical Poisson processes, while
the unit Fréchet margins holds by the same argument as for the Smith
model. Indeed, let consider the following set:
\begin{equation*}
  E = \left\{(\xi, y(x)) \in \mathbb{R}^+_* \times \mathbb{R}^d: \xi
    \max(0, y(x)) > z \right\}
\end{equation*}
for a fixed location $x \in \mathbb{R}^d$ and $z > 0$. Then
\begin{eqnarray*}
  \Pr\left[Z(x) \leq z \right] &=& \Pr\left[\text{no points in } E \right]
  = \exp \left[ - \int_{\mathbb{R}^d} \int_{z/\max(0, y(x))}^{+\infty}
    \xi^{-2} d\xi \nu(dy(x)) \right]\\
  &=& \exp \left[ - \int_{\mathbb{R}^d} z^{-1} \max \{0, y(x) \}
    \nu(dy(x)) \right] = \exp\left(-\frac{1}{z}\right)
\end{eqnarray*}

As with the Smith model, the process defined in
equation~\eqref{eq:SchlatherChar} has a practical
interpretation. Think of $\xi_i Y_i(\cdot)$ as the daily spatial
rainfall events so that all these events have the same spatial
dependence structure but differ only in their magnitude $\xi_i$. This
model differs slightly from Smith's one as we now have no
deterministic shape such as a multivariate normal density for the
storms but a random shape driven by the process
$Y(\cdot)$.

The Schlather and Smith characterisations have strong connections. To
see this, let consider the case for which $Y_i(x) = f_0(x - X_i)$
where $f_0$ is a probability density function and $\{X_i\}$ is a
homogeneous Poisson process both in $\mathbb{R}^d$. With this
particular setting, model~\eqref{eq:SchlatherChar} is identical to
model~\eqref{eq:smithChar}.

equation~\eqref{eq:SchlatherChar} is very general and we need
additional assumptions to get practical models. Schlather proposed to
take $Y_i(\cdot)$ to be a stationary Gaussian process with correlation
function $\rho(x)$, scaled so that $\mathbb{E}[\max\{0, Y_i(x)\}] =
1$. With these new assumtions, it can be shown that the bivariate CDF
of process~\eqref{eq:SchlatherChar} is
\begin{equation}
  \label{eq:schlather}
  \Pr[Z(x_1) \leq z_1, Z(x_2) \leq z_2] = \exp\left[-\frac{1}{2}
    \left(\frac{1}{z_1} + \frac{1}{z_2} \right) \left(1 + \sqrt{1 - 2
        (\rho(h) + 1) \frac{z_1 z_2}{(z_1 + z_2)^2}} \right) \right]
\end{equation}
where $h \in \mathbb{R}^+$ is the Euclidean distance between location
1 and location 2. Usually, $\rho(h)$ is chosen from one of the valid
parametric families, such as

<<label=covariances,echo=FALSE>>=
par(mfrow=c(1,3))
covariance(sill = 1, range = 1, smooth = 4, cov.mod = "whitmat",
           xlim = c(0,6), ylim = c(0, 1))
covariance(sill = 1, range = 1, smooth = 2, cov.mod = "whitmat",
           add = TRUE, col = 2, xlim = c(0,6))
covariance(sill = 1, range = 1, smooth = 1, cov.mod = "whitmat",
           add = TRUE, col = 3, xlim = c(0,6))
covariance(sill = 1, range = 1, smooth = 0.5, cov.mod = "whitmat",
           col = 4, add = TRUE, xlim = c(0,6))
legend("topright", c(expression(nu == 4), expression(nu == 2),
                     expression(nu == 1), expression(nu == 0.5)),
       col = 1:4, lty = 1, inset = 0.05)

covariance(sill = 1, range = 1, smooth = 2, cov.mod = "powexp",
           xlim = c(0, 3), ylim = c(0, 1))
covariance(sill = 1, range = 1, smooth = 1.5, cov.mod = "powexp",
           add = TRUE, col = 2, xlim = c(0, 3))
covariance(sill = 1, range = 1, smooth = 1, cov.mod = "powexp",
           add = TRUE, col = 3, xlim = c(0, 3))
covariance(sill = 1, range = 1, smooth = 0.75, cov.mod = "powexp",
           add = TRUE, col = 4, xlim = c(0, 3))
legend("topright", c(expression(nu == 2), expression(nu == 1.5),
                     expression(nu == 1), expression(nu == 0.75)),
       col = 1:4, lty = 1, inset = 0.05)

covariance(sill = 1, range = 1, smooth = 4, cov.mod = "cauchy",
           xlim = c(0, 3), ylim = c(0, 1))
covariance(sill = 1, range = 1, smooth = 2, cov.mod = "cauchy",
           add = TRUE, col = 2, xlim = c(0, 3))
covariance(sill = 1, range = 1, smooth = 1, cov.mod = "cauchy",
           add = TRUE, col = 3, xlim = c(0, 3))
covariance(sill = 1, range = 1, smooth = 0.75, cov.mod = "cauchy",
           add = TRUE, col = 4, xlim = c(0, 3))
legend("topright", c(expression(nu == 4), expression(nu == 2),
                     expression(nu == 1), expression(nu == 0.75)),
       col = 1:4, lty = 1, inset = 0.05)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<covariances>>
@ 
\caption{Plots of the Whittle--Matérn (left panel), the powered
  exponential (middle panel) and the Cauchy (right panel) correlation
  functions. The sill and the range parameters are $c_1 = c_2 = 1$
  while the smooth parameters are given in the legend.}
  \label{fig:covariances}
\end{figure}

\bigskip
\begin{tabular}{lll}
  \textbf{Whittle--Matérn} & $\rho(h) = c_1
  \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{h}{c_2} \right)^{\nu}
  K_{\nu}\left(\frac{h}{c_2} \right)$, & $0 \leq c_1 \leq 1$, $c_2>0$,
  $\nu > 0$\\
  \textbf{Cauchy} & $\rho(h) = c_1 \left[1 + \left(\frac{h}{c_2}
    \right)^2 \right]^{-\nu}$, & $0 \leq c_1 \leq 1$, $c_2 >0$, $\nu >
  0$\\
  \textbf{Powered Exponential} & $\rho(h) = c_1 \exp\left[-
    \left(\frac{h}{c_2} \right)^{\nu} \right]$, & $0 \leq c_1 \leq 1$,
  $c_2 > 0$, $0 < \nu \leq 2$
\end{tabular}
\bigskip

where $c_1$, $c_2$ and $\nu$ are the sill, the range and the smooth
parameters of the correlation function, $\Gamma$ is the gamma function
and $K_{\nu}$ is the modified Bessel function of the third kind with
order $\nu$. Figure~\ref{fig:covariances} plots the correlation
functions for the parametric families introduced above. The left panel
was generated with the following lines
<<eval=FALSE>>=
covariance(sill = 1, range = 1, smooth = 4, cov.mod = "whitmat",
           xlim = c(0,6), ylim = c(0, 1))
covariance(sill = 1, range = 1, smooth = 2, cov.mod = "whitmat",
           add = TRUE, col = 2, xlim = c(0,6))
covariance(sill = 1, range = 1, smooth = 1, cov.mod = "whitmat",
           add = TRUE, col = 3, xlim = c(0,6))
covariance(sill = 1, range = 1, smooth = 0.5, cov.mod = "whitmat",
           col = 4, add = TRUE, xlim = c(0,6))
legend("topright", c(expression(nu == 4), expression(nu == 2),
                     expression(nu == 1), expression(nu == 0.5)),
       col = 1:4, lty = 1, inset = 0.05)
@ 

<<label=Schlather2Sim,echo=FALSE>>=
x <- y <- seq(0, 10, length = 100)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=TRUE, model="wh", param=c(0,1,0,1, 1), maxstable="extr")
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=TRUE, model="stable", param=c(0,1,0,1.5, 1), maxstable="extr")
ms1 <- t(ms1)

par(mfrow=c(1,2))
image(x, y, ms0, col = terrain.colors(30))
image(x, y, ms1, col = terrain.colors(30))
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<Schlather2Sim>>
@ 
\caption{Two simulations of the Schlather model with different
  correlation functions having approximately the same practical
  range. Left panel: Whittle--Matérn with $c_1 = c_2 = \nu = 1$. Right
  panel: Powered exponential with $c_1 = \nu = 1$ and $c_2 = 1.5$.}
  \label{fig:Schlather2sim}
\end{figure}

Figure~\ref{fig:Schlather2sim} plots two realisations of the Schlather
model with the powered exponential and Whittle--Matérn correlation
functions. It can be seen that the powered exponential model leads to
more rough random fields as, with this particular setting for the
covariance parameters, the slope of the powered exponential
correlation function near the origin is steeper than the
Whittle--Matérn.

<<label=anisoCovFun,echo=FALSE>>=
cov.fun <- covariance(sill = 1, range = 1, smooth = 1,
                      cov.mod = "powexp", plot = FALSE)
A <- matrix(c(cos(pi/4), - sin(pi/4), .5 * sin(pi/4), cos(pi/4)), 2)
cov.fun.aniso <- function(vec.pos)
  cov.fun(sqrt(vec.pos %*% A %*% vec.pos))

rho1 <- rho2 <- matrix(NA, 100, 100)
xs <- ys <- seq(-3, 3, length = 100)
for (i in 1:100){
  x <- xs[i]
  for (j in 1:100){
    rho1[i,j] <- cov.fun(sqrt(x^2+ys[j]^2))
    rho2[i,j] <- cov.fun.aniso(c(x, ys[j]))
  }
}

par(mfrow=c(1,2))
contour(xs, ys, rho1, xlab = expression(paste(Delta, x)),
        ylab = expression(paste(Delta, y)))
contour(xs, ys, rho2, xlab = expression(paste(Delta, x)),
        ylab = expression(paste(Delta, y)))
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<anisoCovFun>>
@ 
\caption{Contour plots of an isotropic (left panel) and anisotropic
  (right panel) correlation function. Powered exponential family with
  $c_1 = c_2 = \nu = 1$. The anisotropy parameters are: $\varphi = \pi
  / 4$, $r = 0.5$.}
  \label{fig:anisoCovFun}
\end{figure}

The correlation functions introduced above are all isotropic, but
model~\eqref{eq:schlather} doesn't require this assumption. From a
valid correlation function $\rho$ it is always possible to get an
elliptical correlation function $\rho_e$ by using the following
transformation:
\begin{equation}
  \label{eq:ellipticalCorrFun}
  \rho_e(\Delta x) = \rho\left(\sqrt{{\Delta x}^T A \Delta x} \right)
\end{equation}
where $\Delta x$ is the distance vector between two stations, $A$ is
any positive semi-definite matrix that may involve additional
parameters. For example, if the spatial domain belongs to
$\mathbb{R}^2$, a convenient parametrization for $A$ is given by
\begin{equation*}
  A =
  \begin{bmatrix}
    \phantom{-}\cos \varphi & r\sin \varphi\\
    -\sin \varphi & \phantom{r}\cos \varphi
  \end{bmatrix}
\end{equation*}
where $\varphi \in [0, \pi)$ is the rotation angle and $0 < r < 1$ is
the ratio between the minor and major principal axes of the
ellipse. Figure~\ref{fig:anisoCovFun} plots the contour of an
isotropic correlation function and an anistropic one derived from
equation~\eqref{eq:ellipticalCorrFun}.

The correlation coefficient $\rho(h)$ can take any value in
$[-1,-1]$. Complete dependence is reached when $\rho(h) = 1$ while
independence occurs when $\rho(h) = -1$. However, most parametric
correlation families don't allow negative values so that independence
is never reached. The next two sections propose two alternatives based
on characterisation~\eqref{eq:SchlatherChar} that bypass this hurdle.

\section{Independent Schlather Process}
\label{sec:indep-schl-proc}

In the previous section, we mentioned that the model proposed by
\citet{Schlather2002} cannot deal with independence. A first attempt
to solve this issue is to consider a new random process that consists
in a mixture of the Schlather model and a fully independent random
process. This model, called the independent Schlather process, is
defined by:
\begin{equation}
  \label{eq:indSchlatherChar}
  Z(x) = \max \left\{ \alpha Y, (1- \alpha) Z^*(x) \right\}
\end{equation}
where $\alpha \in [0, 1]$ is the mixture proportion, $Y$ is a unit
Fréchet random variable and $Z^*(\cdot)$ is the Schlather process as
defined by equation~\eqref{eq:schlather}. It can be seen directly from
equation~\eqref{eq:indSchlather} that independence occurs when $\alpha
= 1$.

$Z(\cdot)$ is a stationary max-stable process with unit Fréchet
margins. Indeed,
\begin{eqnarray*}
  \Pr \left[Z(x) \leq z \right] &=& \Pr \left[ \alpha Y \leq z, (1 -
    \alpha) Z^*(x) \leq z \right] = \Pr \left[ Y \leq z / \alpha \right]
  \Pr \left[Z^*(x) \leq z / (1 - \alpha) \right]\\
  &=& \exp \left[ - \alpha / z \right] \exp \left[ - (1-\alpha) / z
  \right] = \exp \left(-1/z \right)
\end{eqnarray*}
so that $Z(\cdot)$ has unit Fréchet margins. The max-stability of
$Z(\cdot)$ follows directly as $Y$ and $Z^*(x)$ are independent and
both of them are max-stable.

Using the additional assumptions mentioned in the previous section,
the bivariate CDF is given by: {\small
  \begin{equation}
    \label{eq:indSchlather}
    \Pr \left[Z(x_1) \leq z_1, Z(x_2) \leq z_2 \right] = \exp
    \left[ -\left(\frac{1}{z_1} + \frac{1}{z_2} \right) \left\{\alpha +
        \frac{1-\alpha}{2} \left(1 + \sqrt{1 - 2 (\rho(h) + 1) \frac{z_1
              z_2}{(z_1 + z_2)^2}} \right) \right\} \right]
  \end{equation}
}

<<label=indSchlather2Sim,echo=FALSE>>=
rfrechet <- function(n, loc = 0, scale = 1, shape = 1){
  if (min(scale) < 0 || min(shape) <= 0) 
    stop("invalid arguments")
  loc + scale * rexp(n)^(-1/shape)
}

x <- y <- seq(0, 10, length = 100)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=TRUE, model="wh", param=c(0,1,0,1, 1), maxstable="extr")
ms0 <- t(ms0)
alpha <- 0.1
z <- rfrechet(prod(dim(ms0)))
ms0 <- apply(cbind((1-alpha) * as.numeric(ms0), alpha * z), 1, max)
ms0 <- matrix(ms0, nrow = 100, ncol = 100)


set.seed(12)
ms1 <- MaxStableRF(x, y, grid=TRUE, model="wh", param=c(0,1,0,1, 1), maxstable="extr")
ms1 <- t(ms1)
alpha <- 0.9
z <- rfrechet(prod(dim(ms1)))
ms1 <- apply(cbind((1-alpha) * as.numeric(ms1), alpha * z), 1, max)
ms1 <- matrix(ms1, nrow = 100, ncol = 100)

breaks <- c(seq(0, 50, length = 30), 1500)
par(mfrow=c(1,2))
image(x, y, ms0, col = terrain.colors(31)[-31], breaks = breaks)
image(x, y, ms1, col = terrain.colors(31)[-31], breaks = breaks)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<indSchlather2Sim>>
@ 
\caption{Two simulations of the independent Schlather model having a
  Whittle--Matérn correlation function such as $c_1 = c_2 = \nu =
  1$. Left panel: $\alpha = 0.1$. Right panel: $\alpha = 0.9$.}
  \label{fig:indSchlather2sim}
\end{figure}

There are two main drawbacks with the independent Schlather
process. Firstly, although $Z(\cdot)$ is a valid stochastic process,
its spatial structure is quite unnatural. Indeed, the $Y$ component
can be thought as pure noise. Secondly, despite independence is
reached, there is still a range of dependence that couldn't be taken
into account. We will focus on this point in
Section~\ref{sec:extremal-coefficient}. Figure~\ref{fig:indSchlather2sim}
plots two simulated independent Schlather random fields but with two
different values of $\alpha$.

\section{Geometric Gaussian Process}
\label{sec:geom-gauss-proc}

To solve the issue of independence with the Schlather process,
A.C. Davison proposed another model referred to as the geometric
Gaussian model. The term geometric is by analogy with geometric
brownian motion. The geometric Gaussian model is based on
characterisation~\eqref{eq:SchlatherChar} but uses instead of Gaussian
process for $Y_i(\cdot)$, a log-normal process. More precisely, the
geometric Gaussian process is defined by:
\begin{equation}
  \label{eq:geomGaussChar}
  Z(x) = \max_i \xi_i Y_i(x)
\end{equation}
where the $Y_i(\cdot)$ are i.i.d copies of
\begin{equation*}
  Y(x) = \exp\left(\sigma \epsilon(x) - \frac{\sigma^2}{2} \right)
\end{equation*}
where $\epsilon(\cdot)$ is a standard Gaussian process with
correlation function $\rho$.

Obviously this random process is a unit Fréchet max-stable
process. Indeed, it shares the same construction as the Schlather
characterisation. The only condition that need to be satisfied is that
$\mathbb{E}[Y(x)]=1$. This is easily shown as the moment generating
function of a normal random variable $X$ with mean $\mu$ and variance
$\sigma^2$ is
\begin{equation*}
  M_X(t) = \mathbb{E} \left[ \exp(t X) \right] = \exp \left( \mu t +
    \frac{\sigma^2 t^2}{2} \right)
\end{equation*}
so that, we have
\begin{equation*}
  \mathbb{E}[Y(x)] = \exp \left(-\frac{\sigma^2}{2} +
    \frac{\sigma^2}{2} \right) = 1
\end{equation*}

\begin{figure}
  \centering
  \includegraphics[angle=-90,width=.6\textwidth]{Figures/geomGauss2Sim}
  \caption{Two simulations of the geometric Gaussian random process
    with the Whittle--Matérn correlation function such as $c_1 = c_2 =
    \nu = 1$. Left panel: $\sigma^2 = 0.1$. Right panel: $\sigma^2 =
    2$.}
  \label{fig:geomGauss2Sim}
\end{figure}

As $\sigma \rightarrow 0^+$ or $\rho(h) \rightarrow 1$ complete
dependence is reached while $\sigma \rightarrow +\infty$ leads to
independence whatever the $\rho(h)$ value
is. Figure~\ref{fig:geomGauss2Sim} plots two simulations of the
geometric Gausian process with two different values for
$\sigma^2$. Interestingly, the bivariate CDF
for~\eqref{eq:geomGaussChar} is the same as equation~\eqref{eq:smith}
where
\begin{equation}
  \label{eq:geomGaussAsSmith}
  a^2 = 2 \sigma^2 \left(1 - \rho(h) \right)
\end{equation}
\begin{proof}
  \begin{eqnarray*}
    -\log \Pr \left[Z(x_1) \leq z_1, Z(x_2) \leq z_2 \right] &=&
    \iint_{\min\{z_1/Y(x_1), z_2 / Y(x_2) \}}^{+\infty} \xi^{-2} d\xi
    dP(Y(x_1),Y(x_2))\\
    &=& \int \max\left\{\frac{Y(x_1)}{z_1}, \frac{Y(x_2)}{z_2} \right\}
    dP(Y(x_1),Y(x_2))\\
    &=& \int \frac{Y(x_1)}{z_1} \mathbb{I}\left(\frac{Y(x_1)}{z_1} >
      \frac{Y(x_2)}{z_2} \right) dP(Y(x_1),Y(x_2))\\
    &+& \int \frac{Y(x_2)}{z_2} \mathbb{I}\left(\frac{Y(x_1)}{z_1} \leq
      \frac{Y(x_2)}{z_2} \right) dP(Y(x_1),Y(x_2))
  \end{eqnarray*}

  Let first notice that
  \begin{equation*}
    \frac{Y(x_1)}{Y(x_2)} = \exp\left[\sigma \{\epsilon(x_1) -
      \epsilon(x_2)\} \right]
  \end{equation*}
  Define $W_a = \frac{\sigma}{2} \{\epsilon(x_1) - \epsilon(x_2) \}$
  and $W_b = \frac{\sigma}{2} \{\epsilon(x_1) + \epsilon(x_2) \}$. As
  $\epsilon(x_1)$ and $\epsilon(x_2)$ are both standard normal r.v.\
  having correlation $\rho$, $W_a$ and $W_b$ are also normal r.v.\
  with $0$ mean and their variances are equal to $\sigma^2 (1 - \rho)
  / 2$ and $\sigma^2 (1 + \rho) / 2$ respectively. Moreover, it is
  straightforward to see that $W_a$ and $W_b$ are independent.

  We have
  \begin{equation*}
    Y(x_1) = \exp\left( - \frac{\sigma^2}{2} \right)
    \exp\left(W_a\right) \exp\left(W_b\right) \qquad \text{and} \qquad 
    \frac{Y(x_1)}{z_1} > \frac{Y(x_2)}{z_2} \Longleftrightarrow
    W_a > \frac{1}{2} \log\left(\frac{z_1}{z_2}\right)
  \end{equation*}
  so that we get
  \begin{eqnarray*}
    A &:=&\int \frac{Y(x_1)}{z_1} \mathbb{I}\left(\frac{Y(x_1)}{z_1} >
      \frac{Y(x_2)}{z_2} \right) dP(Y(x_1),Y(x_2))\\
    &=& \frac{\exp(-\sigma^2/2)}{z_1} \iint \exp\left(W_a\right)
    \exp\left(W_b\right) I\left\{W_a > \frac{1}{2} \log
      \frac{z_1}{z_2}\right\} dP\left(W_a, W_b\right)\\
    &=& \frac{\exp(-\sigma^2/2)}{z_1} \int \exp\left(W_b\right)
    dP\left(W_b\right) \int \exp\left(W_a\right) I\left\{W_a >
      \frac{1}{2} \log\frac{z_1}{z_2}\right\} dP\left(W_a\right)\\
    &=& \frac{1}{z_1} \exp\left(-\frac{\sigma^2}{2} \right) \exp\left(
      \frac{\sigma^2 (1 + \rho)}{4}\right) \frac{1}{2}
    \exp\left(\frac{\sigma^2 (1 - \rho)}{4} \right)\left[1 -
      \Phi\left(\frac{\log \frac{z_1}{z_2} - \sigma^2 (1 - \rho)}{\sigma
          \sqrt{2 (1 - \rho)}} \right) \right]\\
    &=& \frac{1}{z_1} \Phi\left(\frac{\sigma \sqrt{1 - \rho}}{\sqrt{2}}
      - \frac{\log \frac{z_1}{z_2}}{\sigma \sqrt{2(1-\rho)}} \right)
  \end{eqnarray*}
  where we used that
  \begin{equation*}
    \int \exp\left(X\right) I\left\{X > c\right\}dP\left(X\right) =
    \frac{1}{2} \exp\left(\frac{\sigma^2}{2} \right) \Phi\left(\frac{c -
        \sigma^2}{2 \sigma} \right), \qquad \text{where } X \sim
    N(0,\sigma^2)
  \end{equation*}

  By symmetry, we get
  \begin{equation*}
    \int \frac{Y(x_2)}{z_2} \mathbb{I}\left(\frac{Y(x_2)}{z_2} >
      \frac{Y(x_1)}{z_1} \right) dP(Y(x_1),Y(x_2)) = \frac{1}{z_2}
    \Phi\left(\frac{\sigma \sqrt{1 - \rho}}{\sqrt{2}} - \frac{\log
        \frac{z_2}{z_1}}{\sigma \sqrt{2(1-\rho)}} \right)
  \end{equation*}
  This proves equation~\eqref{eq:geomGaussAsSmith}.
\end{proof}

\chapter{Spatial Dependence of Max-Stable Random Fields}
\label{cha:spatial-depend-max}

Sooner or later, statistical modellers will be interested in knowing
how evolves dependence in space. When dealing with non-extremal data,
a common tools is the (semi-)variogram $\gamma$
\citep{Cressie1993}. Let $Y(\cdot)$ be a stationary Gaussian process
with correlation function $\rho$ and variance $\sigma^2$. It is well
known that $Y(\cdot)$ is fully characterized by its mean and its
covariance. Consequently, the variogram defined as
\begin{equation}
  \label{eq:variogram}
  \gamma(x_1 - x_2) = \frac{1}{2} \mbox{Var}\left[Y(x_1) - Y(x_2)
  \right] = \sigma^2 \left\{1 - \rho(x_1 - x_2) \right\}
\end{equation}
determines the degree of spatial dependence of $Y(\cdot)$.

When extreme values are of interest, the variogram is no longer a
useful tool, as it may not even exist. Therefore, there is a need to
develop more suitable tools for analyzing the spatial dependence of
max-stable fields. In this chapter, we will present the extremal
coefficient as a measure of the degree of dependence for extreme
values, and variogram-based approaches that are especially well adapted
to extremes.

\section{The Extremal Coefficient}
\label{sec:extremal-coefficient}

Let $Z(\cdot)$ be a stationary max-stable random field with unit
Fréchet margins. The extremal dependence among $N$ fixed locations in
$\mathbb{R}^d$ can be summarised by the extremal coefficient, which is
defined as:
\begin{equation}
  \label{eq:extcoeff}
  \Pr\left[Z(x_1) \leq z, \ldots, Z(x_N) \leq z\right] = \exp\left(-
    \frac{\theta_N}{z} \right)
\end{equation}
where $1 \leq \theta_N \leq N$ with the lower and upper bounds
corresponding to complete dependence and independence and thus
provides a measure of the degree of spatial dependence between
stations. Following this idea, $\theta_N$ can be thought as the
effective number of independent stations.

Given the properties of the max-stable process with unit Fréchet
marings, the finite-dimensional CDF belongs to the class of
multivariate extreme value distributions
\begin{equation}
  \label{eq:MEVD}
  \Pr\left[Z(x_1) \leq z_1, \ldots, Z(x_N) \leq z_N\right] =
  \exp\left\{- V(z_1, \ldots, z_N) \right\}
\end{equation}
where $V$ is a homogeneous function of order $-1$ called the exponent
measure \citep{Pickands1981,Coles2001}. As a consequence, the
homogeneity property of $V$ implies a strong relationship between the
exponent measure and the extremal coefficient
\begin{equation}
  \label{eq:extCoeffVfun}
  \theta_N = V(1, \ldots, 1)
\end{equation}

An important special case of equation~\eqref{eq:extcoeff} is to
consider pairwise extremal coefficients, that is
\begin{equation}
  \label{eq:extcoeffPairwise}
  \Pr\left[Z(x_1) \leq z, Z(x_2) \leq z \right] =
  \exp\left\{-\frac{\theta(x_1 - x_2)}{z} \right\}
\end{equation}

Following \citet{Schlather2003}, $\theta(\cdot)$ is called the
extremal coefficient function; it provides sufficient information
about extremal dependence for many problems although it does not
characterise the full distribution.

The extremal coefficient functions for max-stable models presented in
Chapter~\ref{cha:an-introduction-max} can be derived directly from
their bivariate distribution by letting $z_1=z_2=z$. More precisely,
we have:

\bigskip
\begin{tabular}{ll}
  \textbf{Smith} & $\theta(x_1 - x_2) = 2
  \Phi\left(\frac{\sqrt{(x_1 - x_2)^T \Sigma^{-1} (x_1 - x_2)}}{2}
  \right)$\\
  \textbf{Schlather} & $\theta(x_1 - x_2) = 1 + \sqrt{\frac{1 -
      \rho(x_1 - x_2)}{2}}$\\
  \textbf{Independent Schlather} & $\theta(x_1 - x_2) = 2 \alpha +
  \left(1 - \alpha \right) \left(1 + \sqrt{\frac{1 - \rho(x_1 -
        x_2)}{2}} \right)$\\
  \textbf{Geometric Gaussian} & $\theta(x_1 - x_2) = 2 \Phi
  \left(\sqrt{\frac{\sigma^2 (1 - \rho(x_1 - x_2))}{2}} \right)$    
\end{tabular}
\bigskip

<<label=extCoeffModels,echo=FALSE>>=
smith <- function(h) 2 * pnorm(h/2)
cov.fun <- covariance(sill = 1, range = 1, smooth = 1, plot = FALSE)
schlather <- function(h)
  1 + sqrt((1-cov.fun(h))/2)
alpha <- 0.1
ischlather <- function(h)
  2 * alpha + (1 - alpha) * schlather(h)
sigma2 <- 3
geomGauss <- function(h)
  2 * pnorm(sqrt(sigma2 * (1 - cov.fun(h)) / 2))

plot(smith, from = 0, to = 5, xlab = "h", ylab = expression(theta(h)))
plot(geomGauss, add = TRUE, col = 2, from = 0, to = 5)
plot(ischlather, add = TRUE, col = 3, from = 0, to = 5)
plot(schlather, add = TRUE, col = 4, from = 0, to = 5)
legend("bottomright", c("Smith",  "Geom. Gauss", "Ind. Schlather", "Schlather"),
       col = 1:4, lty = 1, inset = 0.05)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,height=3.5>>=
<<extCoeffModels>>
@ 
\caption{Extremal coefficient functions for different max-stable
  models. $\Sigma$ is the $2\times 2$ identity matrix. Correlation
  function: Whittle--Matérn with $c_1 = c_2 = \nu = 1$. $\alpha =
  0.1$. $\sigma^2 = 3$.}
  \label{fig:extCoeffModels}
\end{figure}

Figure~\ref{fig:extCoeffModels} plots the extremal coefficient
function for the different max-stable models introduced. The Smith
model covers the whole range of dependence while Schlather's model has
an upper bound of $1 + \sqrt{1/2}$. Similarly, the independent
Schlather model has an upper bound of $1 + \sqrt{1/2} + \alpha (1 -
\sqrt{1/2})$ but induces a jump of $\alpha$ at the origin. Lastly, the
geometric Gaussian model has an upper bound of $2
\Phi(\sqrt{\sigma^2/2})$ but does not induce any jump at $h=0$. From a
practical point of view, when $\sigma^2 \geq 15$, the geometric
Gaussian model covers the whole range of dependence.


\citet{Schlather2003} prove several interesting properties for
$\theta(\cdot)$:
\begin{enumerate}
\item $2 - \theta(\cdot)$ is a semi-definite positive function;
\item $\theta(\cdot)$ isn't differentiable at $0$ unless $\theta
  \equiv 1$;
\item if $d \geq 2$ and $Z(\cdot)$ is isotropic, $\theta(\cdot)$ has
  at most one jump at $0$ and is continuous elsewhere.
\end{enumerate}

These properties have strong consequences. The first indicates that
the spatial dependence structure of a stationary max-stable process
can be characterised by a correlation function. The second states that
a valid correlation functions does not always lead to a valid extremal
coefficient function. For instance, the Gaussian correlation model,
$\rho(h) = \exp(-h^2)$, $h\geq 0$, is not allowed since it is
differentiable at $h = 0$.

equation~\eqref{eq:extcoeff} forms the basis for a simple maximum
likelihood estimator. Suppose we have $Z_1(\cdot)$, \ldots,
$Z_n(\cdot)$, independent replications of $Z(\cdot)$ observed at a set
$A = \{x_1, \ldots, x_{|A|} \}$ of locations. The log-likelihood based
on equation~\eqref{eq:extcoeff} is given by:
% \begin{equation}
%   \label{eq:llikSTNaive}
%   \ell_A(\theta_A) = \#\left\{i: \max_{j \in A} \left(Z_i(x_j)
%       \overline{Z(x_j)} \right) > 0 \right\} \log \theta_A - \theta_A
%   \sum_{i=1}^{n} \frac{1}{\max_{j \in A} \left( Z_i(x_j)
%       \overline{Z(x_j)} \right)}
% \end{equation}
\begin{equation}
  \label{eq:llikSTNaive}
  \ell_A(\theta_A) = n \log \theta_A - \theta_A \sum_{i=1}^{n}
  \frac{1}{\max_{j \in A} \left\{ Z_i(x_j) \overline{Z(x_j)} \right\}}
\end{equation}
where the terms of the form $\log \max_{j\in A}\{Z_i(x_j)\}$ were
omitted and $\overline{Z(x_j)} = n^{-1} \sum_{i=1}^n 1 /
Z_i(x_j)$. The scalings by $\overline{Z(x_j)}$ are here to ensure that
$\hat{\theta}_A = 1$ when $|A| = 1$.

The problem with the MLE based on equation~\eqref{eq:llikSTNaive} is
that the extremal coefficient estimates may not have the properties on
the extremal coefficient function stated above. To solve this,
\citet{Schlather2003} propose self consistent estimators for
$\theta_A$ by either sequentially correcting the estimates obtained by
equation~\eqref{eq:llikSTNaive} or by modifying the log-likelihood to
ensure these properties.% However, these adjustements seem of poor
% value toward their numerical complexity.

\citet{Smith1991} proposed another estimator for the pairwise extremal
coefficients. As $Z(x)$ is unit Fréchet for all $x \in \mathbb{R}^d$,
$1/Z(x)$ has a unit exponential distribution and, according to
equation~\eqref{eq:extcoeffPairwise}, $1 / \max\{Z(x_1), Z(x_2)\}$ has
an exponential distribution with rate $\theta(x_1 - x_2)$. By the law
of large numbers $\sum_{i=1}^n 1/Z_i(x_1) = \sum_{i=1}^n 1/Z_i(x_2)
\approx n$\footnote{In fact, these relations are exact if the margins
  were transformed to unit Fréchet by using the maximum likelihood
  estimates.}, this suggests a simple estimator for the extremal
coefficient between locations $x_1$ and $x_2$:
\begin{equation}
  \label{eq:extCoeffSmith}
  \hat{\theta}(x_1 - x_2) = \frac{n}{\sum_{i=1}^n \min
    \{Z_i(x_1)^{-1}, Z_i(x_2)^{-1} \}}
\end{equation}

<<label=extCoeffST-Smith,echo=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)
cov.fun <- covariance(sill = 1, range = 1, smooth = 1, cov.mod = "whitmat",
                      plot = FALSE)
ext.coeff <- function(h)
  1 + sqrt((1 - cov.fun(h))/2)

par(mfrow=c(1,2))
ST <- fitextcoeff(ms0, cbind(x, y), loess = FALSE, ylim = c(1, 2))
plot(ext.coeff, from = 0, to = 12, col = 2, add = TRUE, lwd = 1.5)
Smith <- fitextcoeff(ms0, cbind(x, y), estim = "Smith", loess = FALSE, ylim = c(1, 2))
plot(ext.coeff, from = 0, to = 12, col = 2, add = TRUE, lwd = 1.5)
@ 

\begin{figure}
  \centering
<<echo=FALSE,fig=TRUE,width=7,height=3.5>>=
<<extCoeffST-Smith>>
@   
\caption{Pairwise extremal coefficient estimates from the Schlather
  and Tawn (left panel) and Smith (right panel) estimators from a
  simulated max-stable random field having a Whittle--Matérn
  correlation function - $c_1 = c_2 = \nu = 1$. The red lines are the
  theoretical extremal coefficient function.}
  \label{fig:extCoeffST-Smith}
\end{figure}

Figure~\ref{fig:extCoeffST-Smith} plots the pairwise extremal
coefficient estimates from a simulated Schlather model having a
Whittle--Matérn correlation function using
equations~\eqref{eq:llikSTNaive} and \eqref{eq:extCoeffSmith}. This
figure was generated using the following code:

<<eval=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)
par(mfrow=c(1,2))
fitextcoeff(ms0, cbind(x, y), loess = FALSE)
fitextcoeff(ms0, cbind(x, y), estim = "Smith", loess = FALSE)
@ 

\section{Madogram-based approaches}
\label{sec:madogram-based}

As we already stated, variograms are useful quantities to assess the
degree of spatial dependence for Gaussian processes. However their use
for extreme observations is limited as variograms may not exist. To
see this, consider a stationary max-stable process with unit Fréchet
margins. For such random processes, both mean and variance are not
finite and the variogram does not exist theoretically, so we need
extra work to get reliable variogram-based tools.

\subsection{Madogram}
\label{sec:madogram}

A standard tool in geostatistics, similar to the variogram, is the
madogram \citep{Matheron1987}. The madogram is
\begin{equation}
  \label{eq:madogram}
  \nu(x_1 - x_2) = \frac{1}{2} \mathbb{E}\left[ |Z(x_1) - Z(x_2)|
  \right],
\end{equation}
where $Z(\cdot)$ is a stationary random field with mean assumed
finite.

The problem with the madogram is almost identical to the one we
emphasized with the variogram as unit Fréchet random variables have no
finite mean. This led \citet{Cooley2006} to consider identical GEV
margins with shape parameter $\xi < 1$ to ensure that the mean, and
even the variance, are finite. It is possible to use the same strategy
to ensure that the variogram exists theoretically but, as we will show
later, we will see that working with the madogram is particularly
suited for extreme values and has strong links with the extremal
coefficient.

By using simple arguments and some results obtained by
\citet{Hosking1985} on probability weighted moments,
\citet{Cooley2006} show that
\begin{equation}
  \label{eq:mado2ExtCoeff}
  \theta(x_1 - x_2) =
  \begin{cases}
    u_\beta \left(\mu + \frac{\nu(x_1 - x_2)}{\Gamma(1 - \xi)}\right),
    & \xi < 1, \xi \neq 0,\\
    \exp\left(\frac{\nu(x_1 - x_2)}{\sigma}\right), & \xi = 0,
  \end{cases}
\end{equation}
where $\mu$, $\sigma$ and $\xi$ are the location, scale and shape
parameters for the GEV distribution, $\Gamma(\cdot)$ is the Gamma
function and
\begin{equation*}
  u_\beta(x) = \left(1 + \xi \frac{x - \mu}{\sigma} \right)_+^{1/\xi}
\end{equation*}

<<echo=FALSE,label=madogram>>=
cov.fun1 <- covariance(sill = 1, range = 1, smooth = 1,
                       cov.mod = "whitmat", plot = FALSE)
ext.coeff1 <- function(h)
  1 + sqrt((1 - cov.fun1(h))/2)
mado1 <- function(h)
  log(ext.coeff1(h))

cov.fun2 <- covariance(sill = 1, range = 1.5, smooth = 1,
                       cov.mod = "powexp", plot = FALSE)
ext.coeff2 <- function(h)
  1 + sqrt((1 - cov.fun2(h))/2)
mado2 <- function(h)
  log(ext.coeff2(h))

n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)

par(mfrow=c(1,2))
madogram(ms0, cbind(x, y), which = "mado", ylim = c(0, log(2)))
plot(mado1, from = 0, to = 12, add = TRUE, col = 2, lwd = 1.5)
madogram(ms0, cbind(x, y), which = "mado", ylim = c(0, log(2)),
         n.bins = 100, xlim = c(0, 12))
plot(mado1, from = 0, to = 12, add = TRUE, col = 2, lwd = 1.5)
@

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<madogram>>
@   
\caption{Madogram (left panel) and binned madogram (right panel) with
  unit Gumbel margins for the Schlather model with the Whittle--Matérn
  correlation functions. The red lines are the theoretical
  madograms. Points are pairwise estimates.}
  \label{fig:madogram}
\end{figure}

equation~\eqref{eq:madogram} suggests a simple estimator
\begin{equation}
  \label{eq:madogramEstim}
  \hat{\nu}(x_1-x_2) = \frac{1}{2n} \sum_{i=1}^n |z_i(x_1) - z_i(x_2)|
\end{equation}
where $z_i(x_1)$ and $z_i(x_2)$ are the $i$-th observations of the
random field at location $x_1$ and $x_2$ and $n$ is the total number
of observations. If isotropy is assumed, then it might be preferable
to use a ``binned'' version of estimator~\eqref{eq:madogramEstim}
\begin{equation}
  \label{eq:madogramEstimBinned}
  \hat{\nu}(h) = \frac{1}{2n |B_h|} \sum_{(x_1, x_2) \in
    B_h}\sum_{i=1}^n |z_i(x_1) - z_i(x_2)|
\end{equation}
where $B_h$ is the set of pair of locations whose pairwise distances
belong to $[h -\epsilon, h + \epsilon[$, for $\epsilon > 0$.

Figure~\ref{fig:madogram} plots the theoretical madograms for the
Schlather's model having a Whittle--Matérn correlation
function. Pairwise and binned pairwise estimates as defined by
equations~\eqref{eq:madogramEstim} ans~\eqref{eq:madogramEstimBinned}
are also reported. The code used to generate these madogram estimates
was
<<eval=FALSE>>=
n.site <- 50
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)

par(mfrow=c(1,2))
madogram(ms0, cbind(x, y), which = "mado")
madogram(ms0, cbind(x, y), which = "mado", n.bins = 100)
@ 

Using a plugin estimate in equation~\eqref{eq:mado2ExtCoeff} leads to
a simple estimator for $\theta(\cdot)$:
\begin{equation}
  \label{eq:extCoeffEstMado}
  \hat{\theta}(x_1 - x_2) =
  \begin{cases}
    u_\beta \left(\mu + \frac{\hat{\nu}(x_1 - x_2)}{\Gamma(1 -
        \xi)}\right), & \xi < 1, \xi \neq 0,\\
    \exp\left(\frac{\hat{\nu}(x_1 - x_2)}{\sigma}\right), & \xi = 0,
  \end{cases}
\end{equation}

<<label=madogramExtCoeff,echo=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)

cov.fun <- covariance(sill = 1, range = 1, smooth = 1, cov.mod = "whitmat",
                      plot = FALSE)
ext.coeff <- function(h)
  1 + sqrt((1 - cov.fun(h))/2)
mado <- function(h)
  log(ext.coeff(h))

par(mfrow=c(1,2))
madogram(ms0, cbind(x, y), which = "mado", ylim = c(0, log(2)))
plot(mado, from = 0, to = 12, col = 2, lwd = 1.5, add = TRUE)
madogram(ms0, cbind(x, y), which = "ext", ylim = c(1, 2))
plot(ext.coeff, from = 0, to = 12, col = 2, lwd = 1.5, add = TRUE)
@ 

\begin{figure}
  \centering
<<echo=FALSE,fig=TRUE,width=7,height=3.5>>=
<<madogramExtCoeff>>
@   
\caption{Pairwise madograms (left panel) and extremal coefficients
  (right panel) estimates from a simulated max-stable random field
  having a Whittle--Matérn correlation function - $c_1 = c_2 = \nu =
  1$. The red lines are the theoretical madogram and extremal
  coefficient function.}
  \label{fig:madoExtCoeff}
\end{figure}

Figure~\ref{fig:madoExtCoeff} plots the madogram and extremal
coefficient functions estimated from a simulated max-stable process
with unit Fréchet margins and having a Whittle--Matérn correlation
function. These estimates were obtained by using
equations~\eqref{eq:madogramEstim} and \eqref{eq:extCoeffEstMado}
respectively. The figure was generated using the code below.

<<eval=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="wh", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)
madogram(ms0, cbind(x, y))
@ 

\subsection{$F$-Madogram}
\label{sec:f-madogram}

In the previous subsection, we introduced the madogram as a summary
statistic for the spatial dependence structure. However, the choice of
the GEV parameters to compute this madogram is somewhat
arbritrary. Instead, \citet{Cooley2006} propose a modified madogram
called the $F$-madogram
\begin{equation}
  \label{eq:F-madogram}
  \nu_F(x_1 - x_2) = \frac{1}{2} \mathbb{E}
  \left[|F\left(Z(x_1)\right) - F\left(Z(x_2)\right)| \right]
\end{equation}
where $Z(\cdot)$ is a stationary max-stable random field with unit
Fréchet margins and $F(z) = \exp(-1/z)$.

<<echo=FALSE,label=F-madogram>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
sigma <- matrix(c(1, 0, 0, 1),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
set.seed(12)
ms0 <- MaxStableRF(x, y, grid = FALSE, model = model, maxstable = "Bool",
                   n = n.obs)
ms0 <- t(ms0)

ext.coeff <- function(h)
  2 * pnorm(h/2)
Fmado <- function(h)
  0.5 * (ext.coeff(h) - 1) / (ext.coeff(h) + 1)

par(mfrow=c(1,2))
fmadogram(ms0, cbind(x, y), which = "mado", ylim = c(0, 1/5))
plot(Fmado, from = 0, to = 13, add = TRUE, col = 2, lwd = 1.5)
fmadogram(ms0, cbind(x, y), which = "ext", ylim = c(1, 2.4))
plot(ext.coeff, from = 0, to = 13, add = TRUE, col = 2, lwd = 1.5)
@

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<F-madogram>>
@   
\caption{Pairwise $F$-madogram (left panel) and extremal coefficient
  (right panel) estimates for the Smith model with $\Sigma$ equals to
  the identity matrix. The red lines are the theoretical $F$-madogram
  and extremal coefficient function.}
  \label{fig:F-madogram}
\end{figure}

The $F$-madogram is well defined even in the presence of unit Fréchet
margins as we work with $F(Z(x_1))$ instead of $Z(x_1)$. Obviously,
equation~\eqref{eq:F-madogram} suggests a simple estimator:
\begin{equation}
  \label{eq:F-madogramEstim}
  \hat{\nu}_F(x_1 - x_2) = \frac{1}{2 n} \sum_{i=1}^{n}
  |\hat{F}(z_i(x_1)) - \hat{F}(z_i(x_2))|
\end{equation}
where $z_i(x_1)$ and $z_i(x_2)$ are the $i$-th observations of the
random field at location $x_1$ and $x_2$ and $n$ is the total number
of observations.

The $F$-madogram has strong connections with the extremal coefficient
introduced in Section~\ref{sec:extremal-coefficient}. Indeed, we have
\begin{equation}
  \label{eq:F-mado2ExtCoeff}
  2 \nu_F(x_1 - x_2) = \frac{\theta(x_1 - x_2) - 1}{\theta(x_1 - x_2)
    + 1}
\end{equation}
or conversely
\begin{equation}
  \label{eq:extCoeff2F-mado}
  \theta(x_1 - x_2) = \frac{1 + 2 \nu_F(x_1 - x_2)}{1 - 2 \nu_F(x_1 -
    x_2)}
\end{equation}

\begin{proof}
  Let first note that
  \begin{equation}
    \label{eq:abs2max}
    |x - y| = 2 \max\left\{x, y \right\} - (x + y)
  \end{equation}

  Using equation~\eqref{eq:abs2max} in equation~\eqref{eq:F-madogram},
  we have:
  \begin{eqnarray*}
    \nu_F(x_1 - x_2) &=& \frac{1}{2} \mathbb{E}
    \left[|F\left(Z(x_1)\right) - F\left(Z(x_2)\right)| \right]\\
    &=& \mathbb{E}\left[\max\left\{F\left(Z(x_1)\right),
        F\left(Z(x_2)\right) \right\} \right] - \mathbb{E}\left[
      F\left(Z(x_1) \right) \right]\\
    &=& \mathbb{E}\left[F\left(\max\{Z(x_1), Z(x_2)\} \right)\right] -
    \frac{1}{2}
  \end{eqnarray*}
  where we used the fact that $F(Z(x_1))$ is uniformly distributed on
  $[0, 1]$ and $F$ is monotone increasing. Now, from
  Section~\ref{sec:extremal-coefficient}, we know that
  \begin{equation*}
    \Pr \left[ \max\left\{ Z(x_1), Z(x_2) \right\}\leq z \right] = \exp
    \left(- \frac{\theta(x_1 - x_2)}{z} \right)
  \end{equation*}
  so that
  \begin{eqnarray*}
    \mathbb{E}\left[F\left(\max\{Z(x_1), Z(x_2)\} \right) \right] &=&
    \theta(x_1 - x_2) \int_{0}^{+\infty} \exp\left(-\frac{1}{z} \right)
    \exp\left(-\frac{\theta(x_1 - x_2)}{z} \right) z^{-2}\mbox{dz}\\
    &=& \frac{\theta(x_1 - x_2)}{\theta(x_1 - x_2) + 1}
  \end{eqnarray*}
  This proves equations~\eqref{eq:F-mado2ExtCoeff}
  and~\eqref{eq:extCoeff2F-mado}.
\end{proof}

As we can see from equation~\eqref{eq:extCoeff2F-mado}, there is a
one-to-one relationship between the extremal coefficient and the
$F$-madogram. This suggests a simple estimator for $\theta(x_1 - x_2)$
\begin{equation}
  \label{eq:extCoeffEstF-mado}
  \hat{\theta}(x_1 - x_2) = \frac{1 + 2 \hat{\nu}_F(x_1 - x_2)}{1 - 2
    \hat{\nu}_F(x_1 - x_2)}
\end{equation}

Figure~\ref{fig:F-madogram} plots the pairwise $F$-madogram and
extremal coefficient estimates from $100$ replications of the
isotropic Smith model. The code used to produce this figure was:
<<eval=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
sigma <- matrix(c(1, 0, 0, 1),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
set.seed(12)
ms0 <- MaxStableRF(x, y, grid = FALSE, model = model, maxstable = "Bool",
                   n = n.obs)
ms0 <- t(ms0)

par(mfrow=c(1,2))
fmadogram(ms0, cbind(x, y))
@ 

As with the madogram presented in the previous Section, it is also
possible to have binned estimates of the $F$-madogram by passing the
argument \verb|n.bins| into the \verb|fmadogram| function.

\subsection{$\lambda$-Madogram}
\label{sec:lambda-madogram}

The extremal coefficient, and thus the ($F$-)madogram, does not fully
characterize the spatial dependence of a random field. Indeed, from
equation~\eqref{eq:extcoeff}, it only considers $\Pr[Z(x_1) \leq z_1,
Z(x_2) \leq z_2]$ where $z_1 = z_2 = z$. To solve this issue,
\citet{Naveau2009} introduce the $\lambda$-madogram defined as
\begin{equation}
  \label{eq:lambda-madogram}
  \nu_{\lambda}(x_1 - x_2) = \frac{1}{2} \mathbb{E} \left[
    |F^{\lambda}\{Z(x_1)\} - F^{1-\lambda}\{Z(x_2)\}|\right]
\end{equation}
for any $\lambda \in [0, 1]$.

The idea beyond this is that by varying $\lambda$, we will focus on
$\Pr[Z(x_1) \leq z_1, Z(x_2) \leq z_2]$ where $z_1 = \lambda z$ and
$z_2 = (1 - \lambda) z$ and thus explore the whole space. The
$\lambda$-madogram is related to the exponent measure $V$, namely
\begin{equation}
  \label{eq:lambda-madogramVfun}
  \nu_{\lambda}(x_1 - x_2) = \frac{V_{x_1,x_2}(\lambda,
    1-\lambda)}{1+V_{x_1,x_2}(\lambda, 1-\lambda)} - c(\lambda)
\end{equation}
where $c(\lambda) = 3/\{2(1+\lambda)(2-\lambda)\}$.

\begin{proof}
  Applying equation~\eqref{eq:abs2max} with $x =
  F^{\lambda}\{Z(x_1)\}$ and $y = F^{1-\lambda}\{Z(x_2)\}$, we have
  \begin{eqnarray*}
    \nu_{\lambda}(x_1 - x_2) &=& \mathbb{E}\left[\max\{F^{\lambda}
      \{Z(x_1)\}, F^{1-\lambda}\{Z(x_2)\} \} \right] - \mathbb{E}\left[
      F^{\lambda} \{Z(x_1)\} \right] - \mathbb{E}\left[F^{1-\lambda}
      \{Z(x_2)\} \right]\\
    &=& \mathbb{E}\left[\max\{F^{\lambda} \{Z(x_1)\},
      F^{1-\lambda}\{Z(x_2)\} \} \right] - \frac{1}{2(1+\lambda)} -
    \frac{1}{2(2 - \lambda)}
  \end{eqnarray*}
  where we used the fact that $\mathbb{E}[X^k] = 1/(1+k)$, $X \sim
  U(0,1)$, $k>0$. From Section~\ref{sec:extremal-coefficient} we know
  that
  \begin{eqnarray*}
    \Pr\left[\max\{F^{\lambda} \{Z(x_1)\}, F^{1-\lambda}\{Z(x_2)\} \}
      \leq z \right] &=& \Pr \left[Z(x_1) \leq -\frac{\lambda}{\log
        z}, Z(x_2) \leq -\frac{1 - \lambda}{\log z} \right]\\
    &=& \exp\left\{-\log (z) V_{x_1,x_2}\left(\lambda, 1 - \lambda
      \right) \right\}
  \end{eqnarray*}
  where $V_{x_1,x_2}$ is the homogeneous function of order $-1$
  introduced in equation~\eqref{eq:MEVD}. Differentiating this
  distribution with respect to $z$ gives the probability density
  function of the random variable $\max\{F^{\lambda} \{Z(x_1)\},
  F^{1-\lambda}\{Z(x_2)\} \}$, so that we have
  \begin{eqnarray*}
    \mathbb{E}\left[\max\{F^{\lambda} \{Z(x_1)\},
      F^{1-\lambda}\{Z(x_2)\} \} \right] &=& \int_0^1
    -V_{x_1,x_2}(\lambda, 1-\lambda) \exp\left\{-\log (z)
      V_{x_1,x_2}\left(\lambda, 1 - \lambda \right) \right\}
    \mbox{dz}\\
    &=& \frac{V_{x_1,x_2}(\lambda, 1-\lambda)}{1+V_{x_1,x_2}(\lambda,
      1-\lambda)}
  \end{eqnarray*}
  and finally
  \begin{equation*}
    \nu_{\lambda}(x_1 - x_2) = \frac{V_{x_1,x_2}(\lambda,
      1-\lambda)}{1+V_{x_1,x_2}(\lambda, 1-\lambda)} - c(\lambda)
  \end{equation*}
  where $c(\lambda) = 3/\{2(1+\lambda)(2-\lambda)\}$.
\end{proof}

Again there is a one-to-one relationship between $\nu_\lambda$ and the
dependence measure, so that we can express $V_{x_1,x_2}$ as a function
of $\nu_\lambda$
\begin{equation}
  \label{eq:Vfun2lambda-mado}
  V_{x_1,x_2}(\lambda, 1-\lambda) = \frac{c(\lambda) + \nu_\lambda(x_1
    - x_2)}{1 - c(\lambda) -\nu_\lambda(x_1 - x_2)}
\end{equation}

As $V_{x_1,x_2}(0.5, 0.5) = 2 \theta(x_1 - x_2)$, the previous
equation induces that the madogram and the $F$-madogram are special
cases of the $\lambda$-madogram when $\lambda = 0.5$. For instance, we
have
\begin{equation*}
  \nu_{0.5}(x_1 - x_2) = \frac{8 \nu_F(x_1 - x_2)}{3\{3 + 2 \nu_F(x_1 -
    x_2)\}}
\end{equation*}

Equation~\eqref{eq:lambda-madogram} suggests a simple estimator
\begin{equation}
  \label{eq:lambda-madogramEst}
  \hat{\nu}_{\lambda}(x_1 - x_2) = \frac{1}{2n} \sum_{i=1}^n
  |\hat{F}^{\lambda}\{z_i(x_1)\} - \hat{F}^{1-\lambda}\{z_i(x_2)\}|
\end{equation}
where $z_i(x_1)$ and $z_i(x_2)$ are the $i$-th observations of the
random field at location $x_1$ and $x_2$, $n$ is the total number of
observations and $\hat{F}$ is an estimate of the CDF at the specified
location. 

There is an issue with the previous estimator. Indeed, the boundary
conditions for the $\lambda$-madogram when $\lambda = 0$ or $\lambda =
1$ are not always fulfilled. From equation~\eqref{eq:lambda-madogram},
if $\lambda = 0$ or $\lambda = 1$, $\nu_\lambda(x_1 - x_2) = 1/4$. If
$F$ is estimated by MLE, then this condition fails while if
$\hat{F}(x_{i:n}) = i / (n+1)$ we get the required conditions. To
bypass this hurdle, \citet{Naveau2009} propose the following adjusted
estimator
\begin{eqnarray*}
  \hat{\nu}_{\lambda}(x_1 - x_2) &=& \frac{1}{2n} \sum_{i=1}^n
  |\hat{F}^{\lambda}\{z_i(x_1)\} - \hat{F}^{1-\lambda}\{z_i(x_2)\}| -
  \frac{\lambda}{2n} \sum_{i=1}^n \left[1 -
    \hat{F}^\lambda\{z_i(x_1)\} \right]\\
  &-& \frac{1 - \lambda}{2n} \sum_{i=1}^n \left[1 -
    \hat{F}^{1-\lambda}\{z_i(x_2)\}\right] + \frac{1 - \lambda +
    \lambda^2}{2 (2 - \lambda) (1 + \lambda)}
\end{eqnarray*}

By plug in this estimator into equation~\eqref{eq:Vfun2lambda-mado} we
get an estimator for the dependence measure
\begin{equation}
  \label{eq:VfunEst}
  \hat{V}_{x_1,x_2}(\lambda, 1-\lambda) = \frac{c(\lambda) +
    \hat{\nu}_\lambda(x_1 - x_2)}{1 - c(\lambda)
    -\hat{\nu}_\lambda(x_1 - x_2)}
\end{equation}

<<label=lambda-madogram,echo=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="stable", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=FALSE, model="cauchy", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms1 <- t(ms1)

par(mfrow=c(1,2), pty = "s")
lmadogram(ms0, cbind(x, y), n.bins = 60)
lmadogram(ms1, cbind(x, y), n.bins = 60)
@ 

\begin{figure}
  \centering
<<echo=FALSE,fig=TRUE,width=7,height=3.5>>=
<<lambda-madogram>>
@ 
\vspace{-1cm}
\caption{Binned $\lambda$-madogram estimates for two Schlather models
  having a powered exponential (right panel) and a Cauchy covariance
  (left panel) functions. $c_1 = c_2 = \nu = 1$.}
  \label{fig:lambda-mado}
\end{figure}

Figure~\ref{fig:lambda-mado} plots the binned $\lambda$-madogram
estimates from $100$ replications of the Schlather model having a
powered exponential and a Cauchy correlation functions. The code used
to produce this figure was:

<<eval=FALSE>>=
n.site <- 40
n.obs <- 100
x <- runif(n.site, 0, 10)
y <- runif(n.site, 0, 10)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=FALSE, model="stable", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=FALSE, model="cauchy", param=c(0,1,0,1, 1),
                   maxstable="extr", n = n.obs)
ms1 <- t(ms1)

par(mfrow=c(1,2))
lmadogram(ms0, cbind(x, y), n.bins = 60)
lmadogram(ms1, cbind(x, y), n.bins = 60)
@ 

It might be useful to use the excellent \emph{persp3d} function
provided by the contributed \emph{rgl} R package to explore
dynamically the $\lambda$-madogram.

\chapter{Fitting a Unit Fréchet Max-Stable Process to Data}
\label{chat:fit-maxstab-frech}

In Chapter~\ref{cha:an-introduction-max}, we described max-stable
processes and gave two different characterisations of them. However,
we mentioned that only the bivariate distributions are analytically
known so that the fitting of max-stable processes to data is not
straightforward. In this Chapter, we will present two different
approaches to fitting max-stable processes to data. The first one is
based on least squares and was first introduced by
\citet{Smith1991}. The second one uses the maximum composite
likelihood estimator \citep{Lindsay1988}, more precisely the maximum
pairwise likelihood estimator. We will consider these two different
approaches separately.

\section{Least Squares}
\label{sec:least-squares}

As stated by equations~\eqref{eq:smith} and~\eqref{eq:schlather}, the
density of a max-stable process is analytically known only for the
bivariate case so that maximum likelihood estimators are not
available. This observation led \citet{Smith1991} to propose an
estimator based on least squares. This fitting procedure consists in
minimizing the objective function
\begin{equation}
  \label{eq:extCoeffLeastSquares}
  C(\psi) = \sum_{i<j} \left(\frac{\theta_{i,j} -
      \tilde{\theta}_{i,j}}{s(\tilde{\theta}_{i,j})} \right)^2
\end{equation}
where $\psi$ is the vector parameter of the max-stable process,
$\theta_{i,j}$ is the extremal coefficient predicted from the
max-stable model for stations $i$ and $j$ , $\tilde{\theta}_{i,j}$ is
a semi-parametric estimator defined by
equation~\eqref{eq:extCoeffSmith} for stations $i$ and $j$ and
$s(\tilde{\theta}_{i,j})$ is the standard deviation related to the
estimation of $\tilde{\theta}_{i,j}$, estimated by the jacknife
estimator \citep{Efron1982}.

S.J. Neil, in his M.Phil.\ thesis, suggests the use of this weighted
sum of squares criterion to avoid unsatisfactory fits in regions of
high dependence i.e.\ when $\theta_{i,j}$ is close to 1.

Although \citet{Smith1991} proposed this estimator for his own
max-stable model, there is no restriction in applying it to any of the
max-stable models introduced in
Chapter~\ref{cha:an-introduction-max}. An illustration of this fitting
procedure is given by the following lines:
<<>>=
n.site <- 40
n.obs <- 80
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")
sigma <- matrix(c(9/8, 1, 1, 9/8),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
set.seed(12)
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model = model,
                   maxstable = "Bool", n = n.obs)
ms0 <- t(ms0)
fitcovmat(ms0, locations, marge = "emp")
@ 

This approach suffers from two major drawbacks. First, unless we use
Monte-Carlo techniques, standard errors are not available and because
the observations are far from being normal, the least squares
estimator should be far from efficiency in the way given by the
Cramér-Rao lower bound \citep{Cramer1946} and the Gauss-Markov
theorem. Secondly, for concrete analysis, it is hopeless that the
observed (spatial) annual maxima have unit Fréchet margins so that we
need first to transform the data to the unit Fréchet scale. This
suggests the use of a more flexible estimator.

\section{Pairwise Likelihood}
\label{sec:pairwise-likelihood}

As already stated, the ``full'' likelihood for the max-stable model
presented in Chapter~\ref{cha:an-introduction-max} is not analytically
known if the number of stations under consideration is greater or
equal to three. However, as the bivariate density is analytically
known, this suggests the use of the pairwise likelihood in place of
the full likelihood. The log pairwise-likelihood is given by
\begin{equation}
  \label{eq:lplik}
  \ell_p(\mathbf{z};\psi) = \sum_{i<j} \sum_{k=1}^{n_{i,j}}
  \log f(z_k^{(i)}, z_k^{(j)}; \psi)
\end{equation}
where $\mathbf{z}$ is the data available on the whole region,
$n_{i,j}$ is the number of common observations between sites $i$ and
$j$, $y_k^{(i)}$ is the $k$-th observation of the $i$-th site and
$f(\cdot, \cdot)$ is the bivariate density of the unit Fréchet
max-stable process.

\subsection{Misspecification}
\label{sec:misspecification}

Properties of the maximum composite likelihood estimator are well
known \citep{Lindsay1988,Cox2004} and belong to the class of maximum
likelihood estimation under misspecification\footnote{More rigorously
  we should say \emph{partially specified}.}.

A statistical model $\{f(y;\psi), \psi \in \mathbb{R}^p\}$ is said
\emph{misspecified} if the observations $y_i$, $i=1$, \ldots, $n$ are
drawn from a unknown true density $g$ instead of $f$. We say that the
model $\{f(y;\psi), \psi \in \mathbb{R}^p\}$ is \emph{correct} if
there exists $\psi_* \in \mathbb{R}^d$ such that $f(y;\psi_*) = g(y)$,
for all $y$. 

Let $\hat{\psi}$ be the maximum likelihood estimator. Because of the
law of large numbers, we have
\begin{equation}
  \label{eq:LLNMissMLE}
  \frac{1}{n} \sum_{i=1}^n \log f\left(y_i;\hat{\psi}\right)
  \longrightarrow \int \log f\left(y;\psi_g\right) g(y) \mbox{dy},
  \qquad n \rightarrow +\infty
\end{equation}
where $\psi_g$ is the value that minimizes the Kullback--Leibler
discrepancy defined as
\begin{equation}
  \label{eq:KullbackLeibler}
  D\left(f_\psi, g\right) = \int \log \left( \frac{g(y)}{f(y;\psi)}
  \right) g(y) \mbox{dy}
\end{equation}

By definition of $\hat{\psi}$, we have:
\begin{equation*}
  \frac{1}{n} \sum_{i=1}^n \frac{\partial \log
    f(y_i;\hat{\psi})}{\partial \psi} = 0
\end{equation*}
so that, provided the log-likelihood is regular enough, a Taylor
expansion about $\psi_g$ yields
\begin{eqnarray*}
  && \frac{1}{n} \sum_{i=1}^n \frac{\partial \log f(y_i;\psi_g)}{\partial
    \psi} + \left(\hat{\psi} - \psi_g \right)^T \frac{1}{n} \sum_{i=1}^n
  \frac{\partial^2 \log f(y_i;\psi_g)}{\partial \psi \partial \psi^T}
  \stackrel{\cdot}{=} 0\\
  &\Longleftrightarrow& \hat{\psi} \stackrel{\cdot}{=} \psi_g -
  \left\{ \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log
      f(y_i;\psi_g)}{\partial \psi \partial \psi^T} \right\}^{-1}
  \left\{\frac{1}{n} \sum_{i=1}^n \frac{\partial \log
      f(y_i;\psi_g)}{\partial \psi} \right\}
\end{eqnarray*}

It can be shown using the central limit theorem and the weak law of
large numbers \citep[p.~124]{Davison2003} that the previous equation
implies that
\begin{equation}
  \label{eq:MLEMissp}
  \hat{\psi} \stackrel{\cdot}{\sim} N\left(\psi_g, H(\psi_g)^{-1}
    J(\psi_g) H(\psi_g)^{-1} \right)
\end{equation}
where
\begin{eqnarray}
  \label{eq:Hessian}
  H(\psi_g) &=& n \int \frac{\partial^2 \log f(y;\psi)}{\partial
    \psi \partial \psi^T} g(y) \mbox{dy}\\
  \label{eq:VarScore}
  J(\psi_g) &=& n \int \frac{\partial \log f(y;\psi)}{\partial \psi}
  \frac{\partial \log f(y;\psi)}{\partial \psi^T} g(y) \mbox{dy}
\end{eqnarray}

Note that if by a lucky chance our candidate model $f(y;\psi)$
contains the true one, then $\psi_g = \psi$ and $H(\psi_g) =
-J(\psi_g)$ so that equation~\eqref{eq:MLEMissp} reduces to the usual
asymptotic distribution for the MLE.

The use of pairwise likelihood, as a specific case of composite
likelihood, leads to further simplifications. To see this, we consider
the gradient of the log-pairwise likelihood
\begin{equation}
  \label{eq:gradlplik}
  \nabla \ell_p(\mathbf{y};\psi) = \sum_{i<j} \sum_{k=1}^{n_{i,j}}
  \nabla \log f(y_k^{(i)}, y_k^{(j)}; \psi)
\end{equation}
For each fixed $i$ and $j$,
\begin{equation*}
  \sum_{k=1}^{n_{i,j}} \nabla \log f(y_k^{(i)}, y_k^{(j)}; \psi) = 0
\end{equation*}
is an unbiased estimating equation so that $\nabla
\ell_p(\mathbf{y};\psi) = 0$ is unbiased too as a linear combination
of unbiased estimating equations. This leads to a modification of
equation~\eqref{eq:MLEMissp}
\begin{equation}
  \label{eq:lplikAsymp}
  \hat{\psi_p} \stackrel{\cdot}{\sim} N\left(\psi, H(\psi)^{-1}
    J(\psi) H(\psi)^{-1} \right)
\end{equation}
where $H(\psi)$ and $J(\psi)$ are given by
equations~\eqref{eq:Hessian} and \eqref{eq:VarScore}.

Let consider a simple case study to see how it works in practice. Here
we simulate independent replications of the Schlather model with a
Whittle--Matérn correlation function having its sill, range and shape
parameters equal to $0.8$, $3$ and $1.2$ respectively.
<<>>=
n.site <- 40
set.seed(12)
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="wh",
param = c(0,1, .2,3, 1.2), maxstable="extr", n = 80)
ms0 <- t(ms0)
fitmaxstab(ms0, locations, cov.mod = "whitmat", std.err.type = "none")
@ 

From this output, we can see that we indeed use the Schlather's
representation with a Whittle--Matérn correlation function. The
convergence was successful and the parameter estimates for the
covariance function as well as the pairwise deviance are
accessible. Large deviations from the theoretical values may be
expected as the parameters of the Whittle--Matérn covariance function
are far from orthogonal. Thus, the range and smooth estimates may be
totally different while leading (approximately) to the same covariance
function.

The \verb|fitmaxstab| function provides a powerful option that can fix
any parameter of the model under consideration. For instance, this
could be especially useful when using the Whittle--Matérn correlation
function as it is sometimes preferable to fix the smooth parameter
using prior knowledge on the process smoothness
\citep{Diggle2007}. Obviously, this feature is not restricted to this
specific case and one can consider more exotic models. Fixing
parameters of the model is illustrated by the following lines
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "whitmat", smooth = 1.2,
           std.err.type = "none")
fitmaxstab(ms0, locations, cov.mod = "whitmat", sill = 1)
fitmaxstab(ms0, locations, cov.mod = "whitmat", range = 3)
@ 

Although the Whittle--Matérn model is flexible, one may want to
consider other families of covariance functions. This is achieved by
invoking:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "cauchy")
fitmaxstab(ms0, locations, cov.mod = "powexp")
@ 

One may also consider other types of max-stable models, this could be
done as follows
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss")
fitmaxstab(ms0, locations, cov.mod = "iwhithmat")
fitmaxstab(ms0, locations, cov.mod = "gpowexp")
@ 
where \verb|iwhitmat| stands for the independent Schlather model
with the Whittle--Matérn correlation family and \verb|gpowexp| for the
geometric Gaussian model with the powered exponential correlation
family.

It is also possible to used different optimization routines to fit the
model to data by passing the \verb|method| argument. For instance, if
one wants to use the \verb|BFGS| method:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss", cov12 = 0, method = "BFGS")
@ 

Instead of using the \verb|optim| function, one may want to use the
\verb|nlm| or \verb|nlminb| functions. This is done as before using
the \verb|method = "nlm"| or \verb|method = "nlminb"| option.

Finally, it is important to note that maximizing the pairwise
likelihood can be expensive. The choice of the numerical optimizer is
important. In particular, optimizers that use the gradient of the
pairwise likelihood might be clumsy. Indeed, if the Whittle--Matérn
covariance function is considered and the smooth parameter has to be
estimated, then the pairwise likelihood is not differentiable with
respect to this specific parameter. In general, the Nelder--Mead
\citep{Nelder1965} approach seems to perform better even if the
convergence is sometimes slow.

\subsection{Assessing Uncertainties}
\label{sec:assess-uncert}

Recall that the maximum pairwise likelihood estimator $\hat{\psi}_p$
satisfies
\begin{equation*}
  \hat{\psi}_p \stackrel{\cdot}{\sim} \mathcal{N}\left(\psi,
    H(\psi)^{-1} J(\psi) H(\psi)^{-1} \right), \qquad n \rightarrow
  +\infty,
\end{equation*}
where $H(\psi) = \mathbb{E}[\nabla^2 \ell_p(\psi;\mathbf{Y})]$ (the
Hessian matrix) and $J(\psi) = \mbox{Var}[\nabla
\ell_p(\psi;\mathbf{Y})]$, where the expectations are with respect to
the ``full'' density.

In practice, to get the standard errors we need estimates of $H(\psi)$
and $J(\psi)$. The estimation of $H(\psi)$ is straightforward and is
$\hat{H}(\hat{\psi}_p) = \nabla^2 \ell_p(\hat{\psi}_p;\mathbf{y})$;
that is, the Hessian matrix evaluated at $\hat{\psi}_p$. Usually,
standard optimizers are able to get finite-difference based estimates
for $H(\hat{\psi}_p)$ so that no extra work is needed to get
$\hat{H}(\hat{\psi}_p)$.

The estimation of $J(\hat{\psi}_p)$ is a little bit more difficult and
can be done in two different ways \citep{Varin2005}. First,
$J(\hat{\psi}_p)$ can be estimated using the ``naive'' estimator
$\hat{J}(\hat{\psi}_p) = \nabla \ell_p(\hat{\psi}_p;\mathbf{y})
{\nabla \ell_p(\hat{\psi}_p;\mathbf{y})}^T$. This estimator is tagged
\verb|grad| as it uses the gradient of the log pairwise
likelihood. Another estimator is given by noticing that $J(\psi)$
corresponds to the variance of the pairwise score equations
$\ell_p(\psi;\mathbf{Y}) = 0$. Consequently, a second estimator,
tagged \verb|score|, is given by the sample variance of each
contribution to the pairwise score function. The second estimator is
only accessible if independent replications of $\mathbf{Y}$ are
available\footnote{which will mostly be the case for spatial
  extremes.}.

These two types of standard errors are available by invoking the
following two lines:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss", std.err.type = "score")
fitmaxstab(ms0, locations, cov.mod = "gauss", std.err.type = "grad")
@ 

\chapter{Model Selection}
\label{cha:model-selection}

Model selection plays an important role in statistical
modelling. According to Ockham's razor, given several models that fit
the data equally well, we should focus on simple models rather than
more complex ones. Depending on the models to be compared, several
approaches exist for model selection. In this section, we will present
theory on information criteria such as the Akaike Information
Criterion (AIC) \citep{Akaike1974} and the likelihood ratio statistic
\citep[Sec.~4.5]{Davison2003}. We present these two approaches in
turn.

\section{Takeuchi Information Criterion}
\label{sec:take-inform-crit}

Having two different models, we want to know which one we should
prefer for modelling our data. If two models have exactly the same
maximized log-likelihoods, we should prefer the one which has fewer
parameters because it will have a smaller variance. However, if these
two maximized log-likelihoods only differ by a small amount, does this
small increase worth the price of having additional parameters? To
answer this question, we resort to the Kullback--Leibler discrepancy.

Let consider a random sample $Y_1$, \ldots, $Y_n$ drawn from an
unknown density $g$. Ignoring $g$, we fit a statistical model
$f(y;\psi)$ by maximizing the log-likelihood. The Kullback--Leibler
discrepancy measures the discrepancy of our fitted model $f$ from the
true one $g$
\begin{equation}
  \label{eq:KullbackLeibler}
  D\left(f_\psi, g\right) = \int \log \left(
    \frac{g(y)}{f(y;\psi)} \right) g(y) \mbox{dy}
\end{equation}

The Kullback--Leibler discrepancy is always positive. Indeed, as $x
\mapsto - \log(x)$ is a convex function, Jensen's inequality states
\begin{equation*}
  D\left(f_\psi, g\right) = \int \log \left(
    \frac{g(y)}{f(y;\psi)} \right) g(y) \mbox{dy} \geq -\log \left(
    \int \frac{f(y;\psi)}{g(y)} g(y) \mbox{dy} \right) = 0 
\end{equation*}
where we used the fact that $f(y;\psi)$ is a probability density
function. Furthermore, the lower bound is reached if and only if the
convex function is not strictly convex which only occurs iff
$f(y;\psi) = g(y)$.

Consequently, for model selection, we aim to choose models that
minimize $D(f_\psi, g)$. However, $D(f_\psi, g)$ is not enough
discriminant as several models may satisfy $D(f_\psi, g) = 0$. To
solve this issue, suppose we have an estimate $\hat{\psi}$, we need to
average $D(f_{\hat{\psi}}, g)$ over the distribution of
$\hat{\psi}$. Intuitively, because of their larger sampling variance,
averaging over $\hat{\psi}$ will penalized much more models having a
larger number of parameters than those with fewer parameters.

Let $\psi_g$ be the value that minimizes $D(f_\psi, g)$. A Taylor
expansion of $\log f(y;\hat{\psi})$ around $\psi_g$ gives
\begin{equation*}
  \log f(y;\hat{\psi}) \approx \log f(y;\psi_g) + \frac{1}{2}
  \left(\hat{\psi} - \psi_g \right)^T \frac{\partial^2 \log
    f(y;\psi_g)}{\partial \psi \partial \psi^T}
  \left(\hat{\psi} - \psi_g \right)
\end{equation*}
where we use the fact that $\psi_g$ minimise the Kullback--Leibler
discrepancy so its partial derivative with respect to $\psi$
vanishes. By putting this into equation~\eqref{eq:KullbackLeibler} we
get
\begin{eqnarray*}
  D\left(f_{\hat{\psi}},g\right) &\stackrel{\cdot}{=}&
  D\left(f_{\psi_g} \right) - \frac{1}{2 n}
  \mbox{tr}\left\{(\hat{\psi} - \psi_g) (\hat{\psi} -
    \psi_g)^T J(\psi_g)\right\}
\end{eqnarray*}
where $J(\psi_g)$ is given by equation~\eqref{eq:VarScore}. Finaly, we
have
\begin{equation}
  \label{eq:KLAverageOverPsiHat}
  n \mathbb{E}_g \left[D\left(f_{\hat{\psi}},g\right) \right]
  \stackrel{\cdot}{=} n D\left(f_{\psi_g},g\right) - \frac{1}{2} 
  \mbox{tr}\left\{J(\psi_g)^{-1} H(\psi_g) \right\}    
\end{equation}
where $H(\psi_g)$ is given by equation~\eqref{eq:Hessian}.

The AIC is, up to constant, an estimator of
equation~\eqref{eq:KLAverageOverPsiHat} and is defined as
\begin{equation}
  \label{eq:AIC}
  \mbox{AIC} = - 2 \ell(\hat{\psi}) + 2 p
\end{equation}
The simplification $\mbox{tr}\{J(\psi_g)^{-1} H(\psi_g) \} = -p$
arises as the AIC supposes that there is no misspecification.

Because we see in Section~\ref{sec:misspecification} that by using the
maximum pairwise likelihood estimator we work under misspecification,
the AIC is not appropriate. Another estimator of
equation~\eqref{eq:KLAverageOverPsiHat}, which allows for
misspecification, is the Takeuchi information criterion (TIC)
\begin{equation}
  \label{eq:TIC}
  \mbox{TIC} = - 2 \ell(\hat{\psi}) - 2 \mbox{tr}\left\{\hat{J}
    \hat{H}^{-1} \right\}
\end{equation}
In accordance with the AIC, the best model will correspond to the one
that minimizes equation~\eqref{eq:TIC}. Recently, \citet{Varin2005}
rediscover this information criterion and demonstrate its use for
model selection when composite likelihood is involved.

In practice, one can have a look at the output of the
\verb|fitmaxstab| function or use the \verb|TIC| function.

<<>>=
set.seed(1)
n.site <- 40
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="cauchy",
                   param=c(0,1,0.2,3, 1.2), maxstable="extr", n = 80)
ms0 <- t(ms0)
model1 <- fitmaxstab(ms0, locations, cov.mod = "powexp")
model2 <- fitmaxstab(ms0, locations, cov.mod = "cauchy")
TIC(model1, model2)
@ 

The TIC for \verb|model2| is lower that the one for \verb|model1| so
that we might prefer using \verb|model2|.


\section{Likelihood Ratio Statistic}
\label{sec:likel-ratio-stat}

TIC is useful when comparing different models. When dealing with
nested models, one may prefer using the likelihood ratio statistic
because of the Neyman--Pearson lemma \citep{Neyman1933}.

Suppose we are interested in a statistical model $\{f(x;\psi)\}$ where
$\psi^T = (\kappa^T, \phi^T)$ and more especially wether a particular
value $\kappa_0$ of $\kappa$ is relevant with our data. Let
$(\hat{\kappa}^T, \hat{\phi}^T)$ be the maximum likelihood estimate
for $\psi$ and $\hat{\phi}_{\kappa_0}$ the maximum likelihood estimate
under the restriction $\kappa = \kappa_0$. A common statistic to check
if $\kappa_0$ is relevant or not is the likelihood ratio statistic $
W(\kappa_0)$ which satisfies, under mild regularity conditions,
\begin{equation}
  \label{eq:likRatio}
  W(\kappa_0) = 2 \left\{\ell(\hat{\kappa},\hat{\phi}) - \ell(\kappa_0,
    \hat{\phi}_{\kappa_0}) \right\} \longrightarrow \chi_p^2, \qquad n
    \rightarrow +\infty
\end{equation}
where $p$ is the dimension of $\kappa_0$.

Unfortunately, when working under misspecification, the usual
asymptotic $\chi^2_p$ distribution does not hold anymore. There's two
ways to solve this issue: (a) adjusting the $\chi^2$ distribution
\citep{Rotnitzki1990} or (b) adjusting the composite likelihood so
that the usual $\chi^2_p$ holds \citep{Chandler2007}. We will consider
these two strategies in turn.

\subsection{Adjusting the $\chi^2$ distribution}
\label{sec:adjust-chi2-distr}

If the model is misspecified, equation~\eqref{eq:likRatio} has to be
adjusted. More precisely, as stated by \citep{Kent1982}, we have
\begin{equation}
  \label{eq:likRatioMissp}
  W(\kappa_0) = 2 \left\{\ell(\hat{\kappa},\hat{\phi}) - \ell(\kappa_0,
    \hat{\phi}_{\kappa_0}) \right\} \longrightarrow \sum_{i=1}^p
  \lambda_i X_i, \qquad n \rightarrow +\infty
\end{equation}
where $\lambda_i$ are the eigenvalues of ($(H^{-1} J H^{-1})_\kappa
\{-(H^{-1})_\kappa\}^{-1}$, the $X_i$ are independent $\chi_1^2$
random variables and $(H^{-1} J H^{-1})_\kappa$ and $(H^{-1})_\kappa$
are the submatrices of $H^{-1} J H^{-1}$ and $H^{-1}$ corresponding to
the elements of $\kappa$ and where the matrices $H$ and $J$ are given
by equations~\eqref{eq:Hessian} and \eqref{eq:VarScore}. For practical
purposes, the matrices $H$ and $J$ are substituted for their
respective estimates as described in Section~\ref{sec:assess-uncert}.

The problem with equation~\eqref{eq:likRatioMissp} is that generally
the distribution of $\sum_{i=1}^p \lambda_i X_i$ is not known
exactly. This led \citet{Rotnitzki1990} to consider $p W(\kappa_0) /
\sum_{i=1}^p \lambda_i$ as a $\chi_p^2$ random variable. However, a
better approximation uses results on quadratic forms of normal random
distribution.

An application of this approach is given by the following lines:
<<>>=
set.seed(7)
n.site <- 30
locations <- matrix(rnorm(2*n.site, sd = sqrt(.2)), ncol = 2)
colnames(locations) <- c("lon", "lat")
sigma <- matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model=model,
                   maxstable = "Bool", n = 50)
ms0 <- t(ms0)

M0 <- fitmaxstab(ms0, locations, "gauss", , cov11 = 100)
M1 <- fitmaxstab(ms0, locations, "gauss")
anova(M0, M1)
@ 

From this ouput, we can see that the $p$-value is approximately $0.58$
which turns out to be in favour of $H_0$ i.e. $\sigma_{11} = 100$ in
$\Sigma$. Note that the eigenvalue estimate was also reported.

\subsection{Adjusting the composite likelihood}
\label{sec:adjust-comp-likel}

Contrary to the approach of \citet{Rotnitzki1990},
\citet{Chandler2007} propose to adjust the composite likelihood
instead of adjusting the asymptotic likelihood ratio statistic
distribution. The starting point is that, under misspecification, the
log-composite likelihood evaluated at its maximum likelihood estimate
$\hat{\psi}$ has curvature $-\hat{H}^{-1}$ while it should be
$\hat{H}^{-1} \hat{J} \hat{H}^{-1}$. This forms the basis for
adjusting the log-composite likelihood is the following way,
\begin{equation}
  \label{eq:lclik}
  \ell_{adj}(\psi) = \ell(\psi^*), \qquad \psi^* = \hat{\psi}
  + C (\psi - \hat{\psi})
\end{equation}
for some $p\times p$ matrix $C$.

It is straightforward to see that $\hat{\psi}$ maximizes $\ell_A$ with
zero derivative. The key point is that its curvature at $\hat{\theta}$
is $C^T \hat{H}^{-1} C$. By choosing an appropriate $C$ matrix, it is
possible to ensure that $\ell_A$ has the right curvature for
applying~\eqref{eq:likRatio} directly. More precisely, by letting
\begin{equation}
  \label{eq:adjustMatrix}
  C = M^{-1} M_A
\end{equation}
where $M^T M = \hat{H}$ and $M_A^T M_A = \hat{H}^{-1} \hat{J}
\hat{H}^{-1}$, we ensure that $\ell_A$ has curvature $\hat{H}^{-1}
\hat{J} \hat{H}^{-1}$. If $p>1$, the matrix square roots $M$ and $M_A$
are not unique and one may use the Cholesky or the singular value
decompostions.

With this setting, we can apply~\eqref{eq:likRatio} directly i.e.
\begin{equation*}
  W_A(\kappa_0) = 2 \left\{\ell_A(\hat{\kappa}, \hat{\phi}) -
    \ell_A(\kappa_0, \hat{\phi}_A) \right\} \longrightarrow \chi_p^2
\end{equation*}
where $\hat{\phi}_A$ is the maximum adjusted likelihood estimated for
the restricted model.

The problem with the above equation is that it requires the estimation
of $\hat{\phi}_A$ which could be CPU prohibitive. Unfortunately,
substituting $\hat{\phi}$ for $\hat{\phi}_A$ doesn't solve the problem
as $\ell_A(\hat{\phi}) \leq \ell_A(\hat{\phi}_A)$ so that this
substitution will inflate $W_A(\kappa_0)$ and thus
$\Pr_{H_0}[W_a(\kappa_0) > w_a(\alpha)] > 1 - \alpha$, where
$w_a(\alpha)$ is the $1-\alpha$ quantile for the $\chi_p^2$
distribution and $\alpha$ the significance level of the hypothesis
test.

To solve these problems, \citet{Chandler2007} propose to compensate
for the use of $\hat{\phi}$ instead of $\hat{\phi}_A$ by considering a
modified likelihood ratio statistic
\begin{equation}
  \label{eq:8}
  W_A^*(\kappa_0) = 2 c \left\{\ell_A(\hat{\kappa}, \hat{\phi}) -
    \ell_A(\kappa_0, \hat{\phi}_A) \right\} \longrightarrow \chi_p^2
\end{equation}
where 
\begin{equation*}
  c = \frac{(\hat{\kappa} - \kappa_0)^T \{-(\hat{H}^{-1} \hat{J}
    \hat{H}^{-1})_{\kappa_0}\}^{-1} (\hat{\kappa} -
    \kappa_0)}{\{(\hat{\kappa}^T, \hat{\phi}^T)\}^T \{-(\hat{H}^{-1}
    \hat{J} \hat{H}^{-1})_{\kappa_0}\} \{(\hat{\kappa}^T,
    \hat{\phi}^T)\}}
\end{equation*}


The next lines performs the same hypothesis test as that in
Section~\ref{sec:adjust-chi2-distr}.
<<>>=
anova(M0, M1, method = "CB")
@ 

By using the Chandler and Bate methodology, we draw the same
conclusion in the previous section, i.e.\ the observations are
consistent with the null hypothesis $\sigma_{11} = 100$.

\chapter[Fitting a Max-Stable Process to Data]{Fitting a Max-Stable
  Process to Data With Unknown GEV Margins}
\label{cha:fit-maxstab-gev}


In practice, the observations will never be drawn from a unit Fréchet
distribution so that Chapter~\ref{chat:fit-maxstab-frech} won't help
much with concrete applications. One way to avoid this problem is to
fit a GEV to each location and then transform all data to the unit
Fréchet scale. Given a continuous random variable $Y$ whose cumulative
distribution function is $F$, one can define a new random variable $Z$
such as $Z$ is unit Fréchet distributed
\begin{equation}
  \label{eq:Y2Frech}
  Z = - \frac{1}{\log F(Y)}
\end{equation}

More precisely, if $Y$ is a random variable distributed
as a GEV with location, scale and shape parameters equal to $\mu$,
$\sigma$ and $\xi$ respectively, it turns out that
equation~\eqref{eq:GEV2Frech} becomes
\begin{equation}
  \label{eq:GEV2Frech}
  Z = \left(1 + \xi \frac{Y - \mu}{\sigma}\right)^{1/\xi}
\end{equation}

The above transformation can be done by using the \verb|gev2frech|
function
<<>>=
x <- c(2.2975896, 1.6448808, 1.3323833, -0.4464904, 2.2737603, -0.2581876,
       9.5184398, -0.5899699, 0.4974283, -0.8152157)
z <- gev2frech(x, 1, 2, .2)
@
or conversely if $Z$ is a unit Fréchet random variable, then the
random variable $Y$ defined as
\begin{equation}
  \label{eq:frech2GEV}
  Y = \mu + \sigma \frac{Z^{\xi} - 1}{\xi}
\end{equation}
is GEV distributed with location, scale and shape parameter equal to
$\mu \in \mathbb{R}$, $\sigma \in \mathbb{R}_*^+$ and $\xi \in
\mathbb{R}$ respectively.
<<>>=
frech2gev(z, 1, 2, .2)
@ 

The drawback of this approach is that standard errors are incorrect as
the margins are fitted separately from the spatial dependence
structure. Consequently, the standard errors related to the spatial
dependence parameters are underestimated as we suppose that data were
originally unit Fréchet.

One can solve this problem by fitting in \emph{one step} both GEV and
spatial dependence parameters. As the bivariate distributions for the
max-stable models introduced in Chapter~\ref{cha:an-introduction-max}
were imposing unit Fréchet margins, we need to rewrite them for
unknown GEV margins. To this aim, let define the transformation $t$
such that
\begin{equation}
  \label{eq:mapGEV2Frech}
  t: Y(x) \mapsto \left(1 + \xi(x) \frac{Y(x) -
      \mu(x)}{\sigma(x)}\right)^{1/\xi(x)}
\end{equation}
where $Y(\cdot)$ is supposed to be a max-stable random field having
GEV margins such that $Y(x) \sim \mbox{GEV}(\mu(x), \sigma(x),
\xi(x))$, $\sigma(x) >0$ for all $x \in \mathbb{R}^d$. Consequently,
the bivariate distribution of $(Y(x_1), Y(x_2))$ is
\begin{equation*}
  \Pr\left[Y(x_1) \leq y_1, Y(x_2) \leq y_2 \right] = \Pr\left[Z(x_1)
    \leq z_2, Z(x_2) \leq z_2 \right]
\end{equation*}
where $z_1 = t(y_1)$ and $z_2 = t(y_2)$. Thus, one can relate the
bivariate density for $(Y(x_1), Y(x_2))$ to the one for $(Z(x_1),
Z(x_2))$ that we introduced in Chapter~\ref{cha:an-introduction-max}
and the log pairwise likelihood becomes
\begin{equation}
  \label{eq:lplikGEV}
  \ell_p(\mathbf{y};\psi) = \sum_{i<j} \sum_{k=1}^{n_{i,j}} \left\{
    \log f(z_k^{(i)}, z_k^{(j)}; \psi) + \log |J(y_k^{(i)})
    J(y_k^{(j)})| \right\}
\end{equation}
where $n_{i,j}$ is the sample size of common observations between site
$i$ and $j$ and
\begin{equation*}
  z_k^{(i)} = \left(1 + \xi_i \frac{y_k^{(i)} - \mu_i}{\sigma_i}
  \right)^{1/\xi_i-1}
\end{equation*}
where $\mu_i$, $\sigma_i$, $\xi_i$ are the GEV parameters for the
$i$-th site and $y_k^{(i)}$ is the $k$-th observation available at
site $i$ and $|J(t(y_k^{(i)}))|$ is the Jacobian of the mapping $t$
evaluated at the $y_k^{(i)}$ observation i.e.
\begin{equation*}
  |J(t(y_k^{(i)}))| = \frac{1}{\sigma_i} \left(1 + \xi_i
    \frac{y_k^{(i)} - \mu_i}{\sigma_i} \right)^{1/\xi_i-1}
\end{equation*}

Maximizing the log-pairwise likelihood given by
equation~\eqref{eq:lplikGEV} is possible by passing the option
\verb|fit.marge = TRUE| in the \verb|fitmaxstab| function i.e.
<<eval=FALSE>>=
fitmaxstab(data, coord, "gauss", fit.marge = TRUE)
@ 

However, this will be really time consuming as such models will have
$3 n.site + p$ parameters to estimate, where $p$ is the number of
parameters related to the extremal spatial dependence
structure. Another drawback is that prediction at unobserved locations
won't be possible. Indeed, if no model is assumed for the evolution of
the GEV parameters in space, it is therefore impossible to predict
them where no data is available.

Another way may be to fit \emph{response surfaces} for the GEV
parameters. The next section aims to give an introduction to the use
of response surfaces.

\section{Response Surfaces}
\label{sec:response-surfaces}

Response surfaces is a generic term when the problem under concern is
to describe how a \emph{response variable} $y$ depends on
\emph{explanatory variables} $x_1$, \ldots, $x_k$. For instance, with
our particular problem of spatial extremes, one may wonder how is it
possible to predict the GEV parameters at a fixed location given the
knowledge of extra covariables such as longitude, latitude, \ldots The
goal of response surfaces is to get efficient predictions for the
response variable while keeping, so far as we can, simple models.

In this section, we will first introduce the linear regression
models. Next, we will increase in complexity and flexibility by
introducing semiparametric regression models.

\subsection{Linear Regression Models}
\label{sec:line-regr-models}

Suppose we observe a response $y$ through the $y_1$, \ldots, $y_n$
values. For each observed $y_i$, we also have $p$ related explanatory
variables denoted by $x_{1,i}$, \ldots, $x_{p,i}$. To predict $y$
given the $x_{\cdot, \cdot}$ values, one might consider the following
model:
\begin{equation*}
  y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} +
  \epsilon_i
\end{equation*}
where $\beta_0$, \ldots, $\beta_p$ are the regression parameters to be
estimated and $\epsilon_i$ is an unobserved error term.

It is possible to write the above equation in a more compact way by
using matrix notation e.g.
\begin{equation}
  \label{eq:LM}
  \mathbf{y} = \mathbf{X \beta} + \mathbf{\epsilon}
\end{equation}
where $\mathbf{y}$ is a $n \times 1$ vector, $\mathbf{X}$ is a $n
\times p$ matrix called the \emph{design matrix} and
$\mathbf{\epsilon}$ is a $p \times 1$ vector.

Model~\eqref{eq:LM} is called a \emph{linear model} as it is linear in
$\beta$ but not necessarily in the covariates $x$. For example, the
two following models are linear models
\begin{eqnarray*}
  y &=& \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon\\
  y &=& \beta_0 + \beta_1 x_1 + \beta_2 \log x_2 + \epsilon
\end{eqnarray*}
or equivalently in a matrix notation
\begin{eqnarray*}
  \begin{bmatrix}
    y_1\\
    \vdots\\
    y_n
  \end{bmatrix}
   =&
  \begin{bmatrix}
    1 & x_{1,1} & x_{1,1}^2\\
    \vdots  & \vdots & \vdots\\
    1 & x_{1,n} & x_{1,n}^2  
  \end{bmatrix}
  &
  \begin{bmatrix}
    \beta_0\\
    \beta_1\\
    \beta_2
  \end{bmatrix}
  +
  \begin{bmatrix}
    \epsilon_0\\
    \epsilon_1\\
    \epsilon_2    
  \end{bmatrix}\\
   \begin{bmatrix}
    y_1\\
    \vdots\\
    y_n
  \end{bmatrix}
  =&
  \begin{bmatrix}
    1 & x_{1,1} & \log x_{2,1}\\
    \vdots  & \vdots & \vdots\\
    1 & x_{1,n} & \log x_{2,n}
  \end{bmatrix}
  &
  \begin{bmatrix}
    \beta_0\\
    \beta_1\\
    \beta_2
  \end{bmatrix}
  +
  \begin{bmatrix}
    \epsilon_0\\
    \epsilon_1\\
    \epsilon_2    
  \end{bmatrix}
\end{eqnarray*}

Usually, $\mathbf{\beta}$ is estimated by minimizing least squares
\begin{equation}
  \label{eq:least-square}
  SS(\beta) = \sum_{i=1}^n (y_i - x_i^T \mathbf{\beta})^2 =
  (\mathbf{y} - \mathbf{X\beta})^T (\mathbf{y} - \mathbf{X\beta})
\end{equation}
which amounts to solve the equations
\begin{equation*}
  \sum_{i=1}^n x_{j,i} (y_i - \mathbf{\beta}^T x_i) = 0, \qquad j = 1,
  \ldots, p
\end{equation*}
or equivalently in a matrix notation
\begin{equation*}
  \mathbf{X}^T (\mathbf{y} -\mathbf{X\beta}) = 0
\end{equation*}
so that, provided that $\mathbf{X}^T \mathbf{X}$ is invertible, the
least squares estimate for $\mathbf{\beta}$ is given by
\begin{equation}
  \label{eq:betahatLeastSquare}
  \hat{\mathbf{\beta}} = \left(\mathbf{X}^T \mathbf{X} \right)^{-1}
  \mathbf{X}^T \mathbf{y}
\end{equation}
and the fitted $\mathbf{y}$ values are given by
\begin{equation}
  \label{eq:hatMatrix}
  \hat{\mathbf{y}} = \mathbf{X} \hat{\mathbf{\beta}} = \mathbf{X}
  \left(\mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}

The matrix $\mathbf{H} = \mathbf{X} \left(\mathbf{X}^T \mathbf{X}
\right)^{-1} \mathbf{X}^T$ is called the \emph{hat matrix} as it puts
``hats'' on $\mathbf{y}$. $\mathbf{H}$ is a projection matrix that
orthogonally projects $\mathbf{y}$ onto the plane spanned by the
columns of the design matrix $\mathbf{X}$.

The hat matrix plays an important role in parametric regression as it
provides useful informations on the influence of some observations to
the fitted values. Indeed, from equation~\eqref{eq:hatMatrix}, we
have
\begin{equation*}
  \hat{y}_i = \sum_{j=1}^n H_{i,j} y_j
\end{equation*}
so that $H_{i,i}$ is the contribution of $y_i$ to the estimate
$\hat{y}_i$. Furthermore, if we consider the total influence of all
the observations, we have
\begin{eqnarray*}
  \sum_{i=1}^n H_{i,i} &=& \mbox{tr}(\mathbf{H}) = \mbox{tr}\{\mathbf{X}
  (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \}\\
  &=& \mbox{tr}\{\mathbf{X}^T \mathbf{X} (\mathbf{X^T}
  \mathbf{X})^{-1} \} = \mbox{tr} (\mathbf{I}_p) = p
\end{eqnarray*}
and the total influence of all observations is equal to the degrees of
freedom of the model.

\subsection{Semiparametric Regression Models}
\label{sec:semipar-regr}

In the previous section, we talked about linear regression models for
which the relationship between the explanatory variables and the
response has a deterministic shape and is supposed to be
known. However, it may happened applications for which the data have a
complex behaviour. For such cases, we benefit from using
semiparametric regression models defined as
\begin{equation}
  \label{eq:semiparModel}
  y_i = f(x_i) + \epsilon_i
\end{equation}
where $f$ is a smooth function with unknown shape.

The idea of semiparametric regression models is to decompose $g$ into
an appropriate basis for which equation~\eqref{eq:semiparModel}
simplifies to equation~\eqref{eq:LM} e.g.
\begin{equation}
  \label{eq:decompBasis}
  f(x) = \sum_{j=1}^q b_j(x) \beta_j
\end{equation}
where $b_j(\cdot)$ is the $j$-th basis function and $\beta_j$ is the
$j$-th element of the regression parameter $\mathbf{\beta}$.

Several basis functions exist such as the polynomial basis, the cubic
spline basis, B-splines, \ldots It is beyond the scope of this
document to introduce all of them in details but the interested reader
should have a look at \citet{Ruppert2003}. Some details about the
basis implemented in the package are reported in
Annex~\ref{cha:p-splines-with}

Usually, the basis functions $b_j(\cdot)$ depends on \emph{knots}
$\kappa$ so that equation~\eqref{eq:decompBasis} becomes
\begin{equation}
  \label{eq:decompBasis2}
  f(x) = \beta_0 + \beta_1 x + \sum_{j=1}^q b_j(x - \kappa_j)
\end{equation}

<<label=knotsEffect,echo=FALSE>>=
set.seed(12)
x <- runif(100)
fun <- function(x) sin(3 * pi * x)
y <- fun(x) + rnorm(100, 0, 0.15)
knots1 <- quantile(x, prob = 1:2 / 3)
knots2 <- quantile(x, prob = 1:10 / 11)
knots3 <- quantile(x, prob = 1:50 / 51)
M0 <- rbpspline(y, x, knots = knots1, degree = 3, penalty = 0)
M1 <- rbpspline(y, x, knots = knots2, degree = 3, penalty = 0)
M2 <- rbpspline(y, x, knots = knots3, degree = 3, penalty = 0)

par(mfrow=c(1,3))
plot(x, y, col = "lightgrey")
rug(knots1)
lines(M0)
plot(x, y, col = "lightgrey")
rug(knots2)
lines(M1, col = 2)
plot(x, y, col = "lightgrey")
rug(knots3)
lines(M2, col = 3)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<knotsEffect>>
@ 
  \caption{Impact of the number of knots in the fitted
    $p$-spline. Left panel: $q$ = 2, middle panel: $q = 10$, right
    panel: $q=50$. The small vertical lines corresponds to the
    location of each knot.}
  \label{fig:knotsEffect}
\end{figure}

The problem with model~\eqref{eq:decompBasis2} is that it is strongly
affected by the number of knots. Figure~\ref{fig:knotsEffect} depicts
this problem by fitting the same dataset to
model~\eqref{eq:decompBasis2} with $q=2, 10$ and $50$. Clearly, the
first fit is not satisfactory and we need to increase the number of
knots. The second one seems plausible while the last one clearly
overfits. This figure was generated with the following lines
<<eval=FALSE>>=
<<knotsEffect>>
@ 

Consequently, there is a pressing need for a kind of
``automatic knot selection''. One common strategy to overcome this
issue is to resort to \emph{penalized splines} or
\emph{p-splines}. The idea beyond this is to consider a large number
of knots but to constrain, in a sense to be defined, their influence.

<<label=smoothingParamEffect,echo=FALSE>>=
M0 <- rbpspline(y, x, knots = knots3, degree = 3, penalty = 0)
M1 <- rbpspline(y, x, knots = knots3, degree = 3, penalty = 0.1)
M2 <- rbpspline(y, x, knots = knots3, degree = 3, penalty = 10)

par(mfrow=c(1,3))
plot(x, y, col = "lightgrey")
lines(M0)
plot(x, y, col = "lightgrey")
lines(M1, col = 2)
plot(x, y, col = "lightgrey")
lines(M2, col = 3)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<smoothingParamEffect>>
@ 
  \caption{Impact of the smoothing parameter $\lambda$ on the
    fit. Left panel: $\lambda = 0$, middle panel: $\lambda = 0.1$ and
    right panel: $\lambda = 10$.}
  \label{fig:smoothingParamEffect}
\end{figure}

To avoid overfitting, one wish to minimize the sum of square subject
to some constraint on the $\mathbf{\beta}$ parameter i.e.
\begin{equation*}
  \text{minimize } ||\mathbf{y} - \mathbf{X\beta}||^2 \quad
  \text{subject to } \mathbf{\beta}^T \mathbf{K \beta} \leq C
\end{equation*}
for a judicious choice of $C$ and a given matrix $\mathbf{K}$. Using a
Lagrange multiplier argument, this is equivalent to choosing
$\mathbf{\beta}$ to minimize
\begin{equation}
  \label{eq:constraintLeastSquare}
  ||\mathbf{y} - \mathbf{X \beta}||^2 + \lambda \mathbf{\beta}^T
  \mathbf{K \beta}  
\end{equation}
for some $\lambda\geq 0$ called the \emph{smoothing parameter} as it
controls the amount of smoothing. Indeed, if $\lambda =0$, then
problem~\eqref{eq:constraintLeastSquare} is left unconstrained and
leads to wiggly fits, while $\lambda$ being large implies smoother
fits. Figure~\ref{fig:smoothingParamEffect} is a nice illustration of
the impact of the smoothing parameter on the smoothness of the fitted
curve. It was generated using the following code
<<eval=FALSE>>=
<<smoothingParamEffect>>
@ 

It can be shown that problem~\eqref{eq:constraintLeastSquare}
has the solution
\begin{equation}
  \label{eq:beta.hatPspline}
  \hat{\mathbf{\beta}}_\lambda = (\mathbf{X}^T \mathbf{X} + \lambda
  \mathbf{K})^{-1} \mathbf{X}^T \mathbf{y} 
\end{equation}
and the corresponding fitted values for a penalized spline are given by
\begin{equation}
  \label{eq:fitted.valuesPspline}
  \hat{\mathbf{y}} = \mathbf{X} (\mathbf{X}^T \mathbf{X} + \lambda
  \mathbf{K})^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}

In accordance with the hat matrix with linear models, one can define
the \emph{smoother matrix} $\mathbf{S}_\lambda$ such that
\begin{equation}
  \label{eq:smootherMatrix}
  \hat{\mathbf{y}} = \mathbf{S}_\lambda \mathbf{y}
\end{equation}
where $\mathbf{S}_\lambda = \mathbf{X} (\mathbf{X}^T \mathbf{X} +
\lambda \mathbf{K})^{-1} \mathbf{X}^T$. Consequently, a kind of
effective degrees of freedom is given by
$\mbox{tr}(\mathbf{S}_\lambda)$.

If the problem of knot selection seems to be resolved by using these
constrained least squares minimisation, there is still some open
questions: given our data and knots, what is the best value for
$\lambda$? Would it be possible to get an ``automatic selection'' for
$\lambda$?

One common tool for answering these two questions is known as
\emph{cross-validation} (CV)
\begin{equation}
  \label{eq:crossValidation}
  CV(\lambda) = \sum_{i=1}^n \{y_i - \hat{f}_{-i}(x_i;\lambda)\}^2
\end{equation}
where $\hat{f}_{-i}$ corresponds to the semiparametric estimator
applied to the data but with $(x_i, y_i)$ omitted. Intuitively, large
values of $CV(\lambda)$ corresponds to models that are wiggly and/or
have a large variance in the parameter estimates so that minimising
$CV(\lambda)$ is a nice option for an ``automatic selection'' of
$\lambda$.

Unfortunately, the computation of equation~\eqref{eq:crossValidation}
directly is often too CPU demanding. However, it can be shown
\citep{Ruppert2003} that
\begin{equation}
  \label{eq:crossValidationInPractice}
  CV(\lambda) = \sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 -
      S_{\lambda,ii}}\right)^2
\end{equation}
where $S_{\lambda,ii}$ is the $(i,i)$ element of
$\mathbf{S}_\lambda$. Clearly,
equation~\eqref{eq:crossValidationInPractice} does a better job than
equation~\eqref{eq:crossValidation} as it only requires one fit to
compute $CV(\lambda)$.

Sometimes, the weights $1 - S_{\lambda,ii}$ are replaced by the mean
weight, $\mbox{tr}(\mathbf{Id} - \mathbf{S}_\lambda) / n$, where
$\mathbf{Id}$ is the identity matrix, leading to the \emph{generalized
  cross validation} (GCV) score
\begin{equation}
  \label{eq:generalizedCrossValidationInPractice}
  GCV(\lambda) = n^2 \sum_{i=1}^n \left(\frac{y_i -
      \hat{y}_i}{\mbox{tr}(\mathbf{Id} - \mathbf{S}_\lambda)}
  \right)^2
\end{equation}

GCV has computational advantages over CV, and it has also
computational advantages in term of invariance \citep{Wood2006}.

<<label=CVandGCV,echo=FALSE>>=
par(mfrow=c(1,3))
lambda.cv <- cv(y, x, knots = knots3, degree = 3)$penalty
abline(v = lambda.cv, lty = 2)
lambda.gcv <- gcv(y, x, knots = knots3, degree = 3)$penalty
abline(v = lambda.gcv, lty = 2)
cv.fit <- rbpspline(y, x, knots3, degree = 3, penalty = "cv")
gcv.fit <- rbpspline(y, x, knots3, degree = 3, penalty = "gcv")
plot(x, y, col = "lightgrey")
lines(cv.fit, col = 2)
lines(gcv.fit, col = 3)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE,width=7,height=3.5>>=
<<CVandGCV>>
@ 
  \caption{Cross-validation and generalized cross validation curves
    and corresponding fitted curves.}
  \label{fig:CVandGCV}
\end{figure}

Figure~\ref{fig:CVandGCV} plots the CV and GCV curves for the data
plotted in Figure~\ref{fig:smoothingParamEffect} and the corresponding
fitted p-spline. The selection of $\lambda$ using CV or GCV yield
approximately to the same smoothing parameter value. These ``best''
$\lambda$ values are in accordance with the values we held fixed in
Figure~\ref{fig:smoothingParamEffect}. The fitted curves using eigher
CV or GCV lead to indistinguishable curves. The code used to generate
Figure~\ref{fig:CVandGCV} was
<<eval=FALSE>>=
<<CVandGCV>>
@ 

\section{Building Response Surfaces for the GEV Parameters}
\label{sec:build-resp-surf}

In the previous section, we introduced the notion of response surfaces
and we show that they should be used if one is interested in
simultaneously fitting the GEV and the spatial dependence parameters
of a max-stable process. However, one may wonder how to build accurate
response surfaces for the GEV parameters. This is the aim of this
section.

A first attempt could be to fit several max-stable models and identify
the most promising ones by using the techniques on model selection
introduced in Chapter~\ref{cha:model-selection}. Although it is a
legitimate approach, its use in practice is limited because the
fitting procedure, due to the pairwise likelihood estimator, is CPU
prohibitive.

A more pragmatic strategy is to consider only these response surfaces
while omitting temporally the spatial dependence parameters. Although
this strategy doesn't take into account all the uncertainties on the
max-stable parameters, it should lead to accurate model selection as
one expects the spatial dependence parameters and the GEV response
surface parameters to be nearly orthogonal. The main asset of the
latter approach is that fitting a (kind of) spatial GEV model to data
is less CPU consuming.

This spatial GEV model is defined as follows:
\begin{equation}
  \label{eq:spatgev}
  Z(x) \sim GEV\left(\mu(x), \sigma(x), \xi(x)\right)
\end{equation}
where the GEV parameters are defined through the following equations
\begin{equation*}
  \mu = X_\mu \beta_\mu, \qquad \sigma = X_\sigma \beta_\sigma, \qquad
  \xi = X_\xi \beta_\xi
\end{equation*}
where $X_\cdot$ are design matrices and $\beta_\cdot$ are parameters
to be estimated.

The log-likelihood of the spatial GEV model is
\begin{equation}
  \label{eq:llikSpatGEV}
  \ell(\beta) = \sum_{i=1}^{n.site}
  \sum_{j=1}^{n.obs} \left\{ - \log \sigma_i - \left(1 + \xi_i
      \frac{z_{i,j} - \mu_i}{\sigma_i} \right)^{-1/\xi_i} - \left(1 +
      \frac{1}{\xi_i} \right) \log \left(1 + \xi_i \frac{z_{i,j} -
        \mu_i}{\sigma_i} \right) \right\}
\end{equation}
where $\beta = (\beta_\mu, \beta_\sigma, \beta_\xi)$, $\mu_i$,
$\sigma_i$ and $\xi_i$ are the GEV parameters for the $i$-th site and
$z_{i,j}$ is the $j$-th obvservation for the $i$-th site.

From equation~\eqref{eq:llikSpatGEV}, we can see that independence
between stations is assumed. For most applications, this assumption is
clearly incorrect and we require the use of the MLE asymptotic
distribution under misspecification to get standard error estimates:
\begin{equation}
  \label{eq:spatGEVStdErr}
  \left(\beta_\mu, \beta_\sigma, \beta_\xi \right)
  \stackrel{\cdot}{\sim} \mathcal{N}\left(\psi, H(\beta)^{-1} J(\beta)
    H(\beta)^{-1} \right), \qquad n \rightarrow +\infty
\end{equation}
where $H(\beta) = \mathbb{E}[\nabla^2 \ell_p(\beta;\mathbf{Y})]$ (the
Hessian matrix) and $J(\beta) = \mbox{Var}[\nabla
\ell_p(\beta;\mathbf{Y})]$.

In practice, the spatial GEV model is fitted to data through the
\verb|fitspatgev| function. The use of this function is similar to
\verb|fitmaxstab|.

Lets start by simulating a max-stable process with unit Fréchet
margins and transform it to have a spatially structured GEV margins.

<<>>=
n.site <- 20
set.seed(15)
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

sigma <- matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv/2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid = FALSE, model = model,
                   maxstable="Bool", n = 50)
ms1 <- t(ms0)

param.loc <- -10 + 2 * locations[,2]
param.scale <- 5 + 2 * locations[,1] + locations[,2]^2
param.shape <- rep(0.2, n.site)

for (i in 1:n.site)
  ms1[,i] <- frech2gev(ms1[,i], param.loc[i], param.scale[i], param.shape[i])
@ 

Now we define appropriate response surfaces for our spatial GEV model
and fit two different models.
<<>>=
loc.form <- y ~ lat
scale.form <- y ~ lon + I(lat^2)
shape.form <- y ~ 1
shape.form2 <- y ~ lon
M1 <- fitspatgev(ms1, locations, loc.form, scale.form, shape.form)
M2 <- fitspatgev(ms1, locations, loc.form, scale.form, shape.form2)
M1
@ 

The output of model \verb|M1| is very similar to the one of a fitted
max-stable process except the spatial dependence parameters are not
present. As explained in Chapter~\ref{cha:model-selection}, it is easy
to perform model selection by inspecting the following output:
<<>>=
anova(M1, M2)
TIC(M1, M2)
@ 

From these two outputs, we can see that the $p$-value for the
likelihood ratio test is around $0.95$ which advocates the use of
model \verb|M1|. The TIC corroborates this conclusion.

\chapter{Conclusion}
\label{cha:conclusion}


\appendix

\chapter{P-splines with radial basis functions}
\label{cha:p-splines-with}

\section{Model definition}
\label{sec:model-definition}

Let us recall that a general definition of a p-spline is given by
\begin{equation}
  \label{eq:p-splineGeneralDef}
  y_i = \beta_0 + \beta_1 x_i + \sum_{j=1}^q b_j(x_i - \kappa_j)
\end{equation}
for some basis functions $b_j$ and knots $\kappa_j$.

As the purpose of this document is the modelling of spatial extremes,
we benefit from using \emph{radial basis} functions. Radial basis
functions depend only on the distance $|x_i - \kappa_j|$ so that a
generalisation to higher dimension, i.e. $||\mathbf{x}_i -
\mathbf{\kappa}_j||$, $\mathbf{x}_i, \mathbf{\kappa}_j \in
\mathbb{R}^d$, is straightforward.

The model for p-spline with radial basis function of order $p$, $p$
being odd, is
\begin{equation}
  \label{eq:p-splineRadialBasis}
  f(x) = \beta_0 + \beta_1 x + \cdots + \beta_{m-1} x^{m-1} +
  \sum_{j=1}^q \beta_{m + j} |x - \kappa_j|^{2m - 1}
\end{equation}
where $p=2 m - 1$.

The fitting criterion is
\begin{equation}
  \label{eq:pLeastSquaresRadialBasis}
  \text{minimize } ||\mathbf{y} - \mathbf{X \beta}||^2 + \lambda^{2m -
    1} \mathbf{\beta}^T \mathbf{K \beta}
\end{equation}
where
\begin{equation*}
  \mathbf{X} =
  \begin{bmatrix}
    1 & x_1 & \cdots & x_1^{m-1} & |x_1 - \kappa_1|^{2m - 1} & \cdots
    & |x_1 - \kappa_q|^{2m - 1}\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    1 & x_n & \cdots & x_n^{m-1} & |x_n - \kappa_1|^{2m - 1} & \cdots
    & |x_n - \kappa_q|^{2m - 1}
  \end{bmatrix}
\end{equation*}
and $\mathbf{K} = \mathbf{K}_*^T \mathbf{K}_*$ with
\begin{equation*}
  \mathbf{K}_* =
  \begin{bmatrix}
    0 & \cdots & 0 & 0 & \cdots & 0\\
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    0 & \cdots & 0 & 0 & \cdots & 0\\
    0 & \ldots & 0 & |\kappa_1 - \kappa_1|^{m - 1/2} & \cdots &
    |\kappa_1 - \kappa_q|^{m - 1/2}\\
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    0 & \cdots & 0 & |\kappa_q - \kappa_1|^{m - 1/2} & \cdots &
    |\kappa_q - \kappa_q|^{m - 1/2}
  \end{bmatrix}
\end{equation*}
where the $m$ first rows and columns of $\mathbf{K}_*$ have zeros as
elements.

\section{Fast computation of p-splines}
\label{sec:fast-computation-p}

Although this section is included in the p-splines with radial basis
functions, the methodology introduced here can be successfully applied
to any other basis functions.

As we stated in Section~\ref{sec:semipar-regr}, for a fixed smoothing
parameter $\lambda$, the fitted values are given by
\begin{equation*}
  \hat{\mathbf{y}} = \mathbf{X} ( \mathbf{X}^T \mathbf{X} + \lambda
  \mathbf{K})^{-1} \mathbf{X}^T \mathbf{y}
\end{equation*}
for some symmetric matrix $\mathbf{K}$.

Consequently, to perform automatic selection for $\lambda$ by
minimising the CV or GCV criterion might be computationally demanding
and numerically unstable. Fortunately, the Demmler-Reinsch
orthogonalisation often overcomes these issues. The following lines
describe how it works in practice.

\begin{enumerate}
  \item Obtain the Cholesky decomposition of $\mathbf{X}^T
    \mathbf{X}$ i.e.
    \begin{equation*}
      \mathbf{X}^T \mathbf{X} = \mathbf{R}^T \mathbf{R}
    \end{equation*}
    where $\mathbf{R}$ is a square matrix and invertible
  \item Obtain the singular value decomposition of $\mathbf{R}^{-T}
    \mathbf{KR^{-1}}$ i.e.
    \begin{equation*}
      \mathbf{R}^{-T} \mathbf{KR^{-1}} = \mathbf{U} \mathbf{\Lambda}
      \mathbf{U}^T
    \end{equation*}
    \item Define $\mathbf{A} \leftarrow \mathbf{X R^{-1} U}$ and
    $\mathbf{b} \leftarrow \mathbf{A}^T \mathbf{y}$
    \item The fitted values are
    \begin{equation*}
      \hat{\mathbf{y}} = \mathbf{A} \frac{\mathbf{b}}{\mathbf{1} +
        \lambda \mathbf{\Lambda}}
    \end{equation*}
    with corresponding degrees of freedom
    \begin{equation*}
      df(\lambda) = \mathbf{1}^T \frac{\mathbf{1}}{\mathbf{1} +
        \lambda \mathbf{\Lambda}}
    \end{equation*}
\end{enumerate}

Once the matrices $\mathbf{A}$ and $\mathbf{\Lambda}$ and the vector
$\mathbf{b}$ have been computed, the fitted values $\hat{\mathbf{y}}$
and $df(\lambda)$ are obtained through a simple matrix
multiplication. This is appealing as now the automatic selection for
$\lambda$ will be cheaper.

\backmatter
\bibliography{./references}
\bibliographystyle{apalike}
\end{document}

