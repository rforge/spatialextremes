\documentclass[a4paper]{report}
\usepackage{Sweave}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts}
\usepackage{hyperref}
\usepackage[square]{natbib}

\setlength{\parskip}{0.7ex plus0.1ex minus0.1ex}
\setlength{\parindent}{0em}

\renewcommand{\floatpagefraction}{0.95}
\renewcommand{\textfraction}{0.05}

\begin{document}
% \VignetteIndexEntry{A R Package for Modelling Spatial Extremes} 
% \VignetteDepends{SpatialExtremes,RandomFields}
% \VignetteKeyword{Extreme Value Theory, Spatial Extremes, Max-stable processes} 
% \VignettePackage{SpatialExtremes}

\begin{titlepage}
  \vspace*{2cm}
  \begin{center}
    \LARGE A User's Guide to the SpatialExtremes Package\\
    \vspace{1em}
    \Large Mathieu Ribatet$^\dag$ \& Simone Padoan$^\ast$\\
    \vspace{1em}
    Copyright \copyright{2008}\\
    \vspace{2em}
    \large
    $^\dag$Chair of Statistics\\
    École Polytechnique Fédérale de Lausanne\\
    Switzerland\\
    $^\ast$Laboratory of Environmental Fluid Mechanics and Hydrology\\
    École Polytechnique Fédérale de Lausanne\\
    Switzerland\\
  \end{center}
  \hfill
  \begin{center}
<<echo=FALSE,fig=TRUE>>=
library(SpatialExtremes)
library(RandomFields)
image(volcano, col = terrain.colors(100), xaxt = "n", yaxt="n")
contour(volcano, add = TRUE)
@ 
  \end{center}
  \vspace{2em}
\end{titlepage}


\pagenumbering{roman}
\normalsize

\tableofcontents
%%\listoftables
\listoffigures

\pagenumbering{arabic}

\chapter*{Introduction}
\label{cha:introduction}
\addcontentsline{toc}{chapter}{Introduction}


\section*{What is the SpatialExtremes package?}
\label{sec:what-spat-pack}

The \textbf{SpatialExtremes} package is an add-on package for the R
\citep{Rsoft} statistical computing system. It provides functions for
the analysis of spatial extremes using (currently) max-stable
processes.

All comments, criticisms and queries on the package or associated
documentation are gratefully received.

\section*{Obtaining the package/guide}
\label{sec:obta-pack}

The package can be downloaded from CRAN (The Comprehensive R Archive
Network) at \url{http://cran.r-project.org/}.  This guide (in pdf)
will be in the directory \verb+SpatialExtremes/doc/+ underneath
wherever the package is installed. You can get it by invoking
<<eval=FALSE>>=
vignette("SpatialExtremesGuide")
@ 

\section*{Contents}

This guide contains a few elements of theory on the modelling of
spatial extremes as well as examples of the use of the
\textbf{SpatialExtremes}
package. Section~\ref{cha:an-introduction-max} gives an (light)
introduction to max-stable processes and defines two different
characterisations of such processes. Section~\ref{sec:fit-maxstab}
presents the methodology used in the package to fit max-stable
processes to data while Section~\ref{cha:manip-visu-fitt} describes
useful functions for prediction and visualizing fitted models. Details
for the computation of the required pairwise densities and gradients
are given in Annex~\ref{cha:dens-grad-comp}.

\section*{Caveat}

I have checked these functions as best I can but they may contain
bugs.  If you find a bug or suspected bug in the code or the
documentation please report it to me at
\href{mailto:mathieu.ribatet@epfl.ch}{mathieu.ribatet@epfl.ch}.
Please include an appropriate subject line.

\section*{Legalese}

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 3
of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but without any warranty; without even the implied warranty of
merchantability or fitness for a particular purpose.  
See the GNU General Public License for more details.


A copy of the GNU General Public License can be obtained from 
\url{http://www.gnu.org/copyleft/gpl.html}.

\section*{Acknowledgements}

This work has been supported by the
\href{http://www.cces.ethz.ch/index}{Competence Center Environment and
  Sustainability} within the
\href{http://www.cces.ethz.ch/projects/hazri/EXTREMES}{EXTREMES}
project.

\chapter{An Introduction to Max-Stable Processes}
\label{cha:an-introduction-max}

A max-stable process $Z(\cdot)$ is the limit process of maxima of
independent identically distributed random fields $Y_i(x)$, $x \in
\mathbb{R}^d$. Namely, for suitable $a_n(x) > 0$ and $b_n(x) \in
\mathbb{R}$,
\begin{equation}
  \label{eq:maxstab-def}
  Z(x) = \lim_{n \rightarrow +\infty} \frac{\max_{i=1}^n Y_i(x) -
    b_n(x)}{a_n(x)}, \qquad x \in \mathbb{R}^d
\end{equation}
Note that \eqref{eq:maxstab-def} does not ensure that the limit
exists. However, provided it does and from \eqref{eq:maxstab-def}, we
can see that max-stable processes might be appropriate models for
modelling annual maxima of spatial data let say.

Theoretically speaking, there is no loss of generality in transforming
the margins to have a a unit Fréchet scale i.e.
\begin{equation}
  \label{eq:CDFFrechet}
  \Pr\left[Z(x) \leq z\right] = \exp\left(-\frac{1}{z} \right), \qquad
    \forall z \in \mathbb{R}^d
\end{equation}
and we will first assume that the unit Fréchet assumption holds for
which we have $a_n(x) = n$ and $b_n(x) = 0$. However, later in this
document, we will relax this assumption to have unknown GEV margins.

Currently, there are two different characterisations of a max-stable
process. The first one, often referred to the \emph{rainfall-storm}
model, was first introduced by \citet{Smith1991}. More recently,
\citet{Schlather2002} introduced a new characterisation of a
max-stable process allowing for a random shape.

It is out of the scope of this document to describe fully the main
differences between the two canonical constructions. We will restrict
our attention to particular cases of these characterisations.

Unfortunately, closed forms for the density of these two models are
only known for two different points in $\mathbb{R}^d$. Consequently,
fitting max-stable processes to data is not straightforward and the
SpatialExtremes package provides convenient tools for it.

\section{The Smith Model}
\label{sec:smiths-char}

\citet{Smith1991} proposed a method for constructing a stationary
max-stable process. Let $\{(\xi_i, y_i), i \geq 1\}$ denote the points
of a Poisson process on $(0,+\infty) \times \mathbb{R}^d$ with
intensity measure $\xi^{-2} d\xi \nu(dy)$ where $\nu(dy)$ is a
positive measure on $\mathbb{R}^d$. Then one characterisation of a
max-stable process with unit Fréchet margins is defined by:
\begin{equation}
  \label{eq:smithChar}
  Z(x) = \max_i \left\{ \xi_i f(y_i,x) \right\}, \qquad x \in
  \mathbb{R}^d
\end{equation}
where $\{f(y,x), x,y \in \mathbb{R}^d\}$ is a non-negative function
such that
\begin{equation*}
  \int_{\mathbb{R}^d} f(x,y) \nu(dy) = 1, \qquad \forall x \in
  \mathbb{R}^d
\end{equation*}

To see that Equation~\eqref{eq:smithChar} defines a stationary
max-stable process with unit Fréchet margins, we have to check that
the margins are indeed unit Fréchet and $Z(x)$ holds the max-stable
property. Following the idea of Smith, consider the set defined by:
\begin{equation*}
  E = \left\{(\xi, y) \in \mathbb{R}^+_* \times \mathbb{R}^d: \xi
    f(y,x) > z \right\}
\end{equation*}
for a fixed location $x \in \mathbb{R}^d$ and $z>0$. Then
\begin{eqnarray*}
  \Pr\left[Z(x) \leq z \right] &=& \Pr \left[\text{no points in } E
  \right] = \exp \left[ -\int_{\mathbb{R}^d} \int_{z/f(y,x)}^{+\infty}
    \xi^{-2} d\xi \nu(\mbox{dy}) \right]\\ 
  &=& \exp \left[ - \int_{\mathbb{R}^d} z^{-1} f(x,y) \nu(\mbox{dy})
  \right] = \exp \left(- \frac{1}{z} \right)
\end{eqnarray*}
and the margins are unit Fréchet.

The max-stable property of $Z(\cdot)$ follows because the
superposition of $n$ independent, identical Poisson processes is a
Poisson process with its intensity multiplied by $n$. More precisely,
we have:
\begin{equation*}
  \left\{\max_{i=1}^n Z_i(x_1), \ldots, \max_{i=1}^n Z_i(x_k) \right\}
  \stackrel{\cdot}{\sim} n \left\{Z(x_1), \ldots, Z(x_k)\right\}, \qquad k
    \in \mathbb{N}
\end{equation*}

The process defined by \eqref{eq:smithChar} is often referred to as
the rainfall-storm process as one can have a more physical
interpretation of the above construction. Think of each $y_i$ as
realisations of rainfall storm centres in $\mathbb{R}^d$ and $\nu(dy)$
as the spatial distribution of these storm centres over $\mathbb{R}^d$
- usually $\mathbb{R}^2$. Each $\xi_i$ represents the intensity of the
$i$-th storm and therefore $\xi_i f(y_i, x)$ represents the amount of
rainfall for this specific event at location $x$. In other words,
$f(y_i, \cdot)$ drive how the $i$-th storm centred at $y_i$ diffuse in
space.

Definition \eqref{eq:smithChar} is rather general and Smith considered
a particular setting where $\nu(dy)$ is the Lebesgue measure and
$f(y,x) = f_0(y-x)$, where $f_0(y-x)$ is a multivariate Normal density
with mean $y$ and covariance matrix $\Sigma$\footnote{Another form of
  Smith's model that uses a Student distribution instead of the Normal
  one. However, it is not currently implemented.}. With these
additional assumptions, it can be shown that the bivariate CDF is
given by:
\begin{equation}
  \label{eq:smith}
  \Pr[Z(x_1) \leq z_1, Z(x_2) \leq z_2] = \exp\left[-\frac{1}{z_1} \Phi
    \left(\frac{a}{2} + \frac{1}{a} \log \frac{z_2}{z_1} \right) -
    \frac{1}{z_2} \Phi \left(\frac{a}{2} + \frac{1}{a}
      \log\frac{z_1}{z_2} \right) \right]
\end{equation}
where $\Phi$ is the standard normal cumulative distribution function
and, for two given locations \#1 and \#2  
\begin{equation*}
  a^2 = \Delta x^T \Sigma^{-1} \Delta x \quad \text{and} \quad 
  \Sigma = 
  \begin{bmatrix}
    cov_{11} & cov_{12}\\
    cov_{12} & cov_{22}
  \end{bmatrix}
  \quad \text{or} \quad \Sigma = 
  \begin{bmatrix}
    cov_{11} & cov_{12} & cov_{13}\\
    cov_{12} & cov_{22} & cov_{23}\\
    cov_{13} & cov_{23} & cov_{33}
  \end{bmatrix}
  \quad \text{and so forth}
\end{equation*}
where $\Delta x$ is the distance vector between location \#1 and
location \#2. The derivation of the density is reported in Section
\ref{sec:density-computation-smith}. Currenlty, the package only
handle 2 by 2 or 3 by 3 covariance matrices
$\Sigma$. Figure~\ref{fig:Smith2sim} plots two simulations of Smith's
model with different variance-covariance matrices.

<<label=Smith2Sim,echo=FALSE>>=
x <- y <- seq(0, 10, length = 100)
sigma <- matrix(c(9/8, 0, 0, 9/8),ncol = 2)
sigma2 <- matrix(c(9/8, 1, 1, 9/8),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
sigma.inv2 <- solve(sigma2)
sqrtCinv2 <- t(chol(sigma.inv2))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
model2 <- list(list(model = "gauss", var = 1, aniso = sqrtCinv2 / 2))
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=TRUE, model = model, maxstable = "Bool")
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=TRUE, model = model2, maxstable = "Bool")
ms1 <- t(ms1)

par(mfrow=c(1,2))
image(x, y, ms0, col = terrain.colors(30))
image(x, y, ms1, col = terrain.colors(30))
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE>>=
<<Smith2Sim>>
@ 
  \caption{Two simulations of the Smith model with different $\Sigma$ matrices.}
  \label{fig:Smith2sim}
\end{figure}

\section{The Schlather Model}
\label{sec:schl-char}

More recently, \citet{Schlather2002} introduced a second
characterisation of max-stable processes. Let $Y(\cdot)$ be a
stationary process on $\mathbb{R}^d$ such that $\mathbb{E}[\max\{0,
Y(x)\}] = 1$ and $\{\xi_i, i \geq 1\}$ be the points of a Poisson
process on $\mathbb{R}^+_*$ with intensity measure $\xi^{-2}
d\xi$. Then Schlather showed that a stationary max-stable process with
unit Fréchet margins can be defined by:
\begin{equation}
  \label{eq:SchlatherChar}
  Z(x) = \max_i \xi_i \max \left\{0, Y_i(x) \right\}
\end{equation}
where the $Y_i(\cdot)$ are i.i.d copies of $Y(\cdot)$.

As before, the max-stable property of $Z(\cdot)$ stems from the
superposition of $n$ independent, identical Poisson processes. While
the unit Fréchet margins holds by using the same argument as for the
Smith model. Indeed, let consider the following set:
\begin{equation*}
  E = \left\{(\xi, y(x)) \in \mathbb{R}^+_* \times \mathbb{R}^d: \xi
    \max(0, y(x)) > z \right\}
\end{equation*}
for a fixed location $x \in \mathbb{R}^d$ and $z > 0$. Then
\begin{eqnarray*}
  \Pr\left[Z(x) \leq z \right] &=& \Pr\left[\text{no points in } E \right]
  = \exp \left[ - \int_{\mathbb{R}^d} \int_{z/\max(0, y(x))}^{+\infty}
    \xi^{-2} d\xi \nu(dy(x)) \right]\\
  &=& \exp \left[ - \int_{\mathbb{R}^d} z^{-1} \max \{0, y(x) \}
    \nu(dy(x)) \right] = \exp\left(-\frac{1}{z}\right)
\end{eqnarray*}

As with the Smith model, the process defined in
Equation~\eqref{eq:SchlatherChar} allows for practical
interpretation. Think about $\xi_i Y_i(\cdot)$ as the daily spatial
rainfall events so that all these events have the same spatial
dependence structure but differ only in their magnitude $\xi_i$. This
model differs slightly from Smith's one as we now have no
deterministic shape such as a multivariate Normal density for the
storms but a random shape driven by the process
$Y(\cdot)$. Consequently, this model has the main advantage to allows
for a large range of possible storm shapes.

Equation~\eqref{eq:SchlatherChar} is rather vague and we need
additional assumptions to get practical models. Schlather proposed to
take $Y_i(\cdot)$ to be a stationary Gaussian process with correlation
function $\rho(x)$, scaled so that $\mathbb{E}[\max\{0, Y_i(x)\}] =
1$. With these new assumtions, it can be shown that the bivariate CDF
of process~\eqref{eq:SchlatherChar} is given by:
\begin{equation}
  \label{eq:schlather}
  \Pr[Z(x_1) \leq z_1, Z(x_2) \leq z_2] = \exp\left[-\frac{1}{2}
    \left(\frac{1}{z_1} + \frac{1}{z_2} \right) \left(1 + \sqrt{1 - 2
        (\rho(h) + 1) \frac{z_1 z_2}{(z_1 + z_2)^2}} \right) \right]
\end{equation}
where $h$ is the distance between location \#1 and location \#2.

Currently, there is three types of covariance functions implemented:

\bigskip
\begin{tabular}{ll}
  \textbf{Whittle-Matérn} & $\rho(h) = sill
  \frac{2^{1-smooth}}{\Gamma(smooth)} \left(\frac{h}{range}
  \right)^{smooth} K_{smooth}\left(\frac{h}{range} \right)$,\\ 
  \textbf{Cauchy} & $\rho(h) = sill \left[1 + \left(\frac{h}{range}
    \right)^2 \right]^{-smooth}$,\\
  \textbf{Powered Exponential} & $\rho(h) = sill \exp\left[-
    \left(\frac{h}{range} \right)^{smooth} \right]$,
\end{tabular}
\bigskip

where $h$ is the distance between locations \#1 and \#2, $sill$,
$range$ and $smooth$ are the sill, the range and the smooth parameters
of the covariance function, $\Gamma$ is the gamma function and
$K_{smooth}$ is the modified Bessel function of the third kind with
order $smooth$.

The derivation of the density is reported in Section
\ref{sec:density-computation-schlather}.

<<label=Schlather2Sim,echo=FALSE>>=
x <- y <- seq(0, 10, length = 100)
set.seed(12)
ms0 <- MaxStableRF(x, y, grid=TRUE, model="wh", param=c(0,1,0,1, 1), maxstable="extr")
ms0 <- t(ms0)
set.seed(12)
ms1 <- MaxStableRF(x, y, grid=TRUE, model="stable", param=c(0,1,0,1.5, 1), maxstable="extr")
ms1 <- t(ms1)

par(mfrow=c(1,2))
image(x, y, ms0, col = terrain.colors(30))
image(x, y, ms1, col = terrain.colors(30))
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE>>=
<<Schlather2Sim>>
@ 
\caption{Two simulations of the Schlather model with different
  correlation functions with approximately the same practical
  range. Left panel: Whittle-Matérn. Right panel: Powered
  exponential.}
  \label{fig:Smith2sim}
\end{figure}

\section{The Extremal Coefficient}
\label{sec:extremal-coefficient}

The extremal coefficient is a useful way to assess the dependence
between two locations $x_1$ and $x_2 \in \mathbb{R}^d$. Assuming that
the data could be modeled by a stationary max-stable process with unit
Fréchet margin, the extremal coefficient $\theta(x_1 - x_2)$
satisfies:
\begin{equation}
  \label{eq:extcoeff}
  \Pr\left[Z(x_1) \leq z, Z(x_2) \leq z\right] = \exp\left(-
    \frac{\theta(x_1, x_2)}{z} \right)
\end{equation}
where $1 \leq \theta(x_1, x_2) \leq 2$ with the lower and upper bounds
corresponding to complete dependence and independence between
locations $x_1$ and $x_2$.

Consequently, the extremal coefficient function $\theta(\cdot, \cdot)$
is a natural way to know how the dependence between extremes in space
evolves.

\chapter{Fitting a Max-Stable Process to Data}
\label{sec:fit-maxstab}

If one is interested only in fitting the covariance matrix $\Sigma$ or
the covariance function $\rho$ to data, another fitting procedure is
available. The strategy is estimate the extremal coefficient and to
fit either the Smith or the Schlather models using least squares.  We
will consider these two different approaches separately.

\section{Least Squares}
\label{sec:least-squares}

As stated by Equations~\eqref{eq:smith} and~\eqref{eq:schlather}, the
density of the max-stable processes are analytically known only for
the bivariate case so that maximum likelihood estimators isn't
available. This statement leads Smith \citep{Smith1991} to propose an
estimator based on least squares. Namely, the fitting procedure
consists in minimizing the objective function defined by
\begin{equation}
  \label{eq:extCoeffLeastSquares}
  C(\psi) = \sum{i=1}^{n.site-1} \sum_{j=i+1}^{n.site}
  \left(\frac{\theta_{i,j} -
      \tilde{\theta}_{i,j}}{s\left(\tilde{\theta}_{i,j} \right)}
  \right)^2
\end{equation}
where $\psi$ is the vector parameter of the max-stable process,
$\theta_{i,j}$ is the predicted extremal coefficient from the
max-stable model for stations $i$ and $j$, $\tilde{\theta}_{i,j}$
is a semi-parametric estimator of the extremal coefficient for
stations $i$ and $j$ and $s(\tilde{\theta}_{i,j})$ is the standard
deviation related to the estimation of $\tilde{\theta}_{i,j}$.

The SpatialExtremes has a function to fit max-stable processes using
least squares through the \verb|fitcovmat| for the Smith model and
\verb|fitcovariance| for the Schlather model. More precisely, this is
done by the following lines:
<<eval=FALSE>>=
##Simulate a max-stable process
n.site <- 40
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="wh",
param=c(0,1,.2,3, 1.2), maxstable="extr", n = 80)
ms0 <- t(ms0)
##Smith's model
fitcovmat(ms0, locations)
##Schlather's model with Powered exponential covariance function
fitcovariance(ms0, locations, "whitmat")
@ 


\section{Pairwise Likelihood}
\label{sec:pairwise-likelihood}

As stated in the previous chapter, the densities of the two max-stable
models are only known for two different locations. The strategy used
in the package is to use pairwise-likelihood instead of the ``full''
likelihood. The log pairwise-likelihood is given by
\begin{equation}
  \label{eq:lplik}
  \ell_p(\mathbf{y};\psi) = \sum_{i<j} \sum_{k=1}^{n_{i,j}}
  \log f(y_k^{(i)}, y_k^{(j)}; \psi)
\end{equation}
where $\mathbf{y}$ is the data available on the whole region,
$n_{i,j}$ is the number of common observations between sites $i$ and
$j$, $y_k^{(i)}$ is the $k$-th observation of the $i$-th site and
$f(\cdot, \cdot)$ is the bivariate distribution of the max-stable
process - see Annex~\ref{cha:dens-grad-comp} for the analytical forms.

The max-stable process is fitted to data by maximizing the log
pairwise-likelihood\footnote{This is why the fitting procedure may be
  time consuming with large region.}. Properties of the maximum
composite likelihood estimator\footnote{In our case, the maximum
  pairwise likelihood estimator} are well known
\citep{Lindsay1988,Cox2004}. In particular, because each pairwise
score equation is unbiased, the sum of these score equations is
unbiased too and leads to consistent estimators.


\subsection{Assuming Unit Fréchet Margins}
\label{sec:simple-case-study}

To start working with the SpatialExtremes package, let consider a
simple case study for which each location is unit Fréchet
distributed. M. Schlather developed a R package called
\emph{RandomFields} to simulate spatial random fields from a
max-stable process. Consequently, all we need is to define the
coordinate of each location as well as the parameters of the
covariance function in equation \eqref{eq:schlather}.

<<>>=
library(RandomFields)
n.site <- 40
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="wh",
param=c(0,1,.2,3, 1.2), maxstable="extr", n = 80)
ms0 <- t(ms0)
@

For this application, the covariance function is taken to be the
Whittle-Matérn covariance function with sill, range and smooth
parameters equal to $0.8$ $(1 - 0.2)$, $3$ and $1.2$ respectively. The
locations are distributed uniformly on the square $[0,10]^2$.

To fit a max-stable process using pairwise likelihood and assuming
unit Fréchet margins, all we have to do is to invoke:
<<>>=
fitmaxstab(ms0, locations, cov.mod = "whitmat", fit.marge = FALSE)
@ 

From this output, we can see that we indeed use the Schlather's
representation with a Whittle-Matérn covariance function. The pairwise
deviance is given and the Takeuchi information criterion (TIC) is not
available (NA) - see Section~\ref{sec:model-selection} for more
details. But the convergence was successful and the estimates of the
covariance function are accessible. Note that large deviations from
the theoretical values may be expected as the parameters of the
Whittle-Matérn covariance function are far from orthogonal. Thus, the
range and smooth estimates may be totally different while leading
(approximately) to the same covariance function.

When using the Whittle-Matérn covariance function, it is sometimes
preferable to fix the smooth parameter using prior knowledge on the
process smoothness \citep{Diggle2007}. This can be done by:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "whitmat", smooth = 1.2, fit.marge = FALSE)
@ 

Although the Whittle-Matérn is flexible, one may want to consider
covariance functions. This is achieved by invoking:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "cauchy", fit.marge = FALSE)
fitmaxstab(ms0, locations, cov.mod = "powexp", fit.marge = FALSE)
@ 

One may also consider Smith's characterisation instead of Schlather's:
<<eval=FALSE>>=
sigma = matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv = solve(sigma)
sqrtCinv = t(chol(sigma.inv))
model = list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
msSmith <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model=model,
maxstable="Bool", n = 50)
msSmith <- t(msSmith)                   
fitmaxstab(msSmith, locations, cov.mod = "gauss", fit.marge = FALSE)
@ 

For each covariance model, one can fix any parameter:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss", cov12 = 0, fit.marge = FALSE)
fitmaxstab(ms0, locations, cov.mod = "cauchy", range = 3, fit.marge = FALSE)
@ 

Passing \verb|fit.marge = TRUE| in the previous code will result in
fitting the GEV parameters for each location. However, this will be
really CPU demanding as there will be \verb|3 n.site + p| parameters
to estimate where \verb|p| is the number of parameters for the
covariance part.

It is also possible to used different optimization routines to fit the
model to data. This is achieved by passing the \verb|method|
argument. For instance, if one wants to use the \verb|BFGS| method:
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss", cov12 = 0, fit.marge = FALSE, method = "BFGS")
@
Instead of using the \verb|optim| function, one may want to use the
\verb|nlm| or \verb|nlminb| functions. This is done as before using
the \verb|method = "nlm"| or \verb|method = "nlminb"| option.

\subsection{With Unknown GEV Margins}
\label{sec:with-unknown-gev}

In practice, the observations will never be drawn from a unit Fréchet
distribution so that the previous section won't help much with
concrete applications. One way to avoid this problem is to fit a GEV
to each location and then transform all data to the unit Fréchet
scale. This is done using the \verb|gev2frech| function and the
following code:
<<>>=
x <- c(2.2975896, 1.6448808, 1.3323833, -0.4464904, 2.2737603, -0.2581876, 9.5184398, -0.5899699, 0.4974283, -0.8152157)
gev2frech(x, 1, 2, .2)
@ 

The drawback of this approach is that standard errors are incorrect as
the margins are fitted separately from the covariance
structure. Consequently, the standard errors related to the covariance
function are underestimated as we suppose that data are originally
unit Fréchet.

SpatialExtremes solves this problem by fitting in \emph{one step} both
GEV and covariance parameters. This can be done in two ways. First,
one can pass the option \verb|fit.marge = TRUE|.  However, as said in
the previous section this will be really time consuming. Another
drawback is that prediction at ungauged locations won't be
possible. Indeed, if no model is assumed for the evolution of the GEV
parameters in space, it is impossible to predict them where no data is
available.

Another way may be to fit a \emph{response surface} for the GEV
parameters. Currently, the response surfaces allowed by the package
are polynomial surfaces or a penalized smoothing spline\footnote{this
  is an exclusive ``or'' (or \emph{xor}) i.e. a response surface with
  a polynomial and a spline is not possible}.

The SpatialExtremes package defines response surfaces using the
\emph{R formula} approach e.g.
<<eval=FALSE>>=
y ~ lat + I(lon^2)
@ 
for a polynomial surface and
<<eval=FALSE>>=
n.knots <- 5
knots <- quantile(locations[,2], prob = 1:n.knots/(n.knots+1))
y ~ rb(lat, knots = knots, degree = 3, penalty = .5)
@ 
for a penalized smoothing spline with degree 3, 5 knots and a penalty
coefficient (also known as the smoothing parameter) equals to 0.5.

Let us start with a simple polynomial surface. For this purpose, we need
to simulate a max-stable process and then transform the observations
to the desired GEV scale. This could be done by the following lines:
<<eval=FALSE>>=
n.site <- 20
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

sigma = matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv = solve(sigma)
sqrtCinv = t(chol(sigma.inv))
model = list(list(model = "gauss", var = 1,
aniso = sqrtCinv / 2))
ms0 <- MaxStableRF(locations[,1], locations[,2],
grid=FALSE, model=model,
maxstable="Bool",
n = 50)
ms1 <- t(ms0)
param.loc <- -10 + 2 * locations[,2]
param.scale <- 5 + 2 * locations[,1] + locations[,2]^2
param.shape <- rep(0.2, n.site)

for (i in 1:n.site)
ms1[,i] <- param.scale[i] * (ms1[,i]^param.shape[i] - 1) / param.shape[i] + param.loc[i]
@ 

Once the data are appropriately generated, we need to define the
response surface for the fitted max-stable model. This is done
invoking:
<<eval=FALSE>>=
loc.form <- y ~ lat
scale.form <- y ~ lon + I(lat^2)
shape.form <- y ~ 1
@ 
Lastly, one can easily fit the model to data using:
<<eval=FALSE>>=
fitmaxstab(ms1, locations, "gauss", loc.form = loc.form, scale.form = scale.form, shape.form= shape.form)
@ 

If we want to fit a spline for the location GEV parameter while
preserving the polynomials for the scale and shape parameters, this
will require more steps as the knots and the penalty coefficient must
be defined\footnote{Currently, an automatic criteria for defining the
  ``best'' penalty coefficient does not exist.}.
<<eval = FALSE>>=
n.knots <- 5
knots <- quantile(locations[,2], 1:n.knots/(n.knots+1))
loc.form <- y ~ rb(lat, knots = knots, degree = 3, penalty = .5)
fitmaxstab(ms1, locations, "gauss", loc.form = loc.form, scale.form = scale.form, shape.form= shape.form)
@ 

These steps still remain valid when fitting Schlather's model. You can
fix any parameter as before for example, if one want to suppose that
\verb|scaleCoeff3 = 1|, this is done by:
<<eval=FALSE>>=
fitmaxstab(ms1, locations, "powexp", loc.form = loc.form, scale.form = scale.form,
           shape.form= shape.form, scaleCoeff3 = 1)
@ 

When using three smoothing splines for the GEV parameters, you might
have to tweak the \verb|ndeps| option in the \verb|optim| function if
you use the \verb|BFGS| optimization procedure:
<<eval=FALSE>>=
fitmaxstab(ms1, locations, "powexp", loc.form = loc.form,
           scale.form = scale.form, shape.form= shape.form,
           control = list(ndeps = rep(10^-6, n.par)), method = "BFGS")
@ 
where \verb|n.par| is the number of parameters to be estimated.

\subsection{Assessing Uncertainties}
\label{sec:assess-uncert}

As stated in section~\ref{sec:fit-maxstab}, because the model is
fitted by maximizing the pairwise likelihood instead of the ``full''
likelihood, the model is \emph{misspecified}. The maximum pairwise
likelihood estimator is still asymptotically normally distributed but
with a different asymptotic covariance matrix. The maximum pairwise
likelihood estimator $\hat{\psi}_p$ satisfies the following relation:
\begin{equation}
  \label{eq:lplikAsymp}
  \hat{\psi}_p \stackrel{\cdot}{\sim} \mathcal{N}\left(\psi,
    H(\psi)^{-1} J(\psi) H(\psi)^{-1} \right), \qquad n \rightarrow
  +\infty
\end{equation}
where $H(\psi) = \mathbb{E}[\nabla^2 \ell_p(\psi;\mathbf{Y})]$ (the
Hessian matrix) and $J(\psi) = \mbox{Var}[\nabla
\ell_p(\psi;\mathbf{Y})]$, where the expectations are with respect to
the ``full'' density.

In practice, to get the standard errors we need estimates of $H(\psi)$
and $J(\psi)$. The estimation of the former is straightforward and is
given by $\hat{H}(\hat{\psi}_p) = \nabla^2
\ell_p(\hat{\psi}_p;\mathbf{y})$; that is the Hessian matrix evaluated
at $\hat{\psi}_p$.

The estimation of $J(\psi)$ can be done in two different ways. First,
it can be estimated using the ``naive'' estimator
$\hat{J}(\hat{\psi}_p) = \nabla \ell_p(\hat{\psi}_p;\mathbf{y})
{\nabla \ell_p(\hat{\psi}_p;\mathbf{y})}^T$. In the SpatialExtremes
package, this estimator is tagged \verb|grad| as it uses the gradient
of the log pairwise likelihood. Another estimator is given by noticing
that $J(\psi)$ corresponds to the variance of the pairwise score
equations $\ell_p(\psi;\mathbf{Y}) = 0$. Consequently, a second
estimator, tagged \verb|score|, is given by the sample variance of
each contribution to the pairwise score function. Note that the second
estimator is only accessible if independent replications of
$\mathbf{Y}$ are available\footnote{which will mostly be the case for
  spatial extremes.}.

These two types of standard errors are available by invoking the
following two lines:
<<echo=FALSE>>=
n.site <- 20
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

sigma = matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv = solve(sigma)
sqrtCinv = t(chol(sigma.inv))
model = list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model=model,
                   maxstable="Bool", n = 50)
ms0 <- t(ms0)
@ 
<<eval=FALSE>>=
fitmaxstab(ms0, locations, cov.mod = "gauss", fit.marge = FALSE, std.err.type = "score")
fitmaxstab(ms0, locations, cov.mod = "gauss", fit.marge = FALSE, std.err.type = "grad")
@ 

\chapter{Model Selection}
\label{cha:model-selection}

Model selection plays an important role in statistical
modelling. According to the Ockam's razor, given several models that
fit the data equally well, we should focus on simple models rather
than more complex ones. Depending on the models to be compared,
several approaches exist for model selection. In this section, we will
present theory on information criteria such as the Akaike Information
Criterion (AIC) and about the likelihood ratio statistic. We present
these two approaches in turn.

\section{Takeuchi Information Criterion}
\label{sec:take-inform-crit}

Having two different models, we want to know which one we should
prefer for modelling our data. If these two models have exactly the
same maximized log-likelihoods, we should prefer the one which have
less parameters because it will reduce the variance of our
estimates. However, if these two maximized log-likelihoods only differ
by a small amount, does this small increase worth the price of having
additional parameters? To answer this question, we have to resort to
the Kullback-Leibler discrepancy.

Let consider a random sample $Y_1$, \ldots, $Y_n$ drawn from an
unknown density $g$. Ignoring $g$, we fit a statistical model
$f(y;\theta)$ by maximizing the log-likelihood. The Kullback-Leibler
discrepancy measures the discrepancy of our fitted model $f$ from the
true one $g$
\begin{equation}
  \label{eq:KullbackLeibler}
  D\left(f_\theta, g\right) = \int \log \left(
    \frac{g(y)}{f(y;\theta)} \right) g(y) \mbox{dy}
\end{equation}

It can be shown that the Kullback-Leibler discrepancy is positive and
vanishes if and only if $f(y;\theta) = g(y)$. Consequently, we aim to
choose models that minimize $D(f_\theta, g)$. However, $D(f_\theta,
g)$ isn't enough discriminant as several models satisfy $D(f_\theta,
g) = 0$. Indeed, let consider two models $f_0$ and $f_1$ where $f_0$
is nested in $f_1$ and $D(f_1, g) = 0$ then $D(f_0, g) = 0$ too.

Because we work under miss-specification, AIC is not
appropriate, so we use a generalization of the Takeuchi information
criterion. \citet{Varin2005} show that, under miss-specification, an
appropriate selection statistic is given by
\begin{equation}
  \label{eq:TIC}
  \mbox{TIC} = - \ell_p (\psi_p) - \mbox{tr}\left\{J H^{-1} \right\}
\end{equation}
In accordance with the AIC, the best model will corresponds to the one
that minimizes equation~\eqref{eq:TIC}.

In practice, one can have a look at the output of the
\verb|fitmaxstab| function or use the \verb|TIC| function.

<<eval=FALSE>>=
n.site <- 40
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="stable",
                   param=c(0,1,0,3, 1.2), maxstable="extr", n = 80)
ms0 <- t(ms0)
model1 <- fitmaxstab(ms0, locations, cov.mod = "powexp", fit.marge = FALSE, sill = 1)
model2 <- fitmaxstab(ms0, locations, cov.mod = "cauchy", fit.marge = FALSE, sill = 1)
TIC(model1, model2)
@ 

\section{Likelihood Ratio Statistic}
\label{sec:likel-ratio-stat}

TIC is useful when comparing different models. However, it may lack
power if the two models are nested. When dealing with nested models,
one may prefer using the likelihood ratio statistics.

Because we are working with a misspecified model, the usual asymptotic
$\chi^2_p$ distribution, where $p$ is the number of additional
parameters to be estimated in the more complex model, doesn't hold
anymore. There's two way to solve this issue: (a) adjusting the
$\chi^2$ distribution or (b) adjusting the composite likelihood so
that the usual $\chi^2_p$ holds - see \citet{Cox2004}.

<<eval=FALSE>>=
require(RandomFields)

##Define the coordinates of each location
n.site <- 30
locations <- matrix(rnorm(2*n.site, sd = sqrt(.2)), ncol = 2)
colnames(locations) <- c("lon", "lat")

##Simulate a max-stable process - with unit Frechet margins
sigma <- matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model=model,
                   maxstable = "Bool", n = 50)
ms0 <- t(ms0)

##Fit three nested models
M0 <- fitmaxstab(ms0, locations, "gauss", fit.marge = FALSE)
M1 <- fitmaxstab(ms0, locations, "gauss", fit.marge = FALSE, cov11 =
100)
anova(M0, M1)
@ 

\section{Building Response Surfaces for the GEV Parameters}
\label{sec:build-resp-surf}

In section~\ref{sec:with-unknown-gev} we show how to fit a max-stable
processes to data with unknown GEV margins. However, we didn't mention
how to build response surfaces as models for the evolution of the GEV
parameters. One way has been first introduced in the previous section
related to model selection. However, because we fit max-stable
processes by maximizing the log-pairwise likelihood, a comprehensive
selection of models could be CPU prohibitive. Another option may be to
fit only these response surfaces to data while omitting temporally the
spatial dependence parameters. Despite this strategy doesn't take into
account all uncertainties on the max-stable parameters, it should lead
to accurate model selection as one expect that the spatial dependence
parameters and the GEV response surface ones to be nearly
orthogonal. The main asset of the latter approach is that fitting a
(kind of) spatial GEV model to data is clearly less CPU consuming.

This spatial GEV model is defined as follows:
\begin{equation}
  \label{eq:spatgev}
  Z(x) \stackrel{\cdot}{\sim} GEV\left(\mu(x), \sigma(x),
    \xi(x)\right)
\end{equation}
where the GEV parameters are defined through the following equations
\begin{eqnarray*}
  \mu &=& X_\mu \beta_\mu\\
  \sigma &=& X_\sigma \beta_\sigma\\
  \xi &=& X_\xi \beta_\xi
\end{eqnarray*}
where $X_\cdot$ are design matrices and $\beta_\cdot$ are parameters
to be estimated.

The log-likelihood of the spatial GEV model is given by:
\begin{equation}
  \label{eq:llikSpatGEV}
  \ell(\beta) = \sum_{i=1}^{n.site}
  \sum_{j=1}^{n.obs} \left[ - \log \sigma_i - \left(1 + \xi_i
      \frac{z_{i,j} - \mu_i}{\sigma_i} \right)^{-1/\xi_i} - \left(1 +
      \frac{1}{\xi_i} \right) \log \left(1 + \xi_i \frac{z_{i,j} -
        \mu_i}{\sigma_i} \right) \right]
\end{equation}
where $\beta = (\beta_\mu, \beta_\sigma, \beta_\xi)$, $\mu_i$,
$\sigma_i$ and $\xi_i$ are the GEV parameters for the $i$-th site and
$z_{i,j}$ is the $j$-th obvservation for the $i$-th site.

From Equation~\eqref{eq:llikSpatGEV}, we can see that independence
between stations is assumed. For most applications, this assumption is
clearly incorrect and we require the use of the MLE asymptotic
distribution under misspecification to get standard error estimates:
\begin{equation}
  \label{eq:spatGEVStdErr}
  \left(\beta_\mu, \beta_\sigma, \beta_\xi \right)
  \stackrel{\cdot}{\sim} \mathcal{N}\left(\psi, H(\beta)^{-1} J(\beta)
    H(\beta)^{-1} \right), \qquad n \rightarrow +\infty
\end{equation}
where $H(\beta) = \mathbb{E}[\nabla^2 \ell_p(\beta;\mathbf{Y})]$ (the
Hessian matrix) and $J(\beta) = \mbox{Var}[\nabla
\ell_p(\beta;\mathbf{Y})]$.

In practice, the spatial GEV model is fitted to data through the
\verb|fitspatgev| function. The use of this function is similar to the
one of the \verb|fitmaxstab| one. Let show its use by the following
example.

Lets start by simulating a max-stable process with unit Fréchet
margins and transform it to have a spatially structured GEV margins.

<<>>=
n.site <- 20
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

sigma <- matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv <- solve(sigma)
sqrtCinv <- t(chol(sigma.inv))
model <- list(list(model = "gauss", var = 1, aniso = sqrtCinv/2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid = FALSE, model = model,
                   maxstable="Bool", n = 50)
ms1 <- t(ms0)

param.loc <- -10 + 2 * locations[,2]
param.scale <- 5 + 2 * locations[,1] + locations[,2]^2
param.shape <- rep(0.2, n.site)

for (i in 1:n.site)
  ms1[,i] <- param.scale[i] * (ms1[,i]^param.shape[i] - 1) / param.shape[i] + param.loc[i]
@ 

Now we define appropriate response surfaces for our spatial GEV model
and fit two different models.
<<>>=
loc.form <- y ~ lat
scale.form <- y ~ lon + I(lat^2)
shape.form <- y ~ 1
shape.form2 <- y ~ lon
M1 <- fitspatgev(ms1, locations, loc.form, scale.form, shape.form)
M2 <- fitspatgev(ms1, locations, loc.form, scale.form, shape.form2)
M1
@ 

From the output of model \verb|M1| we can see that it is very similar
to the one of a fitted max-stable process apart from the spatial
dependence parameters are not present. In fact, \verb|M1| is an object
of class \verb|spatgev| so that when printing \verb|M1| \textbf{R}
calls a specific printing method. This is the same when calling either
the \verb|anova| or the \verb|TIC| functions.

As explained in Section~\ref{sec:model-selection}, it is easy to
perform model selection by inspecting the following output:
<<>>=
anova(M1, M2)
TIC(M1, M2)
@ 

\chapter{Manipulating and Visualising Fitted Models}
\label{cha:manip-visu-fitt}

\section{Prediction of the GEV parameters}
\label{sec:pred-gev-param}

Once the model is fitted, one may want estimates of the GEV parameters
at any set of location. This is achieved using the \textsl{predict}
function:
<<echo=FALSE,eval=FALSE>>=
n.site <- 20
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

sigma = matrix(c(100, 25, 25, 220),ncol = 2)
sigma.inv = solve(sigma)
sqrtCinv = t(chol(sigma.inv))
model = list(list(model = "gauss", var = 1, aniso = sqrtCinv / 2))
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model=model, maxstable="Bool",n = 50)
ms1 <- t(ms0)
param.loc <- -10 + 2 * locations[,2]
param.scale <- 5 + 2 * locations[,1] + locations[,2]^2
param.shape <- rep(0.2, n.site)

for (i in 1:n.site)
ms1[,i] <- param.scale[i] * (ms1[,i]^param.shape[i] - 1) / param.shape[i] + param.loc[i]

loc.form <- y ~ lat
scale.form <- y ~ lon + I(lat^2)
shape.form <- y ~ 1
@ 
<<eval=FALSE>>=
fitted <- fitmaxstab(ms1, locations, "gauss", loc.form = loc.form, scale.form = scale.form, shape.form = shape.form)
predict(fitted)
@ 

If one want to get estimates of the GEV parameters at an ungauged
locations, this is done by adding a matrix giving the new
coordinates. If new coordinates are supplied, the column names of the
new coordinates should match those. For our application, this could be
done as follows:
<<eval=FALSE>>=
new.coord <- cbind(3:6, 7:10)
colnames(new.coord) <- c("lon", "lat")
predict(fitted, new.coord)
@ 

\section{Visualising the Extremal Coefficient}
\label{sec:visu-extr-coeff}

The SpatialExtremes package proposes two way to plot the evolution of
the extremal coefficient as the distance increases. The first one is
to use an empirical estimation of the extremal coefficient; while the
second uses a parametric estimation using the Smith's and Schlather's
models.

\citet{Schlather2003} introduced a methodology to estimate the
extremal coefficient non-parametrically. The \verb|fitextcoeff|
function uses this methodology to get estimates for each pair of
stations within the region and plots the evolution of these estimates
as the distance increases. In addition, by default, a \emph{lowess}
curve can also be plotted to help detecting trends. This is done using
the following lines and Figure~\ref{fig:fitextcoeff} plots the
resulting output.
<<label=fitextcoeff>>=
n.site <- 30
locations <- matrix(runif(2*n.site, 0, 10), ncol = 2)
colnames(locations) <- c("lon", "lat")

##Simulates a max-stable process - with unit Frechet margins
ms0 <- MaxStableRF(locations[,1], locations[,2], grid=FALSE, model="wh",
                   param=c(0,1,0,30, .5), maxstable="extr",
                   n = 40)
ms0 <- t(ms0)

##Plots the extremal coefficient function
exco <- fitextcoeff(ms0, locations, estim = "Smith")
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE>>=
<<fitextcoeff>>
@   
  \caption{Evolution of the (non-parametric) estimates of the extremal
    coefficient as the distance increases.}
  \label{fig:fitextcoeff}
\end{figure}

The closed form of the extremal coefficient function is known for both
Smith's and Schlather's models. Namely, the function is given by:

\bigskip
\begin{tabular}{ll}
  \textbf{Smith} & $\theta(x_1 - x_2) = 2
  \Phi\left(\frac{\sqrt{(x_1 - x_2)^T \Sigma^{-1} (x_1 - x_2)}}{2}
  \right)$\\
  \textbf{Schlather} & $\theta(||x_1 - x_2||) = 1 + \sqrt{\frac{1 -
      \rho(||x_1 - x_2||)}{2}}$
\end{tabular}
\bigskip

The SpatialExtremes package can plot the evolution of the extremal
coefficient function using the \verb|extcoeff| function - see
Fig.~\ref{fig:extcoeff}.
<<label=extcoeff>>=
fitted <- fitmaxstab(ms0, locations, "whitmat", fit.marge = FALSE)
extcoeff(fitted)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE>>=
<<extcoeff>>
@ 
  \caption{Evolution of the extremal coefficient function in $\mathbb{R}^2$.}
  \label{fig:extcoeff}
\end{figure}

\section{Visualising the Covariance Function}
\label{sec:visu-covar-funct}

Another way to assess how the dependence between extremes evolves as
the distance increases is to plot the covariance function. This is
done using the \verb|covariance| function.

There are two ways to call the \verb|covariance| function. We can call
it once we have fitted a max-stable process or by specifying directly
the covariance parameters.

For illustration purpose, Fig.~\ref{fig:covfun} compares the fitted
covariance function to the theoretical one.

<<label=covfun>>=
covariance(fitted, ylim = c(0,1))
covariance(sill = 1, range = 30, smooth = 0.5, cov.mod = "whitmat", col = 3, add = TRUE)
legend("topright", c("Fitted","Theo"), lty = 1, col = c(1,3), inset = .05)
@ 

\begin{figure}
  \centering
<<fig=TRUE,echo=FALSE>>=
<<covfun>>
@ 
  \caption{Comparison between the fitted covariance function and the
    theoretical one.}
  \label{fig:covfun}
\end{figure}

One can also compute the covariance at a given distance by invoking:
<<>>=
rbind(fitted = covariance(fitted, dist = seq(0,10, 3))$cov.val,
      theo = covariance(sill = 1, range = 30, smooth = 0.5, cov.mod = "whitmat", dist = seq(0,10, 3))$cov.val)
@ 

\section{Producing a map of the GEV parameters and return levels}
\label{sec:poroducing-map-gev}

Most often, practitioners will like to have a map of the GEV
parameters or a map of return levels with a given return period. This
is done using the \verb|map| function.

To illustrate this feature, let use the previous fitted model.  One
can have a contour plot of the evolution of the GEV parameters in
$\mathbb{R}^d$ by invoking the following code:
<<eval=FALSE,label=mapGEV>>=
par(mfrow=c(1,3))
map(fitted, "loc", col = rainbow(80))
title("Location")
map(fitted, "scale", col = heat.colors(80))
title("Scale")
map(fitted, "shape", col = topo.colors(100))
title("Shape")
@ 

Note that tuning the option \verb|col| will allow users to choose an
appropriate color palette.

In addition, it is possible to plot a map of the 50-year return level
while focusing on a specific part of the region under study:
<<eval=FALSE,label=mapQ50>>=
new.ranges <- cbind(c(3, 9), c(2, 10))
colnames(new.ranges) <- c("lon", "lat")

map(fitted, "quant", ret.per = 50 , ranges = new.ranges)
@ 

\appendix

\chapter{Density and Gradient Computations}
\label{cha:dens-grad-comp}

\section{Smith's Model}
\label{sec:smith-char}

Let us recall that Smith's model is given by:
\begin{equation}
  \label{eq:smith}
  \Pr[Z_1 \leq z_1, Z_2 \leq z_2] = \exp\left[-\frac{1}{z_1} \Phi
    \left(\frac{a}{2} + \frac{1}{a} \log \frac{z_2}{z_1} \right) -
    \frac{1}{z_2} \Phi \left(\frac{a}{2} + \frac{1}{a}
      \log\frac{z_1}{z_2} \right) \right]
\end{equation}
where $\Phi$ is the standard normal cumulative distribution function
and, for two locations \#1 and \#2  
\begin{equation*}
  a^2 = \Delta x^T \Sigma^{-1} \Delta x \quad \text{and} \quad 
  \Sigma = 
  \begin{bmatrix}
    cov_{11} & cov_{12}\\
    cov_{12} & cov_{22}
  \end{bmatrix}
  \quad \text{or} \quad \Sigma = 
  \begin{bmatrix}
    cov_{11} & cov_{12} & cov_{13}\\
    cov_{12} & cov_{22} & cov_{23}\\
    cov_{13} & cov_{23} & cov_{33}
  \end{bmatrix}
\end{equation*}
where $\Delta x$ is the distance vector between location \#1 and
location \#2.

\subsection{Useful quantities}
\label{sec:usefull-quantities}

Computation of the density as well as the gradient of the density is
not difficult but ``heavy'' though. For computation facilities and to
help readers, we define:
\begin{eqnarray}
  \label{eq:1}
  c_1 = \frac{a}{2} + \frac{1}{a} \log \frac{z_2}{z_1} \quad
  \text{and} \quad
  c_2 = \frac{a}{2} + \frac{1}{a} \log \frac{z_1}{z_2}
\end{eqnarray}
From these definitions, we note that $c_1 + c_2 = a$.

\subsection{Density computation}
\label{sec:density-computation-smith}

From \eqref{eq:smith}, we note the standard normal distribution
appears. Consequently, we need to compute its derivatives at $c_1$ and
$c_2$ with respect to $z_1$ and $z_2$.
\begin{eqnarray}
  \label{eq:2}
  \frac{\partial c_1}{\partial z_1} = \frac{1}{a} \left(-
    \frac{z_2}{z_1^2} \frac{z_1}{z_2} \right) = -\frac{1}{az_1} &\qquad&
  \frac{\partial c_1}{\partial z_2} = \frac{1}{a} \frac{1}{z_1}
  \frac{z_1}{z_2} = \frac{1}{az_2}\\
  \frac{\partial c_2}{\partial z_1} = - \frac{\partial c_1}{\partial z_1}
  = \frac{1}{az_1} &\qquad&
  \frac{\partial c_2}{\partial z_2} = - \frac{\partial c_1}{\partial z_2}
  = - \frac{1}{az_2}  
\end{eqnarray}

As the normal distribution appears in the Smith's characterisation,
the following quantities will be useful:
\begin{eqnarray}
  \label{eq:6}
  \frac{\partial \Phi(c_1)}{\partial z_1} = \frac{\partial
    \Phi(c_1)}{\partial c_1}
  \frac{\partial c_1}{\partial z_1} =  -\frac{\varphi(c_1)}{az_1} &\qquad&
  \frac{\partial \Phi(c_1)}{\partial z_2} = \frac{\partial
    \Phi(c_1)}{\partial c_1}
  \frac{\partial c_1}{\partial z_2} =  \frac{\varphi(c_1)}{az_2}\\
  \frac{\partial \Phi(c_2)}{\partial z_1} = \frac{\partial \Phi(c_2)}{\partial c_2}
  \frac{\partial c_2}{\partial z_1} =  \frac{\varphi(c_2)}{az_1} &\qquad&
  \frac{\partial \Phi(c_2)}{\partial z_2} = \frac{\partial \Phi(c_2)}{\partial c_2}
  \frac{\partial c_2}{\partial z_2} =  -\frac{\varphi(c_2)}{az_2}\\
  \frac{\partial \varphi(c_1)}{\partial z_1} = \frac{\partial \varphi(c_1)}{\partial c_1}
  \frac{\partial c_1}{\partial z_1} = \frac{c_1 \varphi(c_1)}{az_1} &\qquad& 
  \frac{\partial \varphi(c_1)}{z_2} = \frac{\partial
    \varphi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial z_2} = -
  \frac{c_1 \varphi(c_1)}{a z_2}\\
  \frac{\partial \varphi(c_2)}{\partial z_1} = \frac{\partial \varphi(c_2)}{\partial c_2}
  \frac{\partial c_2}{\partial z_1} = - \frac{c_2 \varphi(c_2)}{a z_1} &\qquad&
  \frac{\partial \varphi(c_2)}{\partial z_2} = \frac{\partial \varphi(c_2)}{\partial c_2}
  \frac{\partial c_2}{\partial z_2} = \frac{c_2 \varphi(c_2)}{a z_2}
\end{eqnarray}

Define
\begin{equation}
  \label{eq:3}
  A = \frac{1}{z_1}\Phi(c_1) \quad \text{and} \quad B = \frac{1}{z_2}\Phi(c_2)
\end{equation}
Consequently, $F(z_1, z_2) = \exp(-A -B)$ and
\begin{equation}
  \label{eq:4}
  \frac{\partial F}{\partial z_1} (z_1, z_2) = - \left(\frac{\partial
      A}{\partial z_1} + \frac{\partial B}{\partial z_1} \right)
  F(z_1, z_2)\qquad
  \frac{\partial F}{\partial z_2} (z_1, z_2) = - \left(\frac{\partial
      A}{\partial z_2} + \frac{\partial B}{\partial z_2} \right)
  F(z_1, z_2)
\end{equation}
By noting that
\begin{eqnarray}
  \label{eq:5}
  \frac{\partial A}{\partial z_1} &=& -\frac{\Phi(c_1)}{z_1^2} +
  \frac{1}{z_1} \left(-\frac{\varphi(c_1)}{az_1}\right) =
  -\frac{\Phi(c_1)}{z_1^2} - \frac{\varphi(c_1)}{az_1^2}\\
  \frac{\partial B}{\partial z_1} &=& \frac{1}{z_2}
  \frac{\varphi(c_2)}{az_1} = \frac{\varphi(c_2)}{az_1z_2}\\
  \frac{\partial A}{\partial z_2} &=& \frac{1}{z_1}
  \frac{\varphi(c_1)}{az_2} = \frac{\varphi(c_1)}{az_1z_2}\\
  \frac{\partial B}{\partial z_2} &=& -\frac{\Phi(c_2)}{z_2^2} +
  \frac{1}{z_2} \left(- \frac{\varphi(c_2)}{az_2}\right) =
  -\frac{\Phi(c_2)}{z_2^2} - \frac{\varphi(c_2)}{az_2^2}
\end{eqnarray}
and
\begin{eqnarray}
  \label{eq:10}
  \frac{\partial^2 A}{\partial z_2 \partial z_1} &=& 
  \frac{\partial }{\partial z_2} \left(-\frac{\Phi(c_1)}{z_1^2} -
    \frac{\varphi(c_1)}{az_1^2}\right) = -\frac{\varphi(c_1)}{a z_1^2
    z_2} + \frac{c_1\varphi(c_1)}{a^2 z_1^2 z_2} = -\frac{c_2
    \varphi(c_1)}{a^2z_1^2z_2}\\
  \frac{\partial^2 B}{\partial z_2 \partial z_1} &=& \frac{\partial
  }{\partial z_2} \frac{\varphi(c_2)}{az_1z_2} =
  -\frac{c_1\varphi(c_2)}{a^2z_1z_2^2}
\end{eqnarray}
So that,
\begin{eqnarray}
  \label{eq:7}
  \frac{\partial F}{\partial z_1} (z_1, z_2) &=&  \left(
    \frac{\Phi(c_1)}{z_1^2} + \frac{\varphi(c_1)}{az_1^2} -
    \frac{\varphi(c_2)}{az_1z_2} \right) F(z_1, z_2)\\
  \frac{\partial F}{\partial z_2} (z_1, z_2) &=& \left(
    \frac{\Phi(c_2)}{z_2^2} + \frac{\varphi(c_2)}{az_2^2}
    -\frac{\varphi(c_1)}{az_1z_2} \right) F(z_1, z_2)
\end{eqnarray}
Finally,
\begin{equation}
  \label{eq:9}
  \frac{\partial^2 F}{\partial z_2 \partial z_1} (z_1,
  z_2) = - \left(\frac{\partial^2 A}{\partial z_2 \partial z_1} +
    \frac{\partial^2 B}{\partial z_2 \partial z_1} \right) F(z_1, z_2)
  - \left(\frac{\partial A}{\partial z_1} + \frac{\partial B}{\partial
      z_1} \right) \frac{\partial F}{\partial z_2} (z_1, z_2)
\end{equation}
Thus, it leads to the following relation:
\begin{equation}
  \label{eq:11}
  f(z_1, z_2) = \left[ \frac{c_2 \varphi(c_1)}{a^2z_1^2z_2} + \frac{c_1
      \varphi(c_2)}{a^2z_1z_2^2} + \left(\frac{\Phi(c_1)}{z_1^2} +
      \frac{\varphi(c_1)}{az_1^2} - \frac{\varphi(c_2)}{az_1z_2} \right)
    \left(\frac{\Phi(c_2)}{z_2^2} + \frac{\varphi(c_2)}{az_2^2} -
      \frac{\varphi(c_1)}{az_1z_2} \right) \right] F(z_1, z_2)
\end{equation}

\subsection{Gradient computation}
\label{sec:gradient-computation-smith}

As said in section \ref{sec:assess-uncert}, the maximum pairwise
likelihood estimator $\psi_p$ satisfies:
\begin{equation*}
  \psi_p \sim \mathcal{N}\left(\psi, H^{-1} J H^{-1}\right)
\end{equation*}
where $H$ is the Fisher information matrix and $J$ as defined in
Section~\ref{sec:assess-uncert}. This section aims to derive
analytical form for $J$.

Let us recall that the log pairwise likelihood is defined by:
\begin{equation*}
  \ell_p(\mathbf{z}, \Psi) = \sum_{k = 1}^{n_{obs}}
  \sum_{i=1}^{n_{site}-1} \sum_{j=i+1}^{n_{site}} \log f(z_k^{(i)},
  z_k^{(j)})
\end{equation*}
where $n_{obs}$ is the number of observations, $\mathbf{z}_k =
(z_k^{(1)}, \ldots, z_k^{(n_{site})})$ is the $k$-th observation vector,
$n_{site}$ is the number of site within the region and $f$ is the
bivariate density.

Consequently, the gradient of the log pairwise density is given by:
\begin{equation*}
  \nabla \ell_p(\Psi) = \sum_{i=1}^{n_{site}-1}
  \sum_{j=i+1}^{n_{site}} \nabla \log f(z_k^{(i)}, z_k^{(j)})
\end{equation*}

Define:
\begin{eqnarray*}
  A &=& - \frac{\Phi(c1)}{z_1} - \frac{\Phi(c2)}{z_2}\\
  B &=& \frac{\Phi(c_2)}{z_2^2} + \frac{\varphi(c_2)}{az_2^2} -
  \frac{\varphi(c_1)}{az_1z_2}\\
  C &=& \frac{\Phi(c_1)}{z_1^2} + \frac{\varphi(c_1)}{az_1^2} -
  \frac{\varphi(c_2)}{az_1z_2}\\
  D &=& \frac{c_2 \varphi(c_1)}{a^2z_1^2z_2} + \frac{c_1
    \varphi(c_2)}{a^2z_1z_2^2}  
\end{eqnarray*}
so that,
\begin{equation*}
  \log f(z_k^{(i)}, z_k^{(j)}) = A + log(B C + D)
\end{equation*}

\subsubsection{With Unit Fréchet Margins}
\label{sec:with-unit-frechet}

For clarity purposes, let start our computations assuming that the
observations have unit Fréchet margins. For this special case, the
logarithm of the bivariate density $f$ is only a function of the
Mahalanobis distance $a$, the gradient w.r.t. the covariance matrix
elements $cov_{11}$, $cov_{12}$ and $cov_{22}$ is given through the
following relation\footnote{algebra operators are defined
  component-wise.}:
\begin{equation*}
  \nabla_\Sigma \log f(z_k^{(i)}, z_k^{(j)}) = \frac{\partial}{\partial a}
  \log f(z_k^{(i)}, z_k^{(j)}) {\nabla_\Sigma a}^T
\end{equation*}
where $\nabla_\Sigma a$ is the gradient of the Mahalanobis distance
w.r.t. the covariance matrix element i.e. $( \frac{\partial
  a}{\partial cov_{11}}, \frac{\partial a}{\partial cov_{12}},
\frac{\partial a}{\partial cov_{22}})$.

For clarity purposes, we first compute the following quantities:
\begin{eqnarray*}
  \frac{\partial c_1}{\partial a} = \frac{1}{2} - \frac{1}{a^2} \log
  \frac{z_2}{z_1} = \frac{c_2}{a} &\qquad& \frac{\partial c_2}{\partial
    a} = \frac{c_1}{a}\\
  \frac{\partial \Phi(c_1)}{\partial a} = \frac{\partial
    \Phi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial a} =
  \frac{c_2 \varphi(c_1)}{a} &\qquad& \frac{\partial
    \Phi(c_2)}{\partial a} = \frac{c_1 \varphi(c_2)}{a}\\
  \frac{\partial \varphi(c_1)}{\partial a} = \frac{\partial
    \varphi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial a} =
  -\frac{c_1c_2 \varphi(c_1)}{a} &\qquad& \frac{\partial
    \varphi(c_2)}{\partial a} = -\frac{c_1c_2 \varphi(c_2)}{a}\\
  \frac{\partial c_2\varphi(c_1)}{\partial a} = \frac{c_1(1 -
    c_2^2)\varphi(c_1)}{a} &\qquad& \frac{\partial
    c_1\varphi(c_2)}{\partial a} = \frac{(1-c_1^2)c_2\varphi(c_2)}{a}
\end{eqnarray*}

Consequently, we have:
\begin{eqnarray*}
  dA_a &=& \frac{\partial A}{\partial a} = - \frac{1}{z_1} \frac{c_2
    \varphi(c_1)}{a} - \frac{1}{z_2} \frac{c_1 \varphi(c_2)}{a} =
  -\frac{c_2 \varphi(c_1)}{az_1} - \frac{c_1 \varphi(c_2)}{az_2}\\
  dC_a &=& \frac{\partial C}{\partial a} = \frac{1}{z_1^2} \frac{c_2
    \varphi(c_1)}{a} + \frac{1}{z_1^2}
  \frac{-\frac{c_1c_2\varphi(c_1)}{a} a  - \varphi(c_1)}{a^2} -
  \frac{1}{z_1z_2} \frac{-\frac{c_1c_2 \varphi(c_2)}{a}a -
    \varphi(c_2)}{a^2}\\
  &=& \frac{c_2 \varphi(c_1)}{az_1^2} -
  \frac{(1+c_1c_2)\varphi(c_1)}{a^2z_1^2} +
  \frac{(1+c_1c_2)\varphi(c_2)}{a^2z_1z_2}\\
  &=& \frac{\left[c_2(a - c_1)-1\right] \varphi(c_1)}{a^2z_1^2} +
  \frac{(1+c_1c_2)\varphi(c_2)}{a^2z_1z_2}\\
  &=& \frac{(c_2^2 - 1) \varphi(c_1)}{a^2z_1^2} +
  \frac{(1+c_1c_2)\varphi(c_2)}{a^2z_1z_2}\\
  dB_a &=& \frac{\partial B}{\partial a} = \frac{(c_1^2 - 1)
    \varphi(c_2)}{a^2z_2^2} +
  \frac{(1+c_1c_2)\varphi(c_1)}{a^2z_1z_2}\\
  dD_a &=& \frac{\partial D}{\partial a} = \frac{1}{z_1^2z_2}\frac{\frac{c_1(1 -
      c_2^2)\varphi(c_1)}{a}a^2 - 2a c_2\varphi(c_1)}{a^4} +
  \frac{1}{z_1z_2^2}\frac{\frac{(1-c1^2)c_2\varphi(c_2)}{a}a^2 - 2a
    c_1\varphi(c_2)}{a^4}\\
  &=& \frac{(c_1- 2 c_2 - c_1c_2^2) \varphi(c_1)}{a^3z_1^2z_2} +
  \frac{(c_2- 2 c_1 - c_1^2c_2) \varphi(c_2)}{a^3z_1z_2^2}
\end{eqnarray*}

Finally,
\begin{equation*}
  \nabla_\Sigma \log f(x_k^{(i)}, x_k^{(j)}) = \left[dA_a + \frac{(C
      dB_a + B dC_a +dD_a)}{BC + D} \right] \cdot{\nabla_\Sigma a}^T
\end{equation*}

\subsubsection{With Ordinary GEV Margins}
\label{sec:with-ordinary-gev}

In the previous section, we derived the gradient assuming unit Fréchet
margins. Now, we consider the more general case where margins are
supposed to be ordinary GEV. 

We have to be aware that the bivariate density changes when we do not
suppose unit Fréchet margins anymore. For instance, the bivariate
density evaluated at two observations $y_1$ and $y_2$ with ordinary
GEV margins is now given by:
\begin{equation}
  \label{eq:densSmithOrdGEV}
  f(y_1,y_2) = f(z_1, z_2) |J(y_1, y_2)|
\end{equation}
where $z_1$ (resp. $z_2$) is the transformation of $y_1$ (resp. $y_2$)
to the unit Fréchet scale and $|J(y_1, y_2)|$ is the determinant of
the Jacobian related to the transformation $(y_1,y_2) \mapsto
(z_1,z_2)$.

For clarity purpose, we can write the logarithm of the bivariate
density as follows:
\begin{equation*}
  \log f(y_1,y_2) = A + \log \left(BC + D\right) + E
\end{equation*}
where $E = \log |J(y_1, y_2)|$ and the quantities $A$,
$B$, $C$ and $D$ are the same as in the previous section.

The transformation from $y_i$ to $z_i$ is given by:
\begin{equation}
  z_i = \left(1 + \xi_i \frac{y_i - \mu_i}{\sigma_i}
  \right)_+^{\frac{1}{\xi_i}}
\end{equation}
where $\mu_i$, $\sigma_i$ and $\xi_i$ are the GEV location, scale and
shape parameters and $x_+ = \min(0,x)$.

Consequently, we need a response surface
(see section~\ref{sec:with-unknown-gev}) to model the evolution of the
GEV parameters in space. Let suppose that we have a polynomial
response surface for each GEV parameter, one can write:
\begin{eqnarray}
  \label{eq:polynomSurface}
  \mu &=& X_\mu \beta_\mu\\
  \sigma &=& X_\sigma \beta_\sigma\\
  \xi &=& X_\xi \beta_\xi
\end{eqnarray}
where $\mu = (\mu_1, \ldots, \mu_{n_{site}})$, $\sigma = (\sigma_1,
\ldots, \sigma_{n_{site}})$ and $\xi = (\xi_1, \ldots,
\xi_{n_{site}})$ are the vector for the location, scale and shape GEV
parameters for all the sites within the region study, $X_\mu$,
$X_\sigma$ and $X_\xi$ are the design matrices for each GEV parameters
and $\beta_\mu$, $\beta_\sigma$ and $\beta_\xi$ are the regression
coefficients to be estimated.

Consequently, from one ordinary GEV observation $\mathbf{y}$, one can
transform it to unit Fréchet margins using the following
transformation:
\begin{equation}
  \label{eq:gev2frech}
  \mathbf{z}_i=\left\{1+\frac{X^{(i)}_\xi
      \beta_\xi(\mathbf{y}_i -X^{(i)}_\mu
      \beta_\mu)}{X^{(i)}_\sigma \beta_\sigma}\right\}^{1/(X^{(i)}_\xi
    \beta_\xi)}, \qquad i=1,\ldots,n_{site} 
\end{equation}
where $X^{(i)}$ stands for the $i$-th row of the design matrix $X$ and
$\mathbf{z}_i$ denotes the $i$-th element of the vector
$\mathbf{z}$.

Consequently, $|J(y_1, y_2)|$ is given by:
\begin{equation}
  \label{eq:jacGev2frech}
  |J(y_1, y_2)| = \frac{1}{X^{(i)}_\sigma \beta_\sigma X^{(j)}_\sigma
    \beta_\sigma} \left(1 + X^{(i)}_\xi \beta_\xi \frac{y_i -
      X^{(i)}_\mu \beta_\mu}{X^{(i)}_\sigma \beta_\sigma}
  \right)_+^{\frac{1}{X^{(i)}_\xi \beta_\xi}-1} \left(1 +
    X^{(j)}_\xi \beta_\xi \frac{y_2 - X^{(j)}_\mu
      \beta_\mu}{X^{(j)}_\sigma \beta_\sigma}
  \right)_+^{\frac{1}{X^{(j)}_\xi \beta_\xi}-1}
\end{equation}


It is easy to see that:
\begin{eqnarray*}
  \frac{\partial \mathbf{z}_i}{\partial \beta_\mu} &=&
  -\frac{\mathbf{z}_i^{1 - X^{(i)}_\xi \beta_\xi} X^{(i)}_\mu}{
    X^{(i)}_\sigma \beta_\sigma}\\
  &=& -\frac{\mathbf{z}_i^{1-\xi_i}}{\sigma_i}\cdot
  X^{(i)}_\mu\\
  \frac{\partial \mathbf{z}_i}{\partial \beta_\sigma} &=&
  -\frac{\mathbf{z}_i^{1 - X^{(i)}_\xi \beta_\xi}
    \left(\mathbf{y}_i - X^{(i)}_\mu
      \beta_\mu\right)}{X^{(i)}_\sigma \beta^2_\sigma}\\
  &=& - \frac{\mathbf{z}_i^{1-\xi_i} \left(\mathbf{y}_i -
      \mu_i \right)}{\sigma_i} \cdot \frac{1}{\beta_\sigma}\\ 
  \frac{\partial \mathbf{z}_i}{\partial \beta_\xi} &=& -
  \frac{\mathbf{z}_i \log \mathbf{z}_i}{\beta_\xi} +
  \frac{\mathbf{z}^{(i)} \left(\mathbf{y}_i - X^{(i)}_\mu
      \beta_\mu\right)}{\beta_\xi X^{(i)}_\sigma \beta_\sigma
    \mathbf{z}_i^{X^{(i)}_\xi \beta_\xi}}\\
  &=& \left[ \mathbf{z}_i^{1 - \xi_i}
    \frac{\left(\mathbf{y}_i - \mu_i\right)}{\sigma_i} -
    \mathbf{z}_i \log \mathbf{z}_i\right] \cdot
  \frac{1}{\beta_\xi}
  % \frac{\partial c_1}{\partial \beta_\mu} &=& \frac{1}{a}
%   \left(\frac{X^{(i)}_\mu}{X^{(i)}_\sigma \beta_\sigma
%       (\mathbf{z}^{(i)})^{X^{(i)}_\xi \beta_\xi}} -
%     \frac{X^{(j)}_\mu}{X^{(j)}_\sigma \beta_\sigma
%       (\mathbf{z}^{(j)})^{X^{(j)}_\xi \beta_\xi}} \right)\\
%   &=& \frac{1}{a \sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \cdot X^{(i)}_\mu
%   - \frac{1}{a \sigma_j (\mathbf{z}^{(j)})^{\xi_j}} \cdot X^{(j)}_\mu\\
%   \frac{\partial c_1}{\partial \beta_\sigma} &=& \frac{1}{a}
%   \left(\frac{\mathbf{y}^{(i)} - X^{(i)}_\mu \beta_\mu}{X^{(i)}_\sigma
%       \beta^2_\sigma (\mathbf{z}^{(i)})^{X^{(i)}_\xi \beta_\xi}} -
%     \frac{\mathbf{y}^{(j)} - X^{(j)}_\mu \beta_\mu}{X^{(j)}_\sigma
%       \beta^2_\sigma (\mathbf{z}^{(j)})^{X^{(j)}_\xi \beta_\xi}}
%   \right)\\
%   &=& \frac{1}{a} \left[\frac{\mathbf{y}^{(i)} - \mu_i}{\sigma_i
%       (\mathbf{z}^{(i)})^{\xi_i}} - \frac{\mathbf{y}^{(j)} -
%       \mu_j}{\sigma_j (\mathbf{z}^{(j)})^{\xi_j}} \right] \cdot
%   \frac{1}{\beta_\sigma}\\
%   \frac{\partial c_1}{\partial \beta_\xi} &=& \frac{1}{a}
%   \left[\frac{\log \mathbf{z}^{(i)}}{\beta_\xi} -
%     \frac{\mathbf{y}^{(i)} - X^{(i)}_\mu \beta_\mu}{\beta_\xi
%       X^{(i)}_\sigma \beta_\sigma (\mathbf{z}^{(i)})^{X^{(i)}_\xi
%         \beta_\xi}} - \frac{\log \mathbf{z}^{(j)}}{\beta_\xi} +
%     \frac{\mathbf{y}^{(j)} - X^{(j)}_\mu \beta_\mu}{\beta_\xi
%       X^{(j)}_\sigma \beta_\sigma (\mathbf{z}^{(j)})^{X^{(j)}_\xi
%         \beta_\xi}}\right]\\
%   &=& \frac{1}{a} \left[\log \frac{\mathbf{z}^{(i)}}{\mathbf{z}^{(j)}}
%     + \frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}}- \frac{\mathbf{y}^{(i)} -
%       \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \right] \cdot
%   \frac{1}{\beta_\xi}
\end{eqnarray*}
where the operator $\cdot$ performs operations component-wise.
% Note that the partial derivatives for $c_2$ are easily obtained as:
% \begin{equation*}
%   \frac{\partial c_2}{\partial \beta_\mu} = - \frac{\partial
%     c_1}{\partial \beta_\mu}, \quad \frac{\partial c_2}{\partial
%     \beta_\sigma} = - \frac{\partial c_1}{\partial \beta_\sigma},
%   \quad \frac{\partial c_2}{\partial \beta_\xi} = - \frac{\partial
%     c_1}{\partial \beta_\xi}
% \end{equation*}

To obtain the gradient of the logarithm of the bivariate density, we
need to compute the partial derivatives of $A$, $B$, $C$, $D$ and $E$
w.r.t. $\beta_\mu$, $\beta_\sigma$ and $\beta_\xi$.
% \begin{eqnarray*}
%   \frac{\partial \Phi(c_1)}{\partial \beta_\mu} &=& \frac{\partial
%     \Phi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial \beta_\mu}\\
%   &=&
%   \frac{\varphi(c_1)}{a \sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \cdot
%   X^{(i)}_\mu - \frac{\varphi(c_1)}{a \sigma_j
%     (\mathbf{z}^{(j)})^{\xi_j}} \cdot X^{(j)}_\mu\\
%   \frac{\partial \Phi(c_2)}{\partial \beta_\mu} &=& \frac{\varphi(c_2)}{a
%     \sigma_j (\mathbf{z}^{(j)})^{\xi_j}} \cdot X^{(j)}_\mu -
%   \frac{\varphi(c_2)}{a \sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \cdot
%   X^{(i)}_\mu\\
%   \frac{\partial \Phi(c_1)}{\partial \beta_\sigma} &=& \frac{\partial
%     \Phi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial
%     \beta_\sigma}\\
%   &=& \frac{\varphi(c_1)}{a} \left[\frac{\mathbf{y}^{(i)} -
%       \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}} -
%     \frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}} \right] \cdot
%   \frac{1}{\beta_\sigma}\\
%   \frac{\partial \Phi(c_2)}{\partial \beta_\sigma} &=&
%   \frac{\varphi(c_2)}{a} \left[\frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}} - \frac{\mathbf{y}^{(i)} -
%       \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \right] \cdot
%   \frac{1}{\beta_\sigma}\\
%   \frac{\partial \Phi(c_1)}{\partial \beta_\xi} &=& \frac{\partial 
%     \Phi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial \beta_\xi}\\
%   &=&
%   \frac{\varphi(c_1)}{a} \left[\log \frac{\mathbf{z}^{(i)}}{\mathbf{z}^{(j)}}
%     + \frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}}- \frac{\mathbf{y}^{(i)} -
%       \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \right] \cdot
%   \frac{1}{\beta_\xi} \\
%   \frac{\partial \Phi(c_2)}{\partial \beta_\xi} &=&
%   \frac{\varphi(c_2)}{a} \left[\log
%     \frac{\mathbf{z}^{(j)}}{\mathbf{z}^{(i)}} + \frac{\mathbf{y}^{(i)}
%       - \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}}-
%     \frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}} \right] \cdot \frac{1}{\beta_\xi}\\
%   \frac{\partial \varphi(c_1)}{\partial \beta_\mu} &=& \frac{\partial 
%     \varphi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial \beta_\mu}\\
%   &=&
%   - \frac{c_1 \varphi(c_1)}{a \sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \cdot
%   X^{(i)}_\mu + \frac{c_1 \varphi(c_1)}{a \sigma_j
%     (\mathbf{z}^{(j)})^{\xi_j}} \cdot X^{(j)}_\mu\\ 
%   \frac{\partial \varphi(c_2)}{\partial \beta_\mu} &=& \frac{\partial 
%     \varphi(c_2)}{\partial c_2} \frac{\partial c_2}{\partial \beta_\mu}\\
%   &=&
%   \frac{c_2 \varphi(c_2)}{a \sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \cdot
%   X^{(i)}_\mu - \frac{c_2 \varphi(c_2)}{a \sigma_j
%     (\mathbf{z}^{(j)})^{\xi_j}} \cdot X^{(j)}_\mu\\
%   \frac{\partial \varphi(c_1)}{\partial \beta_\sigma} &=& \frac{\partial 
%     \varphi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial
%     \beta_\sigma}\\
%   &=& - \frac{c_1 \varphi(c_1)}{a}
%   \left[\frac{\mathbf{y}^{(i)} - \mu_i}{\sigma_i
%       (\mathbf{z}^{(i)})^{\xi_i}} - \frac{\mathbf{y}^{(j)} -
%       \mu_j}{\sigma_j (\mathbf{z}^{(j)})^{\xi_j}} \right] \cdot
%   \frac{1}{\beta_\sigma}\\
%   \frac{\partial \varphi(c_2)}{\partial \beta_\sigma} &=& \frac{c_2
%     \varphi(c_2)}{a} \left[\frac{\mathbf{y}^{(i)} - \mu_i}{\sigma_i
%       (\mathbf{z}^{(i)})^{\xi_i}} - \frac{\mathbf{y}^{(j)} -
%       \mu_j}{\sigma_j (\mathbf{z}^{(j)})^{\xi_j}} \right] \cdot
%   \frac{1}{\beta_\sigma}\\
%   \frac{\partial \varphi(c_1)}{\partial \beta_\xi} &=& \frac{\partial
%     \varphi(c_1)}{\partial c_1} \frac{\partial c_1}{\partial \beta_\xi}\\
%   &=&
%   - \frac{c_1 \varphi(c_1)}{a} \left[\log
%     \frac{\mathbf{z}^{(i)}}{\mathbf{z}^{(j)}} + \frac{\mathbf{y}^{(j)}
%       - \mu_j}{\sigma_j (\mathbf{z}^{(j)})^{\xi_j}}-
%     \frac{\mathbf{y}^{(i)} - \mu_i}{\sigma_i
%       (\mathbf{z}^{(i)})^{\xi_i}} \right] \cdot \frac{1}{\beta_\xi}\\
%   \frac{\partial \varphi(c_2)}{\partial \beta_\xi} &=& \frac{c_2
%     \varphi(c_2)}{a} \left[\log \frac{\mathbf{z}^{(i)}}{\mathbf{z}^{(j)}}
%     + \frac{\mathbf{y}^{(j)} - \mu_j}{\sigma_j
%       (\mathbf{z}^{(j)})^{\xi_j}}- \frac{\mathbf{y}^{(i)} -
%       \mu_i}{\sigma_i (\mathbf{z}^{(i)})^{\xi_i}} \right] \cdot
%   \frac{1}{\beta_\xi}
% \end{eqnarray*}

For shortness, we do it in ``one step'' with the convention $\beta =
(\beta_\mu, \beta_\sigma, \beta_\xi)$.
\begin{eqnarray*}
  \frac{\partial A}{\partial \beta} &=& \frac{\partial A}{\partial z_1}
  \cdot \nabla_\beta z_1 + \frac{\partial A}{\partial z_2} \cdot
  \nabla_\beta z_2\\
  \frac{\partial B}{\partial \beta} &=& \frac{\partial B}{\partial z_1}
  \cdot \nabla_\beta z_1 + \frac{\partial B}{\partial z_2} \cdot
  \nabla_\beta z_2\\
  \frac{\partial C}{\partial \beta} &=& \frac{\partial C}{\partial z_1}
  \cdot \nabla_\beta z_1 + \frac{\partial C}{\partial z_2} \cdot
  \nabla_\beta z_2\\
  \frac{\partial D}{\partial \beta} &=& \frac{\partial D}{\partial z_1}
  \cdot \nabla_\beta z_1 + \frac{\partial D}{\partial z_2} \cdot
  \nabla_\beta z_2\\
\end{eqnarray*}
where $\nabla_\beta z_1$ (resp. $\nabla_\beta z_2$) is the gradient of
$z_1$ (reps. $z_2$) w.r.t. $\beta$ and the partial derivatives of $A$,
$B$, $C$ and $D$ w.r.t. $z_1$ are given by the following equations:
\begin{eqnarray*}
  dA_{z_1} &=& \frac{\partial A}{\partial z_1} = \frac{\varphi(c_1) +
    a \Phi(c_1)}{a z_1^2} - \frac{\varphi(c_2)}{a z_1 z_2}\\
  dB_{z_1} &=& \frac{\partial B}{\partial z_1} = \frac{c_1
    \varphi(c_2)}{a^2 z_1 z_2^2} + \frac{c_2 \varphi(c_1)}{a^2 z_1^2
    z_2}\\
  dC_{z_1} &=& \frac{\partial C}{\partial z_1} = \frac{(a + c_2)
    \varphi(c_2)}{a^2 z_1^2 z_2} - \frac{2\Phi(c_1)}{z_1^3} -
  \frac{(2a+c_2)\varphi(c_1)}{a^2z_1^3}\\
  dD_{z_1} &=& \frac{\partial D}{\partial z_1} = \frac{\left[1 - c_2
      (a + c_2) \right] \varphi(c_1)}{a^2 z_1^3 z_2} - \frac{\left[1 +
    c_1 (a + c_2) \right] \varphi(c_2)}{a^3 z_1^2 z_2^2}
\end{eqnarray*}
while the partial derivatives of $A$, $B$, $C$ and $D$ w.r.t. $z_2$
are given by:
\begin{eqnarray*}
  dA_{z_2} &=& \frac{\partial A}{\partial z_2} = \frac{\varphi(c_2) +
    a \Phi(c_2)}{a z_2^2} - \frac{\varphi(c_1)}{a z_1 z_2}\\
  dB_{z_2} &=& \frac{\partial B}{\partial z_2} = \frac{(a + c_1)
    \varphi(c_1)}{a^2 z_1 z_2^2} - \frac{2\Phi(c_2)}{z_2^3} -
  \frac{(2a+c_1)\varphi(c_2)}{a^2z_2^3}\\
  dC_{z_2} &=& \frac{\partial C}{\partial z_2} = \frac{c_1
    \varphi(c_2)}{a^2 z_1 z_2^2} + \frac{c_2 \varphi(c_1)}{a^2 z_1^2
    z_2}\\
  dD_{z_2} &=& \frac{\partial D}{\partial z_2} = \frac{\left[1 - c_1
      (a + c_1) \right] \varphi(c_2)}{a^2 z_1 z_2^3} - \frac{\left[1 +
    c_2 (a + c_1) \right] \varphi(c_1)}{a^3 z_1^2 z_2^2}
\end{eqnarray*}

For the Jacobian part $E$, we have:
\begin{eqnarray*}
  dE_\mu &=& \frac{\partial E}{\partial \beta_\mu} = 
  \frac{\xi_1-1}{\sigma_1 z_1^{\xi_1}} \cdot X^{(1)}_\mu +
  \frac{\xi_2-1}{\sigma_2 z_2^{\xi_2}} \cdot X^{(2)}_\mu\\
  dE_\sigma &=& \frac{\partial E}{\partial \beta_\sigma} =  \left(
    \frac{(y_1 - \mu_1)(\xi_1-1)}{\sigma_1z_1^{\xi_1}} +
    \frac{(y_2-\mu_2)(\xi_2-1)}{\sigma_2 z_2^{\xi_2}} - 2\right)
  \cdot \frac{1}{\beta_\sigma}\\
  dE_\xi &=& \frac{\partial E}{\partial \beta_\xi} =
  \frac{(1-\xi_1)(y_1 - \mu_1)}{\sigma_1 \xi_1 z_1^{\xi_1}} \cdot
  X^{(1)}_\xi + \frac{(1-\xi_2)(y_2 - \mu_2)}{\sigma_2 \xi_2
    z_2^{\xi_2}} \cdot X^{(2)}_\xi - \log z_1 \cdot
  \frac{1}{\beta_\xi} - \log z_2 \frac{1}{\beta_\xi}\\
\end{eqnarray*}

Finally, we have:
\begin{equation}
  \nabla_\beta \log f(y_1,y_2) = \frac{\partial A}{\partial \beta} +
  \frac{C \frac{\partial B}{\partial \beta} + B \frac{\partial
      C}{\partial \beta}}{BC + D} + \frac{\partial E}{\partial \beta}  
\end{equation}
where
\begin{equation*}
  \frac{\partial E}{\partial \beta} = (dE_\mu, dE_\sigma, dE_\xi)^T  
\end{equation*}


\section{Schlather's Model}
\label{sec:schlather-char}

Schlather's model is given by:
\begin{equation}
  \label{eq:schlather}
  \Pr[Z_1 \leq z_1, Z_2 \leq z_2] = \exp\left[-\frac{1}{2}
    \left(\frac{1}{z_1} + \frac{1}{z_2} \right) \left(1 + \sqrt{1 - 2
        (\rho(h) + 1) \frac{z_1 z_2}{(z_1 + z_2)^2}} \right) \right]
\end{equation}
where $h$ is the distance between location \#1 and location \#2 and
$\rho(h)$ is a valid correlation function such as $-1 \leq \rho(h)
\leq 1$.

\subsection{Density computation}
\label{sec:density-computation-schlather}

Computation of the density as well as the gradient of the density is
not difficult but ``heavy'' though.

By noting that, 
\begin{equation*}
  \frac{\partial^2 }{\partial z_1 \partial z_2} \exp(V(z_1, z_2)) =
  \left[\frac{\partial^2}{\partial z_1 \partial z_2} V(z_1, z_2) +
    \left(\frac{\partial }{\partial z_1} V(z_1, z_2) \right)
    \left(\frac{\partial }{\partial z_2} V(z_1, z_2) \right) \right]
  \exp(V(z_1, z_2))
\end{equation*}
where $V(z_1, z_2)$ is any function in $\mathcal{C}^2$.

Consequently, to compute the (bivariate) density, we only need to
compute the partial derivatives and the mixed partial derivatives. For
our case, it turns out to be:

\begin{equation*}
  V(z_1, z_2) = -\frac{1}{2} \left(\frac{1}{z_1} + \frac{1}{z_2} \right)
  \left(1 + \sqrt{1 - 2 (\rho(h) + 1) \frac{z_1 z_2}{(z_1 + z_2)^2}}
  \right)
\end{equation*}


\begin{equation*}
  \frac{\partial}{\partial z_1} V(z_1, z_2) = -\frac{\rho(h) z_1 -
    c1 - z_2}{2 c_1 z_1^2} \quad
  \frac{\partial}{\partial z_2} V(z_1, z_2) = -\frac{\rho(h) z_2 -
    c1 - z_1}{2 c_1 z_2^2} \quad
  \frac{\partial^2}{\partial z_1\partial z_2} V(z_1, z_2) =
  \frac{1 - \rho(h)^2}{2 c_1^3}
\end{equation*}
where
\begin{equation*}
  c_1 = \sqrt{z_1^2 + z_2^2 - 2 z_1 z_2 \rho(h)}
\end{equation*}

Lastly,
\begin{equation}
  \label{eq:schlatherDens}
  f(z_1, z_2) = \left[ \frac{1 - \rho(h)^2}{2 c_1^3} +
    \left(-\frac{\rho(h) z_1 - c1 - z_2}{2 c_1 z_1^2} \right) \left(
      -\frac{\rho(h) z_2 - c1 - z_1}{2 c_1 z_2^2} \right) \right]
  \exp(V(z_1, z_2))
\end{equation}

\subsection{Gradient computation}
\label{sec:gradient-computation-schlather}

\subsubsection{With Unit Fréchet Margins}
\label{sec:with-unit-frechet-1}

From equation \eqref{eq:schlatherDens}, we have:
\begin{equation*}
  \log f(z_1, z_2) = A + \log(B + C D)
\end{equation*}
where
\begin{equation*}
  A =  V(z_1, z_2) \quad
  B = \frac{1 - \rho(h)^2}{2 c_1^3} \quad
  C = -\frac{\rho(h) z_1 - c1 - z_2}{2 c_1 z_1^2} \quad
  D = -\frac{\rho(h) z_2 - c1 - z_1}{2 c_1 z_2^2}
\end{equation*}

As the bivariate density is only a function of the covariance function
$\rho(h)$, we have:
\begin{equation*}
  \nabla \log f(z_1, z_2) = \frac{\partial}{\partial \rho(h)} \log
  f(z_1, z_2) \left(\nabla \rho(h) \right)^T
\end{equation*}
where $\nabla \rho(h)$ is the vector of the partial derivatives of the
covariance function $\rho(h)$ with respect to its parameters.

\begin{eqnarray*}
  dA_\rho &=& \frac{\partial A}{\partial \rho(h)} = \frac{1}{2c_1}\\
  dB_\rho &=& \frac{\partial B}{\partial \rho(h)}  = -\frac{\rho(h)}{c_1^3} +
  \frac{3(1 - \rho(h))z_1 z_2}{c_1^5}\\
  dC_\rho &=& \frac{\partial C}{\partial \rho(h)} = -\frac{z_1-z_2\rho(h)}{2
    c_1^3}\\
  dD_\rho &=& \frac{\partial D}{\partial \rho(h)} = -\frac{z_2-z_1\rho(h)}{2
    c_1^3}\\
\end{eqnarray*}
So that,
\begin{equation*}
  \nabla \log f(z_1, z_2) = \left[dA_\rho + \frac{(C dB_\rho + B
      dC_\rho + dD_\rho)}{BC + D} \right] \left(\nabla \rho(h)
  \right)^T
\end{equation*}

Note that when using the Whittle-Matérn covariance function, the
standard errors are not available if the \verb|smooth| parameter is
hand fixed because the Bessel function is not derivable w.r.t. this
parameter.

\subsubsection{With Ordinary GEV Margins}
\label{sec:with-ordinary-gev-1}

For the derivation of the gradient with ordinary GEV margins, most of
the computations have already been done in
Section~\ref{sec:with-ordinary-gev}. Especially, we only need to
compute the partial derivatives of $A$, $B$, $C$ and $D$ w.r.t. $z_1$
and $z_2$.

\begin{eqnarray*}
  dA_{z_1} &=& \frac{\partial A}{\partial z_1} = -\frac{\rho(h) z_1 -
    c1 - z_2}{2 c_1 z_1^2}\\
  dB_{z_1} &=& \frac{\partial B}{\partial z_1} = \frac{3 (\rho(h)^2 -
    1) (z_1 - \rho(h) z_2)}{2 c_1^5}\\
  dC_{z_1} &=& \frac{\partial C}{\partial z_1} = \frac{2z_1^3 \rho(h)
    + 6 z_1 z_2^2 \rho(h)^2 - 3 z_1^2 z_2 (1 + \rho(h)^2) - 2 c_1^3 -
    2 z_2^3}{2 c_1^3 z_1^3}\\
  dD_{z_1} &=& \frac{\partial C}{\partial z_2} = -\frac{(z_2 \rho(h) -
    c_1 - z_1) (z_2 \rho(h) + c1 - z_1)}{2 c_1^3 z_2^2}  
\end{eqnarray*}
and
\begin{eqnarray*}
  dA_{z_2} &=& \frac{\partial A}{\partial z_2} = -\frac{\rho(h) z_2 -
    c1 - z_1}{2 c_1 z_2^2}\\
  dB_{z_2} &=& \frac{\partial B}{\partial z_2} = \frac{3 (\rho(h)^2 -
    1) (z_2 - \rho(h) z_1)}{2 c_1^5}\\
  dC_{z_2} &=& \frac{\partial C}{\partial z_2} = -\frac{(z_1 \rho(h) -
    c_1 - z_2) (z_1 \rho(h) + c1 - z_2)}{2 c_1^3 z_1^2}\\
  dD_{z_2} &=& \frac{\partial D}{\partial z_2} = \frac{2z_2^3 \rho(h)
    + 6 z_1^2 z_2 \rho(h)^2 - 3 z_1 z_2^2 (1 + \rho(h)^2) - 2 c_1^3 -
    2 z_1^3}{2 c_1^3 z_2^3}
\end{eqnarray*}

Finally, we have:
\begin{equation}
  \nabla_\beta \log f(y_1,y_2) = \frac{\partial A}{\partial \beta} +
  \frac{C \frac{\partial B}{\partial \beta} + B \frac{\partial
      C}{\partial \beta}}{BC + D} + \frac{\partial E}{\partial \beta}  
\end{equation}
where $\frac{\partial A}{\partial \beta}$, $\frac{\partial B}{\partial
  \beta}$, $\frac{\partial C}{\partial \beta}$, $\frac{\partial
  D}{\partial \beta}$ and $\frac{\partial E}{\partial \beta}$ have
been already defined in Section~\ref{sec:with-ordinary-gev}.

\bibliography{./references}
\bibliographystyle{apalike}
\end{document}

